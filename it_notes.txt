________________________________
IT - NOTES

plt.plot(temperature['2010-01'], color='r', label='Temperature')
dewpoint = weather['DewPoint']
plt.plot(dewpoint['2010-01'], color='b', label='Dewpoint')
plt.legend(loc='upper right')
plt.xticks(rotation=60)
plt.show()

### notes:
- scratch > programming for childeren

### books/media started:
- [ ] Automate the boring stuff with python
- [ ] Python Playground
- [ ] Expert recipes for Linux bash and more
- [ ] Wicked cool shell scripts
- [ ] Python Automation Cookbook
- [ ] Python3 Object Oriented Programming
- [ ] The Practice of Network Security Monitoring
- [ ] NMap network exploration and security
- [ ] Network analysis using Wireshark
- [ ] Mastering Python networking
- [ ] Malware Data Science
- [ ] LPIC-1 LPIC-2
- [ ] Linux command line and shell scripting bible
- [ ] Linux bible
- [ ] Linux basics for hackers
- [ ] Invent your own computer games with python
- [ ] Impractical Python Projects
- [ ] Hands on software engineering with python
- [ ] Gray Hat Python
- [ ] Black Hat Python
- [ ] Fluent Python
- [ ] Expert Python Programming
- [ ] Doing Math with Python
- [ ] Django
- [ ] Cracking Codes with Python
- [ ] CompTIA
- [ ] Code Craft
- [ ] Clean Code in Python
- [ ] Beginning Linux Programming
- [ ] Assambly language step by step
- [ ] Erickson - Hacking - the Art of Exploitation - 
- [ ] Station X Platform
- [ ] Datacamp
- [ ] Udemy
	- [ ] Python - Bootcamp Anfänger > Profi A-Z
	- [ ] Python - Ethical Hacking
	- [ ] Python - Data Science, Apache Spark & Python
	- [ ] TCP/IP in der Praxis
	- [ ] Python - Automate Boring Stuff
	- [ ] Eric Amberg - Computer Netzwerke
	- [ ] Eric Amberg - Wireshark

- [ ] Bryant/O‘Hallaron - Computer Systems
- [ ] Kurose/Ross - Computer-Netwerke
- [ ] Tanenbaum - Netzwerke
- [ ] Häberlein - Informatik
- [ ] Andy Field - Discovering Statistics
- [ ] Wordpress Praxisbuch
- [ ] Tanenbaum - Betriebssysteme
- [ ] Springer - Betriebssysteme
- [ ] Tanenbaum - Rechnerarchitektur
- [ ] Lucas - Networking for Systemadministrators
- [ ] Mark B. - Hacken mit Kali-Linux
- [ ] How Linux works
- [ ] Unix and Linux Systems for System Administrators
- [ ] Grundkurs Künstliche Intelligenz


________________________________
Content

- [Notes](#notes)
- [Network](#network)
- [Linux General](#linux_general)
- [Linux Bash | Commands | Scripts](#linux_bash)
	- [Shell](#bash_shell)
	- [Scripting](#bash_scripting)
	- [Input/Output](#bash_IO)
	- [Regular Expressions](#bash_regular_expressions)
	- [Files](#bash_files)
	- [Aliases](#bash_aliases)
	- [Directories](#bash_directories)
	- [Links](#bash_links)
	- [Archives/Compression](#bash_archives)
	- [Devices](#bash_devices)
	- [User Management](#bash_user_management)
	- [Packet Manager](#bash_packet_manager)
	- [Processes](#bash_processes)
	- [Editors](#bash_editors)
	- [Shutdown](#bash_shutdown)
	- [X_Screens](#bash_x-screens)
	- [Operating System](#bash_os)
	- [Networks](#bash_network)
	- [Devices](#bash_printer)
	- [Raspberry Pi](#bash_raspi)
- [Python](#python)
	- [Installation](#python_installation)
	- [Jupyter Notebook](#python_jupyter_notebook)
	- [Basics](#python_basics)
	- [Variables](#python_variables)
	- [Operators](#python_operators)
	- [Special Functions](#python_special_functions)
	- [Flow Control](#python_flow_control)
	- [Functions](#python_functions)
	- [Composed Variables](#python_composed_variables)
	- [References](#python_references)
	- [Functional Programming](#python_functional_programming)
	- [Files and Directories](#python_files_directories)
	- [Objects](#python_objects)
	- [Regular Expressions](#python_regular_expressions)
	- [Debugging | Errors | Exeptions](#python_debugging)
	- [Module](#python_modules)
	- [Numpy](#python_numpy)
	- [MathPlot](#python_mathplot)
	- [Pandas](#python_pandas)
	- [Cleaning Data](#python_cleaning_data)
	- [Visualization](#python_visualization)
	- [Interactive Data Visualization](#python_interactive_visualization)
	- [Statistics](#python_statistics)
	- [Data Bases](#python_data_bases)
	- [Scikit Learn](#scikit)
	- [Django](#python_django)
		- [Notes](#python_django_notes)
		- [Backend](#python_django_backend)
		- [Virtual Environment](#python_django_venv)
		- [Django Project](#python_django_project)
		- [Django Apps](#python_django_apps)
		- [Templates](#python_django_templates)
		- [Static Files: Media, CSS, JS, ...](#python_django_static_files)


- [R Statistics](#r_statistics)
	- [Notes](#r_notes)
	- [Fundamentals](#r_fundamentals)
	- [Vectors](#r_vectors)
	- [Regular Expressions](#r_regexp)
	- [Timestamps](#r_timesstamps)
	- [Matrices](#r_matrices)
	- [Dataframes](#r_dataframes)
	- [Lists](#r_lists)
	- [Data Manipulation](#r_datamanipulation)
	- [Visualization with GGPlot2](#r_ggplot2)

- [SQL](#sql)

- [GIT version control](#git)

- [Applications in adition](#applications_in_adition)

________________________________
<a name="notes"/>
# Notes
________________
### IT todo
- network
- wlan
- onion tor
- ai
- git
- linux/unix admin
- exploitation
- docker - server
-  

________________
### to be continued:
- Video 35 - https://www.youtube.com/watch?v=rJMFxIbDe-g&index=35&list=PLtK75qxsQaMLZSo7KL-PmiRarU7hrpnwK
- Informatik: Eine praktische Einführung mit Bash und Python
- The Hacker Playbook
- How Linux Works
- Hacken mit Kali Linux
- Linux and Unix System Administration Handbook
- art of exploitation
- job interview questions https://www.facebook.com/tutorialinux/videos/949817708477667/






________________________________
<a name="network"/>
# Network

________________
### Concepts

- IPv4 IPv6 - Router: 192.168.0.1 | Me: 127.0.0.1
- DNS
- NAT <> Router





________________________________
<a name="linux_general"/>
# Linux - Allgemein

________________________________
### Linux - documentation

- man
	- search "/something" - previous p - next n
- info > gnu version
- whatis > first line of man description - before: sudo mandb
- apropos > everythin linked in man pages - before: sudo mandb
- "ls usr/share/doc/"
- reddit:
	- https://www.reddit.com/r/sysadmin/
	- https://www.reddit.com/r/linux/
	- https://www.reddit.com/r/freebsd/
- arch linux doc
	- https://wiki.archlinux.org/index.php/General_recommendations
	- https://wiki.archlinux.org/index.php/List_of_applications
- http://www.tldp.org/LDP/abs/html/abs-guide.html
________________________________
### Linux - directory hierarchy

- OVERVIEW: "man hier" | filesystem hierarchy
- /bin Contains ready-to-run programs (also known as an executables), including most of the basic Unix commands such as ls and cp. Most of the programs in /bin are in binary format, having been created by a C compiler, but some are shell scripts in modern systems.
- /dev Contains device files. You’ll learn more about these in Chapter 3. 
- /etc EHT CEE "edit to configure" This core system configuration directory (pronounced EHT-see) contains the user password, boot, device, networking, and other setup files. Many items in /etc are specific to the machine’s hardware. For example, the /etc/ X11 directory contains graphics card and window system configurations. 
- /home Holds personal directories for regular users. Most Unix installations conform to this standard. 
- /lib An abbreviation for library, this directory holds library files containing code that executables can use. There are two types of libraries: static and shared. The /lib directory should contain only shared libraries, but other lib directories, such as /usr/ lib, contain both varieties as well as other auxiliary files. (We’ll discuss shared libraries in more detail in Chapter   15.) 
- /proc Provides system statistics through a browsable directory-and-file interface. Much of the /proc subdirectory structure on Linux is unique, but many other Unix variants have similar features. The /proc directory contains information about currently running processes as well as some kernel parameters. 
- /sys This directory is similar to /proc in that it provides a device and system interface. You’ll read more about /sys in Chapter   3. 
- /sbin The place for system executables. Programs in /sbin directories relate to system management, so regular users usually do not have /sbin components in their command paths. Many of the utilities found here will not work if you’re not running them as root. 
- /tmp A storage area for smaller, temporary files that you don’t care much about. Any user may read to and write from /tmp, but the user may not have permission to access another user’s files there. Many programs use this directory as a workspace. If something is extremely important, don’t put it in /tmp because most distributions clear /tmp when the machine boots and some even remove its old files periodically. Also, don’t let /tmp fill up with garbage because its space is usually shared with something critical (like the rest of /, for example). 
- /usr Although pronounced “user,” this subdirectory has no user files. Instead, it contains a large directory hierarchy, including the bulk of the Linux system. Many of the directory names in /usr are the same as those in the root directory (like /usr/bin and /usr/lib), and they hold the same type of files. (The reason that the root directory does not contain the complete system is primarily historic — in the past, it was to keep space requirements low for the root.) 
- /var The variable subdirectory, where programs record runtime information. System logging, user tracking, caches, and other files that system programs create and manage are here. (You’ll notice a /var/tmp directory here, but the system doesn’t wipe it on boot.)

- /boot Contains kernel boot loader files. These files pertain only to the very first stage of the Linux startup procedure; you won’t find information about how Linux starts up its services in this directory. See Chapter   5 for more about this. 
- /media A base attachment point for removable media such as flash drives that is found in many distributions. 
- /opt This may contain additional third-party software. Many systems don’t use /opt.

- /usr/include Holds header files used by the C compiler. 
- /usr/info Contains GNU info manuals (see 2.13 Getting Online Help). 
- /usr/local Is where administrators can install their own software. Its structure should look like that of / and /usr. 
- /usr/man Contains manual pages. 
- /usr/share Contains files that should work on other kinds of Unix machines with no loss of functionality. In the past, networks of machines would share this directory, but a true /share directory is becoming rare because there are no space issues on modern disks. Maintaining a /share directory is often just a pain. In any case, /man, /info, and some other subdirectories are often found here.






________________________________
<a name="linux_bash"/>
### Linux - Bash - Shell - Commands
- options: "-" short options - "--" long options
difference: internal command (built-in) - external command
- "type <command>" find out type
- "type -a <command>" check wether internal command has external duplicate
- to acces external command use eg "usr/bin/pwd" instead of "pwd"
normal user: $
super user: #
home directory: ~
Sondereichen:
- "|" ALT+">"
Wildcards - Globbing - files/directory names in current dir:
- use characters not as wild card > use backslash \ before character
- "?" single character
- "*" indefinite number of characters
	- e.g. "echo *"
- "[]" character class - single character containing in brackets
- "[]*" character class - indefinite number of characters
- "[!]" exclude characters
- "[a-g]" or "[3-6]" ranges
- predefined char classes
	- "[[:alpha:]]" alphabetic letters
	- "[[:alnum:]]" alphanumeric letter
	- "[[:digit:]]" numbers
	- "[[:lower:]]" lower case
	- "[[:upper:]]" upper case
	- "[[:space:]]" white space
- Unterdrücken durch '*' bzw '?'

________________
<a name="bash_shell"/>
### shell
- bash config files: eg ~/.bashrc /etc/profile
- "which bash" path, where the program is executed
- "clear"
- /bin/sh -> points to default shell of system/user - normally /bin/bash
- clear
	- "CTRL-c" clear current command
	- "CTRL-l" clear screen
- navigation
	- "CTRL-a" beginning of the line
	- "CTRL-e" end of the line
	- "CTRL-f" forward in the line
	- "CTRL-b" back in the line
- deletion
	- "CTRL-d" delete a character
	- "CTRL-d" delete word forwards
	- "CTRL-w" delete word backwards
	- "CTRL-u" delete beginning -> curser
	- "CTRL-k" delete curser -> end
- history
	- "history" - "history -c" clear history - stored in ~/.bash_history
	- "CTRL-p" history previous
	- "CTRL-n" history next
	- "CTRL-r" history search - repeat to nav backwards
	- "CTRL-s" history search - to nav forward - if results in hang (exit CTRL-q) type "stty -ixon" toggle flow-control
	- "CTRL-g" history search - terminate search in $FCEDIT and $EDITOR
- editor
	- "CTRL-x CTRL-e" open in editor
	- "set -o vi" set vi shell-mode on
- "#" bash comment
- "chsh" changes into another Shell
- Variablen:
	- Set "testvariable=bla" - without $ sign
	- Access "echo $testvariable" or "echo "bla ${testvariable} bla""
	- append variable: test=$test:append | test=$append:test
	- echo "there are `wc -l < /etc/group` groups" or  groupNumber=`wc -l | /etc/group`, echo $groupNumber
- Environment Variables - part of the sytem - other programs can/need to modify
	- "env" returns all environment variables
	- "unset VAR" deletes variable VAR
	- "$PATH" command path - path directories sep by ":" - list of paths to search for commands
		- change: "PATH = dir:$PATH" or "PATH = $PATH:dir"
	- "$TERM" info about terminal program
	- "$PWD"
	- "$OLDPWD"
	- "$HOME" home dir
	- "$SHELL" std shell program
	- "$EDITOR" std editor
		- editor: "which editor" >>> link to /usr/bin/editor
		- ls -l /usr/bin/editor >>> link to /etc/alternatives/editor
		- ls -l /etc/alternatives/editor >>> /usr/bin/vim.gtk3
	- "$$" current shell PID
	- "$(date)" actual time/date
	- "$PS1" customize Bash/ksh/sh
		- persist changes in ~/.bash_profile
		- \d date in "Weekend Month Date" format "Tue May 26"
		- \h hostname up to first period
		- \H hostname
		- \n newline
		- \t current time in 24-hour HH:MM:SS format
		- \T current time in 12-hour HH:MM:SS format
		- \@ current time in 12-hour am/pm format
		- \A current time in 24-hour am/pm format
		- \u username of current user
		- \w current working directory
		- \W basename of current working directory
		- \$ if the effective UID is 0, a #, otherwise a $
	- "$prompt" customize Csh, tcsh, zsh
	- "read -p "Bitte Vor- und Nachname eingeben:" VNAME NNAME" Einlesen von Variablen - Dateneingabe
	- "read A <test.txt" liest erste Zeile von test.txt in Variable A
	- "$[$1+$2]" ausführen von einfachen Rechenaufgaben
	- "export VARIABLE" make shell variable to environmental variable (readable for other programs) - eg all variables before are environmental variables
	- "!!" includes last command
- manuals: "man command"
	- search option "man -k keyword" lists commands for keyword | use string "keyw1 keyw2" for many keywords
	- numbers mean sections...
	- "man n command" scrolls to section n
	- navigation: arrows, page down "d" | page up "u" | to the top "g" | to the bottom "G" | quit "q" - default: less - change: "man -P /bin/more pwd"
- help: add "-h" or "--help" or "help comman"
- "info command"
- "where sudo" find where binary (of sudo) is
- "echo $PATH" looks up where any command can be found (in this order) and stops there
	- "PATH=$PATH:/some/dir" edit $PATH variable
	- "PATH=/root:$PATH" finds things first in /root - evil ssh script
- "script" program
	- "script script" begin script, end with "ctrl+d" -> generates typescript
	- "script myscript.log" start -> end "exit" generates file
	- real time recordings
		- rec "script myscript.log timing=tim.log" ... "ctrl+d" 
		- load "script scriptplay -s myscript.log -t time.log"
		- load "script scriptplay -s myscript.log --timing=time.log"
		- "less time.log" prints out timing log
	- "script" + "script -c 'netstat -tupln' netstat.log" log a single command
		- "script" + "script myscript -t time.log"
		- "scriptplay -s myscript.log --timing=time.log"
- shellcheck: linting bash scripts
- tricks for speed and efficiency
	- "sudo !!" runs the last non-sudo command with sudo
	- killing and yanking text
		- "ctrl+k" cut to end of the line: kill
		- "ctrl+u" cut to beginning of the line: kill
		- "ctrl+y" paste text: yank
		- "ctrl+u" > "sudo" > "ctrl+y" sudorize command
	- replace tail with less:
		- "tail -f /var/log/auth.log"
		- "less +F /var/log/auth.log" better "ctr+c" to scroll "shift+F"
	- editing current command in a text editor
		- "write a line " > "ctrl+x+e" > buffer in std editor (see  $EDITOR) > quitting executes commands
	- paste argument of previous command
		- "ping 8.8.8.8" bunch of different things
		- "mtr --curses"  > "alt+." > "mtr 8.8.8.8"
	- "reset" unbork your terminal
	- shell aliases
		- "alias lr="ls -lrc"" eg - not peristent
		- add alias to ~/.bashrc > new terminal
		- 	"ls="ls -a --color"" shadowing: overwrite same command
		- "alias" list all alias
		- "alias | grep lr"
- "time <command>" returns time processing the command: execution time, user time, systems time
- "set" set various bash-shell options
- "logout" - "exit"
- "update-alternatives" - determines std within group of programs with same function
        - "update-alternatives --get-selection" show all program groups with std entry
        - "sudo update-alternatives --config editor" menu for std choice


________________
<a name="bash_scripting"/>
Shell Scripting

- run shell script
	- run in login-shell "source shellScript.sh" or "./shellScript.sh" > use all internal variables | warning: don't exit in the script
	- "bash shellScript.sh" >> no internal variables "globalized", just run the script
	- "source shellScript.sh" >> take existing login shell and add variables to it
- beginning for kernel "shebang": "#!/bin/bash", "#!/bin/csh", "#!/bin/ksh", "#!/bin/zsh" by "wich bash"
	- consider: "which bash" -> use std bash
- others, eg python: "#!/usr/bin/python"
- return values - ending returns error:
	- possible values: 0-255
	- "exit" bare exit
	- "exit 0" no problems here
	- "exit 1" some error
	- "exit 12" special error code
	- "exit $?" returns value from last statement
- "# this is a comment"
- "$#" number of arguments of script
- "$0" script file name
- "$1" - "$n" arguments
- "$@" array of arguments

			ourfilename=$0
			echo $sourfilename
			
			num_arguments=$#
			echo "we have $num_arguments arguments"

			echo "the first three arguments were ${1}, ${2} and ${3}"
			echo $5 # returns blank line if not defined
			
- if statement: if [ statement ] then elif elif else fi

			if [condition-is-true];
			then
				<command>
			elif [condition-2-is-true];
			then
				<command>
			else
				<commands>
			fi

- "for arg in cond; do command1 done"

			for COLOR in red green blue
			do
				echo "COLOR: $COLOR"
			done
			
			for arg in "@$"; do
				echo "$arg"
			done

- "while"

			while [condition-to-be-true]
			do
				<command>
			done

- functions: 
	- definition "funcName (var1, var2) {}"
	- call a function "funcName var1"
- tests: "$str1"="$str2" | "$str1"!="$str2" | -n "$str1" notNull | -z "$str1" null | $int1 -eq $int2 | $int1 -ne $int2 | $int1 -gt $int2 | $int1 -lt $int2 | $int1 -ge $int2 | $int1 -le $int2 | $int1 -st $int2 OLD STYLE: "if [...];"
-  $int1 == $int2 |  as well < > <= >= != NEW STYLES: "if ((...));"
- and "||" - or "&&"
- testing conditions - returns boolean
	- "test -w fileName" file exists and user has right for writing
	- gleich "[ -w fileName ]"
	- option -e: file exists
	- option -r: file exists and user has right for reading
	- option -w: file exists and user has right for writing
	- option -x: file exists and user has right for executing
	- option -f: file exists and is a simple file
	- option -d: file exists and is a directory
	- option -s: file exists and is not empty
	- option -l: file exists and is a symbolic link
	- option -d: file exists and is a directory
	- option... many more -> "man test"
	- expl: "test -w /etc/passwd && echo "I am root!""
- long scripts >>> python etc
- variables

		SOME_VARIABLE="a strings" # no space between!
		echo "here is $SOME_VARIABLE for you"
		echo "this is my ${SOME_NUMBER}th beer"
		echo "there are 'wc -l < /etc/group' lines in /etc/group"
		
	- convention: underline: some_variable

	
________________
<a name="bash_IO"/>
### Input/Output
- linux represents practically everything as a file
- Std I/O: STDIN 0 STDOUT 1 STDERR 2
	- default: 0 keyboard, 1 screen
	- std in/output/error from/to files
		- file into std-input "<"
		- std-output redirect (overwrites) ">" or "1>"
		- std-output redirect (appends) file ">>"
		- std-output and std error redirect to file "&>"
		- "&" used with redirection: signals that file descriptor is being used
		- std-error redirect "2>" in std-output "2>&1"
		- std-error AND std-output: ">&"
		- std-output/error redirect to nowhere: "2>/dev/null" or ">/dev/null 2>&1"
- Pipes:
	- Send std-output of command1 to std-input of command2 by "|"
	- composition fo functions/commands
	- command1 | command1 | command1
	- "cat file.txt | sort -bf" sort lines by first name
	- "cat file.txt | uniq" only print unique lines
	- "cat file.txt | grep ..."
	- "cat file.txt | wc -l" word count - counting lines
- "xargs" runs command for each word generated by xargs, passed through std input - multiple options in quotation marks
	- "xargs [options] [command [initial argument]]
	- "find / -user Christine | xargs -d "\n" rm" - delimiter option with new line
	- "rm `find ./ -user tomo`" backtick syntax - not always works
	- "rm $(find ./ -user tomo)" backtick syntax - same as above
- logical symbols
	- "prog1 && prog2" runs prog2 only if prog1 was successful
	- "prog1 || prog2" runs prog2 only if prog1 was not successful
- "cat" concatenate and print files - Ausgabe (std Output Terminal-Bildschirm) des Inputstreams
	- zb gesamter Inputstream "cat"
	- zb speziell Inhalt einer Datei: "cat datei.bla"
	- access devices in form of device nodes
	- rückwärts: "tac"
	- option "-n" line numbers
- Lesen großer Textdateien "less" | "more"
	- "space" fwd | "b" back | "q" quit
	- search inside file: fwd "/exampleword " or backwd "?exampleword" weiter "n"
	- less: "v" opens std editor
- "head file" | "tail file" first/last 10 lines
	- "head -n change number by n | neg numbers possible >> last lines
	- "tail -f file" follow file: wait for additional data to be appended
	- "tail -1 file" show last line of file
- "echo" Argument in Std-Output
	- useful for finding expansions of sh globs/variables
- "grep" - search lines in files matching a pattern 
	- "grep <search pattern> <file>" 
	- "grep 'money' *"
	- "grep -i" case insensitive
	- "grep -c" counts number of occurancer in file
	- "grep -v" invert match: returns all lines not matching the pattern
	- "grep -n" precede output with line numbers
	- "grep -E extRegExp" extended regular expressions - other regular expression types possible - use simple quotation '
	- mächtigere Alternative: "egrep"
	- reguläre Ausdrücke: Mächtiger als Wildcard-Ausdrücke zb ".*" für * und "." für ? -> "man grep"
- "sort" Sortiert Zeilen alphanumerisch
	- "sort -n" numerisch
	- "sort -r" reversed
	- "sort -b" ignore leading whitespace
	- "sort -f" case insensitive
	- "sort -kF" sort by key with field number F - e.g. 2nd column
	- "sort -u" sort unique - no duplicate lines
	- eg "sort -bf" 
- "cut -d: -f2 <fileName>" divide file into fields with sep ":" and print field 2
- "tr" translates characters, eg "tr a-z A-Z"
- "wc <fileName>" wordcount
	- "wc -l" number of lines
- "column <fileName>" formats input to columns
	- "column -t"
	- "column -s" specify delimiting characters
- "tee" read from std input | write to std output and files | split output and do several things
	- "date | tee mylog.log" outputs in file AND in terminal
	- "date | tee -a mylog.log" append in file
- "sed" stream editor for filtering and transforming text
	- "sed 's/Welt/Linux' hallo_welt.txt" substitutes the first 'Welt' with 'Linux' in each line
	- "sed 's/Welt/Linux/p' print these line on screen | only line with -n flag
	- "sed 's/Welt/Linux/g' hallo_welt.txt" globale | substitutes all 'Welt' with 'Linux' in each line
	- "sed 's/Welt/Linux/2' hallo_welt.txt" substitutes 2x 'Welt' with 'Linux' in each line
	- "sed 's/Welt/Linux/2;s/Hallo/Servus/1' add commands separating them by ';' 
	- "sed '3d' hallo_welt.txt" deletes 3ed line
	- "sed '2,3d' hallo_welt.txt" deletes 2nd and 3ed line
	- "sed '$d' hallo_welt.txt" deletes last line
	- "sed '/irgendwas/d' hallo_welt.txt" deletes 'irgendwas' in each line
	- "sed -i" output on screen AND to file (inplace)
	- "sed -n" no output on screen
	- "sed -n -E 's/model name(\s*):(.*)@//p' /proc/cpuinfo" - returns processor speed
	- "sed -n -E 's/model name(\s*):(.*)@(.*)/processor. \2/p' /proc/cpuinfo" - returns processor name



________________
<a name="bash_regular_expressions"/>
- eg https://regexr.com
- use single quotation marks '...'
- (.*) . wildcard | * repetition zero or more
- ([0-9]*) class of numbers | * repetition

________________
<a name="bash_files"/>
### files:
- containing space > use quotes/doublequotes: 'test file.txt' or "test file.txt"
- "file <fileName>" display file type - additional informations
- types: file (-) | (soft)link (l) | dir (d) | block (b) | character (c) | pipes (p) | sockets (s)
- last four (block, char, pipes, sockets) > devices /dev/
- Copy "cp sourceFiles destFile" creates destination if neccesary (-r)
	- "cp -r" - recursive copying of (several) whole paths with files
	- "cp -a" - Attribute unverändert
	- "cp -i" - Interactive mode - asks if something will be overwritten
- Move "mv", > funktioniert als rename, bewegen, überschreiben - like "cp"
- Remove "rm"
	- "rm -f" forces deletion
	- "rm -r" recursive
	- "rm -i" interactive mode - asks if something will be overwritten
- Erzeugen "touch"
- Unterschied zw zwei dateien 
	- "diff file1 file2"
		- option -u output to sth else
		- returns string like nxm, nth considering n-th line, m-th line, and x in {(a)dd, (c)hange, (d)elete}
	- "sdiff file1 file2" - side by side
		- between lines: | differing lines, < line only in file1, > line only in file2
	- "vimdiff file1 file2" - highlight differences in vim
		- "ctrl-w w" go to next window
		- ":q" quit
		- ":qa" quit all
		- ":qa!"
		- "dp" diffput: puts changes under the cursor into the other file
                       making them identical (thus removing the diff).
		- "do" diffget: (o => obtain). The change under the cursor is replaced
                       by the content of the other file making them identical.
		- "]c" Jump to the next diff
		- "[c" Jump to the previous diff
		- ":vs otherFile" For vertical split
		- ":split otherFile" - or horizontal split
		- "ctrl+w ctrl+w" - Switch cursors to different split screen
		- ":diffthis" - Invoke "diff mode" in file
		- ":diffthis" - Switch to other file and invoke "diff mode"
		- ":diffoff" - To turn off "diff mode"
- "file file" displays the file-format
- "string file" returns human readable strings in a file
- "find" - good: "find --help"
	- "find dir" returns all files in this dir
	- "find dir -name file -print"
	- "find dir -maxdepth" maximal depth of subdirectories
	- "find dir -name pattern" e.g. pattern = "fileNumer*.txt" - need quotation to pass * as pattern - alt: \*
	- "find dir -iname pattern" ignores case
	- "find dir -ls"
	- "find dir -type f" -> f|d|l|p...
	- "find dir -mmin -10" modified within last 10 min
	- "find dir -mmin +10" modified over 10 min ago
	- "find dir -mtime -10" modified within last 10 days
	- "find dir -cmin -10"
	- "find dir -amin -10"
	- "find dir -size +2M" -> 2M 5k 3G, + over, - under
	- logical ops: -and -or -not -o | "find . -size -1M -and -name '*skype*'" - nest by parantheses \( and \)
	- "find dir -newer file"
	- "find dir -empty" search for empty files
	- "find dir \( -type d \( ! -readable \) -prune \) -or \( -iname 'ubuntu*.iso' -print \)  | ! > -not | -prune just consider those | -print just this part will be printed!
	- "find dir -perm 777"
	- "find dir -exec command {} \;"
		- e.g. command = file to execute
		- e.g. "chown tomoBones:bonesGroup"
		- placeholder: {] for each of all results
		- end command: /;
	- "find dir -user tomo" search for files with user "tomo"
	- "find dir -delete" in combination with other options - deletes all found files
- locate - "locate --help" - wildcard not working > need regular expressions
	- "locate" not real time but by index -> faster but maybe not actual | find looks on actual device
	- "locate filename" locates file in subdirectory
	- "locate -i filename" case insensitive
	- "locate -r file_number_(.*).jpg" regular expressions
	- "locate -r -b file_number_(.*).jpg" runs just over basenames - ie file names
	- "locate --statistics" considers data base with properties
	- "sudo updatedb"
- Editor "nano" or "pico"
	- "ctrl x" exit
	- ...

________________
<a name="bash_aliases"/>
### aliases:

- shortcuts > longer commands, often used commands
- "alias" list of set aliases 
- "alias <name>='<value>'"
- "unalias name" remove alias
- "unalias -a" remove all
- persistent alias > add to .bach_profile

________________
<a name="bash_directories"/>
### directories:
- "man hier" filesystem hierarchy
- Navigation durch Pseudoverzeichnisse:
	- absolute/relative Pfade beginnen mit "/" bzw ""
	- "." pwd
	- ".." Elternverzeichnis
	- "../.." Großelternverzeichnis
- Change Directory "cd"
	- ohne Argument > home-Verzeichnis
	- relative paths "..." - absolute paths "/..."
	- "cd -" previous path
- Make Directory "mkdir"
	- "mkdir -p" create path - nested directories "mkdir dir1/dir2/dir3"
- Remove Directory "rmdir"
	- alternatively: "rm -rf dir"
	- "rmdir -p" deletes empty paths/nested dirs
- Print Working Directory "pwd"
- List "ls"
	- "ls -l" long | option -a all - dot files | -h human readable
	- "ls -F" assigns in the end "/" for dir | "@" for link | "*" for Executables
	- "ls -t" list by time | -r reverse order  | -tr reverse time order
	- "ls -R" shows all directories recursively
	- "ls --color" colorize output
	- "ls -h" human readable - prints sizes with easy units easy to read
	- "ls -i" inode numbers - eg same inode for hard links
	- "ls *" list all subdirectories
- "tree" - of current dir
	- "tree -a" all (hidden) files
	- "tree -d" only directories
	- "tree -C" colorized output
- File "file"

________________
<a name="bash_links"/>
### links
- "ln target hardLinkName" hard link pointing to the same target 
	- same inode! 
	- only in one partition possible 
	- only for files possible, not directories
	- not easy to delete all associated hard links
- "ln -s target linkname" soft/symbolic link points to target - pointer
	- also to directories possible
	- no linkname > file behind targetName will be used locally

________________
<a name="bash_archives"/>
### archiving/compressing files
- tar archives 
	- "tar -cvf archive.tar file1 file2 …"
	- "tar -xvf archive.tar"
	- "tar -tf archive.tar" shows content
	- "tar -c" create mode - adds all files recursively!
	- "tar -x" extract mode
	- "tar -t" table of content
	- "tar -v" verbose diagnostic output
	- "tar -f" file - followed by the archive file name
	- "tar -p" while unpacking preserve permissions - not overwrite according "umask"
	- "tar -z" automatically invoke gzip to tarFile for de/compression
	- "tar -j" automatically invoke bzip2 to tarFile for de/compression
	- "tar -P" use absolute paths
	- for std-i/o use "-" instead of fileName
- cpio archives
- compressed archives .tar.gz or .tgz
	- gzip single files
		- "gzip file" compression
		- "gzip -n" compression intensity 1<=n<=9 default=9
		- "gzip -d file" decompression
		- "gunzip file.gz" decompression
		- "gzcat" concatenate compressed files
	- "zcat file .tar.gz | tar xvf -"
		- "zcat" wie "gunzip -dc" d decompression c std-output
		- good as pipeline: "zcat file.tar.gz | tar -xvf"
	- "bzip2"|"bunzip2" .bz2 slower, better compression text-files
		- "bzip2 source" compression
		- "bzip2 -z source" compression
		- "bzip2 -n source" compression - compression intensity 1<=n<=9 default=9
		- "bzip2 -v source" verbose
		- "bunzip2 source" decompression
		- "bzip2 -d source" decompression
	- "xz"|"unxz"
	- "du" estimates file usage
		- "du -k" display sizes in kilobytes
		- "du -h" display sizes in human readable format
		- "du -sh dir" size of directory dir

________________
<a name="bash_devices"/>
### device management:
- 	different file sytems: different purposes
- interface for applications /sys/devices/
	- kernel provides configuration info through sysfs
	- more foundational - since kernel 2.6 - extract infos about devices
	- view information, manage device - complete details about device
- 	api for users in /dev/ - udev system
	- 	devs as files called device nodes - ordinary file operations can be used - accessable to some programs (cat) - the way how kernel interacts with user space wrt devices - dev management running in user space - manages dev nodes in /dev - enables userspace to config/use devices - user processes can use these devices aka files - since kernel 2.6 - dynamically created - e.g. "/dev/null": throw data away
	- 	"ls -l /dev/" devices: block (b) | character (c) | pipes (p): like char devices, with another end of I/O stream instead of kernel driver | sockets (s): see later (chapter 10), not confuse with network sockets - major/minor device numbers iot identify device - name tells a lot about device - problem: name changes, if one device is removed
	- 	files: link driver <> hardware | where hw and kernel writes/picks up informations/data
	- 	eg: "/dev/block", "/dev/sda"	- show path in sysfs "udevadm info --query=all --name=/dev/sda"
	- old way to create dev file	"mknod /dev/sda1 b 8 2" - specify block dev, maj num 8, min num 2 - alternatively: use c or p - only useful for occansionally named pipe - new way: devtmpfs, udev
	- "ps -aux | grep udevd" -> driver, hardwaremount
	- "ps -aux | grep process_name" -> cpu usage of process_name
- common devices, naming convention
	- /dev/sd - scsi (small computer system interface) disk - e.g. hard disks, usb - list scsi devices: old command "lsscsi" - problem, when removing one disk (from sda, sdb, sdc), the others are renamed - hence: use UUIDs universally unique identifiers
	- /dev/sr - cd/dvd drives - optical storage drives - as scsi drives - write/rewrite: use /dev/sg0: generic scsi devices
	- /dev/hda, /dev/hdb, /dev/hdc, /dev/hdb - pata hard disks - sometimes sata
	- /dev/tty, /dev/pts - terminals - moving chars btw user process and i/o device - usually text output > screen - pseudoterminal: emulated terminals - 1st virtual console "/dev/tt1" - first pseudoterminal "/dev/pts/0" - "/dev/tty" device of controlling terminal of current process - change to other console: "chvt 1" as root
	- /dev/ttyS - serial ports - special terminal devices (rs-232)
	- /dev/lp0, /dev/lp1 - unidirectional parallel port devices - e.g. printer - better: print server such as CUPS - /dev/parport1, /dev/parport2 - bidirectional parallel ports
	- /dev/snd, /dev/dsp, /dev/audio, ... - audio devices
- "fd" display free disk space on specified filesystem
	- "fd -h" human readable
- "dd" copies data from/to file or stream in blocks of a fixed size
	- old unix command - uses options with "=" - working with block/character devices
	- e.g. "dd if=/dev/zero of=new_file bs=1024 count=1"
	- options:
		- "if=inputFile"
		- "of=outputFile"
		- "bs=size" blocksize
		- "ibs=size" input block size
		- "obs=size" output block size
		- "count=num" total number of blocks - where to stop
		- "skip=num" skips past first "num" blocks, does not copy them
- udev rules in "etc/udev/rules.d/*.rules"
- 	look for devices:
	- "udevadm" search for a device
	- "dmesg" prints last few kernel messages - see chapter 7.2 system loggin
	- check output of "mount" command
	- run "cat /proc/devices" see block/character devices currently driven by major/minor number
- tools to manage devices
	- "lsscsi" to list scsi devices - dev paths by sysfs (doesn't run on my linux manjaro)
	- "lsblk" to list block devices
	- "fdisk -l" check all devices
	- "diskutil list"
- formating a device:
	- "sudo diskutil eraseDisk FAT32 UNTITLED MBRFormat /dev/disk2"
	- "sudo diskutil eraseDisk EXFAT BOOT MBRFormat /dev/disk2"
	- "diskutil unmountDisk /dev/disk4"
	- "diskutil eject /dev/disk2"
	- "sudo diskutil eject /dev/disk4"
- disk imager: 
	- "dd if=<filename> of=<diskname>"
	- "sudo dd bs=1m if=2018-04-18-raspbian-stretch.img of=/dev/disk4 conv=sync"
- usb device list
	- "lsusb"
- bluetooth
	- "systemctl status bluetooth"
	- "bluetoothctl"


________________
<a name="bash_user_management"/>
### user management
- normal user "$" - root "#"
- cal | cal 2018
- date
	- "date" show system time
	- "date <MMDDHHmmYY>" set system time
- "hostname"
	- "hostnamectl set-hostname <domain.name>" set name of system
	- "hostname" show hostname
	- "hostname -s" show only hostname
	- "hostname -d" show only domain
- "who" | "w" who is logged in
	- "whoami" my username
- infos about me "id" | "whoami" | "ls -al" check who owns files
- "groups" 
	- "groups userName1 userName2"
- "id"- shows properties of (me as) user
	- "is userName"
	- "id -GnG" | 
- permission categories: ugoa -> user | group | other | all
- Change Modus: "chmod option fileName" - change rights for user/group/world wrt to rwx
	- user u | group g | others/world o | all a
	- set = | add + | take -
	- "chmod +x" | "chmod -r recursively in a directory" | "chmod go+w" | "chmod u=rwx,g+w,o-rx file" | "chmod u+w,g-x,o= dir"
	- "chmod -R" - recursively for all child directories
	- absolute: sum bits for each user/group/others r=4 w=2 x=1
	- eg "chmod 755 filename"
- "umask" - defines, what will NOT be set > subtracted from default value
	- "umask" shows actual umask - what will NOT be set - first numer related to special rights
	- "umask -S" more readable form what will actually be set
	- default permission: 777 for dir, 666 for files >>> new progs must make executable EXPLICITLY! eg "chmod a+x script"
	- good umask to subract: 022 or 002
	- check with "umask" | "umask -S"
	- "umask 007" set to different value
- "chown" - set new user:group for file or directory
	- "chown owner:group fileOrDirName"
	- "chown owner"
	- "chown :group"
- "chgrp groupName fileName" change group of a file
- "umask 777" changes std mode for files that this user creates
- "su" change user, see below
- "sudo command" run command as superuser
	- /etc/sudoers
	- "sudo -i" interactive session
	- "sudo !!" run last command with sudo
	- "sudo su -" start root session 
	- "sudo su - userName" start session with new userName - doesn't need sudo 
- change default mode in etc/login.defs UMASK
- users: "tail /etc/passwd"|"/etc/shaddow"|"/etc/group"|"/etc/skel"
- "useradd username"
	- "useradd -m newUser" create a home directory /home/newUser - copies .files from /etc/skel - need homeDir to show in login screen
	- alt: cp -r /etc/skel /home/newUser | sudo chown -R newUser:newUser /home/newUser
	- "useradd -d dirPath userName" directory path - eg for system user
	- "useradd -u 150 userName" sets UID to 150
	- "useradd -g 2222 userName" set primary group id to '2222'
	- "useradd -G UID1,groupName2 userName" add user to groups
	- "useradd -s /bin/bash" default shell
	- "useradd -c 'this is a commentary for this user'" add a commentary - shows in login screen
	- "useradd -r systemUserName" - add system user - automatic UID in default range (sth under 1000)
- "groupadd groupName"
	- "goupadd -g 2222 groupName" add group with certain GID
- "passwd" set password - need to set to show user in login screen
- "userdel"
- "groupdel"
- "usermod"
	- "usermod -L userName" lock password of user (with bang "!" in front of password)
	- "usermod -U userName" unlock password
	- "usermod -s /bin/bash userName" set shell
	- "usermod -c 'comment' userName" set comment
	- "usermod -G userGroup1,userGroup2,UID3 userName" add userName to userGroup | names and GIDs may mix
	- "usermod -g 10000 userName" primary group
- "newusers"
- "chage" - settings in /etc/shadow
	- "chage -l userName" show entries of 'userName' in /etc/shadow
	- "chage userName" set minPasswdChange, maxPasswdChage, lastPasswdChange, warningBeforeExpiration, deactivationAfterExpiration, expirationDate
- "adduser" - alternatively to "useradd" - change behavior in /etc/adduser.conf
- "deluser" - change behavior in /etc/deluser.conf
	- "deluser --remove-all-files userName" remove all files of userName
- "getent" - outputs data for users from passwd, group, shadow:
	- "getent passwd username"
	- "getent group username"
	- "getent shadow username"

________________
<a name="bash_packet_manager"/>
### package manager
- sources in "etc/apt/source.list - own sources in dir "etc/apt/source.list.d"
- ig with sudo rights 
- packages: special archives
- debian
	- "apt-get upgrade"
	- "apt-get update"
	- "apt-get install prog1"
	- "apt-cache search prog1"
	- "add-apt-repository ppa:cassou/emacs"
	- "apt update" - download possible updates - update lists of packet versions
	- "apt upgrade" - install new versions of packages - does not upgrade if depending on not yet installed packages 
	- "apt dist-upgrade" installs with all depending software
	- "apt search KEYWORD"
- install single packages:
	- "dpkg -i PATH/something.deb" - will create new list in "etc/apt/source.list.d" - also updated under "apt update" etc - in general add key by "apt-key" - ig must trust package sources
	- "dpkg -l" shows list of installed packages
	- "dpkg -l | grep vim"
- ubuntu
	- add package source ppa - must trust source
- arch linux
	- pacman
- red hat
	- yum
	- "yum install epel-release" add package source
	- "yum install PACKAGE" - ig look if public key is authentic
	- "yum install URL"
	- "yum remove PACKAGE"
	- "yum downgrade PACKAGE" to version before
	- "yum search KEYNAME"
- install by source code - README!! - eg
	- download archive
	- "./conigure" run script
	- "make" compile - sometimes need packages make|gcc|g++|automake|cmake
	- now can be run in this folder - alternatively: install in syste >>>
	- "sudo make install" maybe collide with package administration (apt)
________________
<a name="bash_processes"/>
### processes
- theory
	- two aspects:
		- address space -> space in memory | virtual
		- kernel data structure dealing with process -> owners | priorities/niceness | resources | files using | signal mask
	- process ids PID -> "top" | PID=1 init process | mother of all processes (by forking)
	- basic lifecycle: init | parent process -> fork/clone -> child is parent | system call -> exit with some value
	- signals: processes communicating | kernel communicating with process | eg bad errors, ia with devices, process lifecycle, network, ... -> posix standard | signals -> handler functions: certain code | signal numbers, eg SIGKILL 09 | http://www.man7.org/linux/man-pages/man7/signal.7.html
	- communicating over files...
	- 4 processes states in cpu: running (R) | sleeping/waitingForNeedsFromParents (S, I) | zombie/finished/waiting2ReturnValues (Z) | stocked/waiting2BeResumed (T)
	- processes mounted into proc/ filesystem | this is what ps/top/htop uses | directories are PIDs | eg init proc/1/ | don't look to long into the matrix!
- "ps" - shows status STAT, see above
	- "ps -x" show all of your running processes
	- "ps -ax" show ALL processes of system
	- "ps -u" more detailed info of processes
		- eg "ps $$ -u" for current shell PID
	- "ps -ef"
	- "ps -w" full command names
	- add the PID for specific processes
	- use | less or | more
- "top" "top -c" monitoring processes
- "htop" alternatively
- "kill -l" list of signal processes - also in "man 7 signal"
- "kill -option pidnumber"
	- no option > default "kill -TERM" terminate > CTRL-c
	- "kill -n" n signal number or signal name
	- "kill -STOP" freeze process
	- "kill -CONT" continue
	- "kill -KILL" or "kill -9" hard, erzwungener sofortiger Abbruch ohne verzögerung
- job control:
	- "CTRL-d": Stoppt momentanen Input im Terminal
	- "CTRL-z": background > foreground "fg"
- Abbruch/terminierung Programm/Prozess in terminal unabh von Input/Output: "CTRL-c" - wie kill -TERM
- "CTRL-z" + fg bg: bring to fore-/background
- put prozess in bg by adding "command &"
	- problem: cannot access std I/O - unexpected output on screen > best: redirect ouput
- "killall"
- "pkill -u user"
- "man 7 signal"
- state of processes: active, waiting/sleeping, zombie, stopped
- "nice -n 6 program" launch with certain niceness | highest prio -20, lowest prio 19
- "renice -n -5 PIDNumber" 
- "df -ah" proc file system > /proc - (d)isk space (f)ree
- "strace" attach/trace system calls and signals

### schedule processes
- "crontab -l" list for me as user
- "sudo /etc/crontab" system wide
- "crontab -e" edit with default editor
- entries: minute|hour|dayOfMonth|monthOfYear|weekday|command
- "crontab -e -u user"
- configured in "/var/spool/cron/crontabs/user" resp "/etc/cron.d"

________________
<a name="bash_editors"/>
### editors:
- change default editor "sudo update-alternatives --config editor"
- nano
	- Strg-G - help
	- Strg-X - quit
	- Alt-A - select
	- Strg-K - cut
	- Strg-U - paste
	- Alt-U - undo
	- Strg-W - find
	- /etc/nanorc: "set regexp" globally activate regular expressions
	- ~/.nanord:  "set regexp" locally activate regular expressions
	- synthax highlighting: 
		- /usr/share/nano/python.nanorc ...
		- linked over /etc/nanorc: "include '/usr/share/nano/*.nanorc'"
		- "Alt-Y" de/activate
	
- vi - vim - view
	- "set -o vi" terminal in vim mode
	- "~/.vimrc" - edit "set ..."
	- "/usr/share/vim/vim84/colors" color schemes, eg darkblue...
	- run tutor: "vimtutor"
	- https://www.washington.edu/computing/unix/vi.html
	- 3 modes: command-mode | insert-mode | last-line-mode 
	- insert mode: by pressing "i"/"a" and leaving by "esc"
	- command mode
		- enter command-mode: "esc"
		- INSERTING TEXT
			- i         insert text left of cursor
			- I         insert text at beginning of the line
			- a         append text right of cursor
			- A         append text at end of the line
		- MOVING THE CURSOR
			- h         left one space
			- j         down one line
			- k         up one line
			- l         right one space
		- BASIC EDITING
			- x         delete character
			- nx        delete n characters
			- X         delete character before cursor
			- nX        delete n characters before cursor
			- dw        delete word
			- ndw       delete n words
			- dd        delete line
			- ndd       delete n lines
			- D         delete characters from cursor to end of line
			- r         replace character under cursor (ready for insertion)
			- cw        replace a word (ready for insertion)
			- ncw       replace n words (ready for insertion)
			- C         change text from cursor to end of line (ready for insertion)
			- o         insert blank line below cursor (ready for insertion)
			- O         insert blank line above cursor (ready for insertion)
			- J         join succeeding line to current cursor line
			- nJ        join n succeeding lines to current cursor line
			- u         undo last change
			- U         restore current line
			- ctrl+r    redo last change
			- ~         switching to lower/upper case
		- REPEATING COMMANDS
			- 5k        move up line 5 times
			- 80i<text><esc> insert text 80 times
			- 80i_<esc> insert 80 '_' characters
		- SELECT TEXT - VISUAL MODE
			- <ctrl>v
			- navigate like in normal mode
		- COPY PASTE
			- yy        yank/copy current line
			- yw        yank/copy current word
			- y3w       yank/copy 3 words
			- y<position> yank/copy the position
			- p         paste most recent deleted/yanked text after line
			- P         paste most recent deleted/yanked text before line
		- UNDO/REDO
			- u         undo
			- <ctrl>R   redo
		- SEARCHING/REPLACING
			- /<pattern> start fwd search
			- ?<pattern> start bckwd search
			- n          next match
			- N          previous match
			- :s/oldstring/newstring/ # replaces first
			- :s/oldstring/newstring/g # greedy: replaces all
			- :s/oldstring/newstring/gc # greedy: replaces all after asking
			- :%s/oldstring/newstring # replaces in all lines
			- :n,ms/oldstring/newstring # replace in between lin n and m
		- AUTOCOMPLETE
			- <ctrl>N    autocomplete with words from file
		- MACROS
      			- qn          start recording macro number n
      			- q           quit macro recording
 			- @n          run macro number n
 			- m@n         run macro number n m times
		- MOVING AROUND IN A FILE
			- $            to end of line
			- w            forward word by word
			- e            forward word by word | end of the word
			- b            backward word by word
			- W            forward word by word - special characters
			- E            forward word by word | end of the word - special characters
			- B            backward word by word - special characters
			- ^            to beginning of the line
			- 0 (zero)     to beginning of line
			- H            to top line of screen
			- M            to middle line of screen
			- L            to last line of screen
			- G            to last line of file
			- 1G           to first line of file
			- nG           to nth line of file
			- gg           to first line of file
			- <Control>f   scroll forward one screen
			- <Control>b   scroll backward one screen
			- <Control>d   scroll down one-half screen
			- <Control>u   scroll up one-half screen
			- <Control>o   curser back before jump
			- <Control>i   curser forward/repeat next jump
			- n            repeat last search in same direction
			- N            repeat last search in opposite direction
			- fn           moving forward to next letter "n"
			- Fn           moving backward to next letter "n"
			- tn           moving forward before next letter "n"
			- Tn           moving backward before next letter "n"
			- %            highlights parantheses around curser
		- CLOSING AND SAVING A FILE
			- ZZ           save file and then quit
			- :w           save file
			- :w!          forces file to be saved
			- :q           quit file
			- :q!          discard changes and quit file
			- :wq!         write and quit
			- :x           same as :wq
		- commands
			- :n           position curser at line n
			- :$           position curser at last line
			- :set nu      turn on line numbering
			- :set nonu    turn off line numbering
			- :set nocp    turn off compatible mode
			- :set ic - :set ignorecase - case sensitive
			- :set noic - :set noignorecase 
			- :syntax on/off  (de)activate syntax highlighting
			- :colorscheme darkblue - change color scheme
			- :help        get help
			- :help :[subcommand] get help
			- !<command>   run bash command in vim
			- <ctrl>d      runs suggestions for typed characters
	- visual mode
		- navigation: as in insert mode
- emacs
	- "emacs file"
	- C-<char> holding ctrl key - M-<char> holding alt/esc key
	- "C-h t" build-in tutorial
	- basics
		- "C-h" Help
		- "C-x C-c" exit
		- "C-x C-s" save file
		- "C-x C-f" open (new) file - only ENTER opens folder
		- "C-h k <key>" describe key
		- "ALt-x" > "front-lock-mode" activates syntax highlightening
	- navigation
		- "C-p" previous line
		- "C-n" next line
		- "C-b" backward one character
		- "C-f" forward one character
		- "M-f" forward one word
		- "M-b" backward one word
		- "M-<" beginning of document
		- "M->" end of document
		- "C-a" beginning of line
		- "C-e" end of line
	- deletion
		- "C-d" delete character
		- "M-d" delete word
	- copy/paste/undo
		- "C-k" kill/cut
		- "C-y" yank/paste
		- "C-x u" undo
	- searching
		- "C-s" start forward search - next match
		- "C-r" start backward search
	- repeat command
		- "C-u N <command>" repeat command N times
	- select
		- "C-space" start selection
		- "M-w" copy
		- "C-q" cut
- graphical editors:
	- emacs
	- gedit - gnome
	- gvim
	- kedit - kde
	- kate - source code editor

________________
<a name="bash_shutdown"/>
### shutdown
- "shutdown -r now" restart | "shutdown -h +60" hold, after an hour
- "poweroff"
- "init 0" poweroff
- "init 6" restart

________________
<a name="bash_x-screens"/>
### X-Screens:
- Gnome
- Xfce
- KDE
- Cinnemon
- MATE
- LXDE

________________
<a name="bash_os">
### operating system
- https://www.youtube.com/watch?v=rLgRkjM7amo
- "uname -a" print info: operating system name, kernel version, system's host name, ...
	- "uname -v" version
	- "uname -r" release
- "lsof" list info about open files by processes
	- "lsof | head" most important: COMMAND | PID | ... | NAME
	- "lsof fileName.dat" lists all processes for special file
	- "lsof -u userName" lists all files open by a user
	- "lsof -p 1103" find all files for special process with PID
	- "lsof -i :80" search for interfaces (sockets) with port 80
	- "lsof -i tcp"
	- "lsof -i udp"
	- "lsof -i -n -P" search ip sockets - resolve dns - port num
	- "lsof -Pnl -i4"
- "systemd": managing services, services/packages that were installed, start their config files - may be since they have stucked - enable/disable them (starting at boot time) - sytemd: modern init system - "unit" = daemon
	- "systemctl" alternatively manage services
		- "systemctl <command> <unit name>"
		- "systemctl list-units"
		- "systemctl list-units | grep .system"
		- "systemctl status mysql"
		- "systemctl start nginx php5-fpm monit" list of programs
		- "systemctl enable mysql nginx php5-fpm monit"
		- "systemctl --failed"
		- "systemctl is-enabled <unit name>"
		- "systemctl reboot"
		- "systemctl poweroff"
		- "systemctl suspend"
		- "sudo systemctl start NetworkManager.service" run the network manager
		- "sudo systemctl enable NetworkManager.service" enable network manager while rebooting
		- commands: en/disable, start/stop, reload, restart, status
		- "systemctl status bluetooth" - alternatively "/etc/init.d/bluetooth status"
	- "journalctl" alternatively manage/parse logs - handles loggings - eg boot logs
		- "journalctl -xn"
		- "journalctl -u <unit>" logging from special unit
		- "journalctl -b" loggings from boot
		- "journalctl -f"
		- "journalctl --since "10 min ago""
	- older version: "service <service> <action>"
			- eg "service mysql status"
			- eg "service nginx enable"
	- "lscpu" info about the cpu/processor
	- "neofetch" system info script

________________
<a name="bash_network"/>
### network
- 5 layers: 
	- 1 physical: 
	- 2 datalink: ethernet, mac, 1<>2: arp
	- 3 network: ip
	- 4 transport: tcp, udp
	- 5 yours: mail, html, ftp, ...
- "telnet"
	- The telnet command is used for interactive communication with another host using the TELNET protocol. It begins in command mode, where it prints a telnet command prompt ("telnet>") >>> eg https://www.computerhope.com/unix/utelnet.htm
	- "telnet google.com 80" opens http-session
	- "telnet mailServerName 25" opens smtp session
	- "telnet mailServerName 110" opens pop3 session
	- send request/messages in prompt, press 2x ENTER
- ssh secure shell
	- "ssh <serverName>"
	- "sudo ssh user@192.168.178.196"
- ssh install local server
	- "sudo apt-get install openssh-server"
	- "sudo nano /etc/ssh/sshd_config" make it safer
	- "sudo systemctl restart ssh" restart ssh server
- "scp <sourceFile> <servername:/dirName/>" secure copy
- "sftp <serverName>" secure file transfer protocol
	- put "l" before commands -> talk to local system
	- "put <fileName>" copies local file to server
	- "get <fileName>" copies server file to local folder
	- "quit" quit sftp client
- ftp file transfer protocol - not secure >>> not available on many systems
- "mail -s "subject" tomo < test.txt"
- tmux - multiplex sessions
		- "tmux" starts tmux server
		- "tmux list-sessions"
		- "tmux new-session -s backupsession" new session
		- "tmux attach -t backupsession" attach to existing session
	- commands in sessions
		- "ctrl+b <command>" command in tmux
		- "ctrl+b c" create a new window
		- "ctrl+b ," rename window
		- "ctrl+b p" previous window | "ctrl+b " next window
		- "ctrl+b w" menu list of open windows - scroll up/down
		- "ctrl+b %" splits vertically
		- "ctrl+b :split-window" splits horizontally
		- "exit" closes window
		- "ctrl+b d" detatch from session, letting it run
- "curl" - tool to transfer data through the network
	- "curl <url>:<portnumber>" - GET html from url - default portnumber: 80 for http request
	- "curl -i <url>:<portnumber>" - more informations
	- "curl -d "postAppendix" <url>:<portnumber>" - POST method with postAppendix
	- "curl -o example.html https://www.google.de" - output in file with filename example.html
	- "curl -O https://www.google.de" - output in file
	- "curl --ftp-ssl -u gsi_7640_1gsi-7640-1 ftp://thomasvogg.de" - explicit ftps connection with (u)ser login
- "ss" socket stat - former "netstat" datalink statistics
	- "ss -r"
	- "ss -nr"
	- "ss -ta"
	- "ss -i" Ethernet statistics with errors
	- "ss -l" listening connections on this host
	- "ss -tupln" tcp udp program listeningports numerically
	- "ss -tupac"
- "netstat" show network connections, routing tables, statistics, masked connections
	- "netstat -r" show routing table
- "ifconfig" > use "ip" config nw interface param
	- "ifconfig -a" list all
	- "ifconfig en0" configuration of en0
	- "ifconfig ifname up" enable interface
	- "ifconfig ifname down" disable interface
	- "ifconfig hw ether de:ad:be:ef:c0:fe" set new mac address
	- "ifconfig -a" list all
	- similar to "ipconfig /all"
	- maybe obsolete -> "ip a"
- "iwconfig" wireless configuration
	- "sudo iwconfig wlan0 essid amHarras" connect to access point with essid amHarras
	- "sudo iwconfig wlan0 essid amHarras key password"
	- "sudo iwconfig wlan0 essid amHarras key s:password" s: ascii text
	- "sudo iwconfig wlan0 mode managed" set wifi if to managed mode
	- "sudo iwconfig wlan0 channel 1 essid myadhocnetwork mode ad-hoc" set to ad-hoc mode
	- supported modes: monitor, managed, ad-hoc (peer2peer), mesh, repeater
- "iwlist" get more info from wireless interface
	- "sudo iwlist wlan0 s" scan access pts
	- "sudo iwlist wlan0 scanning | less" scan access pts
	- "iwlist wlan0 rate"
	- "iwlist wlan0 freq"
	- 'sudo iwlist wlan0 scan | egrep "(ESSID|IEEE)"'
- "dhclient" dynamic host config protocol client
	- "dhclient wlan0" give this interface an ip address
- "ip" - show/manipulate routing/devices/policy routing/tunnels - brew iproute2mac
	- "ip addr" Shows addresses assigned to all network interfaces
	- "ip neigh" Shows the current neighbour table in kernel - pendant for arp: nd neighbour discovery on 2-datalink layer
	- "ip link" hardware configuration of network
	- "ip link set x up" Bring up interface x
	- "ip link set x down" Bring down interface x
	- "ip route" Show table routes
- "ping" network reachability
	- "ping ipAdress" tests thenetwork layer >>> ip protocol
- "traceroute" print route, packets take to network host
- "dig" dns lookup utility
- "lsof" list info about open files by processes
	- "lsof -i -n -P" search for ip sockets - resolve dns - port number
	- "lsof -Pnl -i4"
- "nmcli" - network manager 
	- "nmcli dev wifi" check all essid's
	- "nmcli dev wifi connect ESSID_NAME password ESSID_PASSWORD" connect with essid
	- "nmcli nm" show states
	- "nmcli nm enable false" nm parameter for network management states, all connections disconnected
	- "nmcli c" check existing connections
	- "nmcli c down id NAME"
	- "nmcli c ip id NAME"
	- "nmcli c delete id NAME
	- connections stored in /etc/NetworkManager/system-connections
- "sudo /etc/init.d/networking restart"
- "arp" adress resolution protocol - lists ethernet/MAC adresses in the broadcast network: links MAC to IPv4-Adresses
	- "arp IPv4Adr" check entry for special ipAdress
	- option -a: local arp cache/list that my system communicated with
	- option -n: without hostnames
	- option -a IPv4Adr: request MAC adress for ip adress on datalink-layer (2)
	- option -s IPv4Adr MAC: add entry into cache
	- pendent for IPv6 Adresses: nd neighbour discovery- see "ip neigh" or "ndp"
- "ndp" unix - neighbour discovery: arp pendent for IPv6y
- "nslookup" dns anfragen verschicken
- "nmap" network mapper - analysis, security and port scanner - ident hosts, version/os detection...
	- typically check ports and connect with netcat
	- "nmap --version"
	- "nmap 192.168.178.138" port scan specific ip addresses
	- "nmap -v 192.168.178.138" verbous - more infos
	- "nmap 192.168.178.138,1" for two ip addresses
	- "nmap 192.168.178.1-138" for a range of ip's
	- "nmap 192.168.178.1/24" for subnets for a range of ip's - also use "*" asterix
	- "nmap 192.168.178.1/24 --exclude 192.168.178.184" exclude one/some ip adresses 
	- "nmap -iL ipList.txt" read ip addresses from file
	- "nmap 192.168.178.1/24 --excludefile ipList.txt" exclude ip addresses from file
	- "nmap -iR 3" search/scan for 3 random targets
	- "nmap -A 192.168.178.138" aggressive scan option - options: -o -sC --traceroute
	- scan with different protocols: https://nmap.org/man/de/man-host-discovery.html 
	- "nmap -PN 190.168.178.72" deep scan - search for sleeping targets
	- "nmap -sP 190.168.178.72" simple ping scan - just little info about targets
	- "nmap -PS 190.168.178.72" scan ports blocked for reg icmp ping (firewall)  > use a syn packet
	- "nmap -PO 190.168.178.72" ip protocol ping
	- "nmap -PO1,2,4 190.168.178.72" ip protocol ping with several protocols
	- "nmap -pR 190.168.178.72" arp protocol - over local network
	- "nmap --traceroute 190.168.178.72"
	- "nmap -R 190.168.178.72" reversed DNS resolution 
- "tcpdump" packet sniffer
	- "tcpdump -D" check interfaces
	- "tcpdump -i wlan0" check packets over interface wlan0
	- "tcpdump -i wlan0 port 22" check packets over interface wlan0, port 22
	- "tcpdump -i any" check packets over all interfaces
	- "tcpdump -i any -c5" capture 5 packet
	- "tcpdump -i any -c5 -n" numbers instead of names - no dns requestest on top!
	- "tcpdump -i any -c5 -n -s96" size 96 bytes - default: 65525 bytes max tcp packet size
	- "tcpdump -i any -c5 -n -s96 -w capture.pcap" write to file capture.pcap
	- "tcpdump -i any -c5 -n -s96 -w capture.pcap -v" verbose mode
	- "tcpdump -i any -c5 -n -s96 -vv" more verbose details: -vv -vvv
	- "tcpdump -i any -c5 -n -s96 -q" quite mode - just necessary infos
	- "tcpdump -i any -c5 -n -s96 -t" adding a time stamp
	- "tcpdump -i any -c5 -n -s96 -ttt" shows time difference btw packets
	- "tcpdump -i any -c5 -n -s96 -ttttt" shows time since first packet
	- "tcpdump -n -r capture.pcap" reading a capture file
	- "tcpdump -c10 -A" more details about packets
	- "tcpdump -XX -i wlan0" more details about packets in hex
	filters:
	- "tcpdump -i eth1 -n host 10.0.0.3 -c5" only to/from ip 10.0.0.3
	- "tcpdump -i eth1 -n src host 10.0.0.3 -c5" only from ip 10.0.0.3
	- "tcpdump -i eth1 -n dst host 10.0.0.3 -c5" only to ip 10.0.0.3
	- "tcpdump -i eth1 -n host 10.0.0.1 and host 10.0.0.3-c5" logic ops: and/or/not
	- "tcpdump -i eth1 -n host 10.0.0.1 and port 80" port filter
	- "tcpdump -i eth1 -n "host 10.0.0.1 and (port 80 or port 443)""
	- "tcpdump -i eth0 -n -c100 "src net 192.168.0.0/16 and not dst net 192.168.0.0/16""
	- "tcpdump -i eth0 ether host 28:16:2e:1f:25:49 -n -c10" filter mac adress
	- "tcpdump -i eth0 ether host 28:16:2e:1f:25:49 -n -c10 -e" make mac adresses visible: -e
	- "tcpdump -i any ip6" make ip6 visible
- "nc" netcat - transfer data via tcp/udp protocols - install "ncat" - connecting to ports
	- "nc -h" all options of netcat
	- "nc -l -p 31337" listener on port 31337
	- "nc 10.73.31.145 31337" connect as client with listener: ip 10.73.31.145, port 31337
- "wget" network downloader
	- "wget --help"
	- "wget https://www.hausarzt-altenbach.de" downloads html file
- "aircrack-ng" - 802.11 WEP / WPA-PSK key cracker | a lot more
	- "airmon-ng" list wifi interfaces
	- "airmon-ng start wlan0" start wifi interface in monitor mode
	- "airmon-ng stop wlan0mon" stop wifi interface in monitor mode
	- "iwconfig wlan0mon" check mode of wifi interface
	- "airodump-ng wlan0mon" get frame traffic over wifi interface
	- "airdriver-ng loaded"g
	- "iw phy phy1 info" shows all supported modes for wifi if "phy1"
	- "aireplay-ng" inject wireless frames 
	- "aireplay-ng -9 mon0" test mode for injections
	- "airbase-ng -c 11 -e fakeTomo mon0" mimic wireless interface
- "mdk3" wireless attack tool for IEEE 802.11
	- "mdk3 mon0 b -f ssid.list -g -a -c 11" beacon flood mode on channel 11
- "macchanger" change mac addresses
_______
<a name="bash_printer"/>
### printer
- "cups" common unix printer system

		lpc status
		lpq
		
		lpc status HP_LaserJet_4100_Series
		lpstat -P HP_LaserJet_4100_Series
		lpstat -pHP_LaserJet_4100_Series
		
		lpr test.pdf
		
		cancel HP_LaserJet_4100_Series-1234 # remove one
		cancel -a HP_LaserJet_4100_Series
		
		cupsenable HP_LaserJet_4100_Series
		cupsdisable HP_LaserJet_4100_Series
		
		http://kb.eclipseinc.com/kb/how-do-i-manage-linux-print-queues/

________________
<a name="bash_raspi"/>
### raspberry pi
- pinout
- setup vnc
	- sudo apt-get install realvnc-vnc-server realvnc-vnc-viewer
	- sudo raspi-config >>> interface options >>> vnc=yes
- setup web server
	-  sudo apt-get install apache2 -y
	-  sudo apt install mariadb-server
	-  sudo apt-get install php5
	-  sudo apt-get install php5-mysql
-  bka
	- "sudo iwlist wlan0 scan"
	- "sudo vim /etc/wpa_supplicant/wpa_supplicant.conf" ->

			network={
		    ssid="HomeOneSSID"
		    psk="passwordOne"
		    priority=1
		    id_str="homeOne"
			}
			network={
			    ssid="HomeTwoSSID"
			    psk="passwordTwo"
			    priority=2
			    id_str="homeTwo"
			}
			
	- "wpa_cli -i wlan0 reconfigure"
	- "ifconfig wlan0"
- install x server on headless system
	- "sudo apt-get install lxde lxde_core lxterminal lxappearance" 
	- "apt install lightdm" display manager
	- "apt install xinit" for startx command


________________
<a name="bash_special_functions"/>
### special functions

















________________________________
<a name="python"/>
# python

- alle Aufgaben in "Eine praktische Einführung in Bash und Python" mal durchführen

________________
<a name="python_installation"/>
### installation
- package: anaconda - alternatively miniconda - small version
- jupyter notebook:
	- shift enter: safe and run
	- command mode in a cell: esc-key
		- change cells by arrow-keys
		- use keyboard shortcuts
	- different cell types | important: code/markdown
	- keyboard shortcuts: click keyboard image
	- export -> pdf, latex
	- markdown formatting, include latex formulars $...$ or one line $$...$$
	- widgets: https://ipywidgets.readthedocs.io/en/stable/examples/Widget%20List.html#Complete-list
- ide: pyCharme >>> https://www.jetbrains.com/pycharm/
	- create python programs (packages) >>> udemy python bootcamp - chapter 123

________________
<a name="python_jupyter_notebook"/>
### jupyter notebook

shortcuts: type "h"

In this article, I’ll show you my favorite ones. Note that the shortcuts are for Windows and Linux users. Anyway, for the Mac users they’re different buttons for Ctrl, Shift, and Alt:

- Ctrl: command key ⌘
- Shift: Shift ⇧
- Alt: option ⌥

First, we need to know that they are 2 modes in the Jypyter Notebook App: command mode and edit mode. I’ll start with the shortcuts shared between the two modes. Shortcuts in the both modes:

- Shift + Enter run the current cell, select below
- Ctrl + Enter run selected cells
- Alt + Enter run the current cell, insert below
- Ctrl + S save and checkpoint

While in command mode (press Esc to activate): Enter take you into edit mode

- H show all shortcuts
- Up select cell above
- Down select cell below
- Shift + Up extend selected cells above
- Shift + Down extend selected cells below
- A insert cell above
- B insert cell below
- X cut selected cells
- C copy selected cells
- V paste cells below
- Shift + V paste cells above
- D, D (press the key twice) delete selected cells
- Z undo cell deletion
- S Save and Checkpoint
- Y change the cell type to Code
- M change the cell type to Markdown
- P open the command palette. 

This dialog helps you run any command by name. It’s really useful if you don’t know some shortcut or when you don’t have a shortcut for the wanted command. Command Palette:

- Shift + Space scroll notebook up
- Space scroll notebook down

While in edit mode (pressEnter to activate) Esc take you into command mode Tab code completion or indent

- Shift + Tab tooltip
- Ctrl + ] indent
- Ctrl + [ dedent
- Ctrl + A select all
- Ctrl + Z undo
- Ctrl + Shift + Z or Ctrl + Y redo
- Ctrl + Home go to cell start
- Ctrl + End go to cell end
- Ctrl + Left go one word left
- Ctrl + Right go one word right
- Ctrl + Shift + P open the command palette
- Down move cursor down
- Up move cursor up

These are the shortcuts I use in my daily work. If you still need something that is not mentioned here you can find it in the keyboard shortcuts dialog (H). You can also edit existing or add more shortcuts from the Help > Edit Keyboard Shortcuts link in the menu bar. Clicking the link will open a dialog. At the bottom of it there are rules for adding or editing shortcuts. You need to use hyphens - to represent keys that should be pressed at the same time. For example, I added a shortcut Ctrl-R for the restart kernel and run all cells command.

________________
<a name="python_basics"/>
### basic ideas
- distinction: statements, expressions
- commentaries: starting with "#"
- be mindful: does action MANIPULATE object or RETURN a new object?
- load python prog into shell: "python -i prog.py" >>> test python programs directly
- variables types
	- instruction: asigns new values to variables, new state of program/system
	- expression: state is not altered, often return booleans or string representation
- dir(type) lists all functions to be applied with this type
- help(type.method) eg help(list.append)
- run programs in terminal: "/anaconda/bin/python.app program.py"
- alternatively: python program.py
- windows eingabeaufforderung: "C:\ProgramData\Anaconda3\python.exe program.py"
- characters:
	- ascii: 7bit | basic characters and control characters
	- iso 8859-1: 8bit | more characters | for german/french
	- iso 8850-11: 8bit | completes ascii chart
	- utf-8: 8 or 32 bit presentations | nearly all characters supported
- terminal: install packages:
	- "python3 get-pip.py"
	- "pip3 install --upgrade pip"
	- "pip3 install numpy" -> use numpy arrays!


________________
<a name="python_variables"/>
### variables types
- "type()" get type of a variable
- bool: True, False
	- add booleans: False+False=0, True+False=1, True+True=2
- numbers
	- integers (int)
	- long integers (long int)
	- floatingpoint number (float)
	- complex number (complex)
- example:
	- 12
	- 3.141
	- 4.23E-5 (float-representation)
	- 0xFE (hexadecimale representation)
	- 3/4 (nominator/denominator)
	- 2** 100
	- 1j + 3 (complex number).
- strings
	- "Test" - 'Test' - with ticks in it: '"Test"' - '\'Test\''
	- strings with many lines: """Hello this has many lines"""
	- slicing: only chars 3-5: string[3:5] | every 2ed char: string[::2]
	- strings are inmutable
	- cancatinate strings with "+"
- Variable Names
	- have to begin with letters or subline "_"
	- types of variables are dynamically generated, not with program-execution -  don't have to be declared - derives type with the first allocation
- Conversion
	- int() string ec >>> integer
	- str() number ec >>> string
	- float() >>> floating point number
- two kinds of variables:
	- primitive variables: boolean, integers, strings
	- datastructure/datastructures: lists, dictionaries

________________
<a name="python_operators"/>
### operators
- "+"|"-"
	- numbers: 2+3 = 5
	- strings: '2' + '3' = '23'
	- arrays: [1,2,3] + [4,5] = [1,2,3,4,5] (no "-" operation possible)
- "*"|"**"
	- numbers: multiplication/exponentiation
	- strings: mulitply a string
	- array: multiply an (sorted) array
- "/"|"//"|"%"
	- numbers: division (with rest) | division (w/o rest) | rest/modulo
	- strings: 
- "<" | ">" | ">=" | "<="
	- numbers: compares orders
	- strings: lexicographic
- "==" | "!=": equal values | Also works on strings
- "is" | "is not": equal objects
- "&" | "|" | "^" | "˜": operations on booleans (tilde = ˜ATL SHIFT 8) bitwise "and" | "or" | "exclusive or" | "not" | negation
- other syntax: "and" | "or" | "not"
- ">>" | "<<": shift X to the right|left by Y bits
- logical "X and Y" | "X or Y" | "not X" | "X in Y" | "X not in Y": last two >>> eg in arrays
- indention depth >>> essential syntax in python >>> replaces brakets
	- linked commands need same depth

________________
<a name="python_special_functions"/>
### functions
	- print(<string>) output to terminal
		- print('Das ist %s, er ist $d Jahre alt' % (name, age)) insert variables
	- age = input("Your age please:") # input from keyboard
	- random.randint(n,m)

________________
<a name="python_flow_control"/>
### flow of control
- block is just indented - no brackets
- if (elif and else optional)

		if <condition1>:
			<expression1>
		elif <condition2>:
			<expression2>
		else:
			<expression3>
			
- while (else optional)

 		while <condition>:
			<expression1>
		else:
			<expression2>
			
- for (else optional) with seq

		for <variable> in <sequence>: # learn about sequences later,  eg ranges
			<expression1>
		else:
			<expression2>
			
		for index, <variable> in enumerate(<sequence>) : # use the index as well
			<expression>
			
		for x in range(n) # ordinary iteration n times

- for (else optional) with dict

		for <key> in <dict>: #learn about dict later
			<expression>
			
		for <key>, <value> in <dict>.items()
			<expression>

- for (else optional) with tuples in lists

		mypairs = [(1,2),(3,4),(5,6)]
		for (tup_1, tup_2) in mypairs: #learn about lists later
			<expression>

- "break" abort flow of while loop
- "continue" get back to head
- conditional instruction
			
		<expression1> if <condition> else <exprexsion2>	
		
- iterables (object, can return an iterator, eg list/string/...)
- iterators (obj, keeps state, produces next value when calling next())

		flash = ['jay garrick', 'barry allen', 'wally west', 'bart allen']
		# Create an iterator a list (string)
		superspeed = iter(flash)
		# Print each item from the iterator
		print(next(superspeed))
		print(next(superspeed))
		print(next(superspeed))
		print(next(superspeed))
		
- enumerate() - returns an enumerate object: produces a sequence of tuples, and each of the tuples is an index-value pair.

		# Create a list of strings: mutants
		mutants = ['charles xavier','bobby drake','kurt wagner','max eisenhardt','kitty pryde']
		
		# Create a list of tuples: mutant_list
		mutant_list = list(enumerate(mutants))
		
		# Print the list of tuples
		print(mutant_list)
		
		# Unpack and print the tuple pairs
		for index1, value1 in enumerate(mutants):
		    print(index1, value1)
		
		# Change the start index
		for index2, value2 in enumerate(mutants, start=1):
		    print(index2, value2)

- zip

		# Create a list of tuples: mutant_data
		mutant_data = list(zip(mutants, aliases, powers))
		
		# Print the list of tuples
		print(mutant_data)
		
		# Create a zip object using the three lists: mutant_zip
		mutant_zip = zip(mutants, aliases, powers)
		
		# Print the zip object
		print(mutant_zip)
		
		# Unpack the zip object and print the tuple values
		for value1, value2, value3 in mutant_zip:
		    print(value1, value2, value3)
		    
		# Create a zip object from mutants and powers: z1
		z1 = zip(mutants, powers)
		
		# Print the tuples in z1 by unpacking with *
		print(*z1)
		
		# Re-create a zip object from mutants and powers: z1
		z1 = zip(mutants, powers)
		
		# 'Unzip' the tuples in z1 by unpacking with * and zip(): result1, result2
		result1, result2 = zip(*z1)
		
		# Check if unpacked tuples are equivalent to original tuples
		print(result1 == mutants)
		print(result2 == powers)
		 
		
________________
<a name="python_functions"/>
### functions
- encapsulation: all variables def in the body are local, global variables are just copies in the body
- if you want to set a variable in a function body to its global variable, type "global" - else it remains local, even globally def out of scope

		global x # bind x to global variable - on base scope
		nonlocal x # bind x to variable on next upper scope
		
- dictionary of all local/global variables

		locals()
		globals()

- "def" and "return" and docStrings

		def functionName(par1, par2): # function header
			""" # begin of function body containing docstrings, computation, return
			THIS IS THE DOCSTRING:
			This function does this and that
			"""
			instructions
			return par3

- copies of functions

		newFunction = functionName
		newFunction()

- lambda expression:

		lambda <param1>,<param2>,... :<expr>
		
		f = lambda x,y:x+y
		f(2,3)
		
		(lambda x,y:x+y)(2,3)
		
- higher functions of x()

		(lambda x,y,z:x(y)+y(z)) (lambda a:a+1,10,11)
		
- multiple function parameters

		def multi_print(number = 3, word = "Hallo"):
			for i in range(0, number):
			print(word)
	
		multi_print(5)
		multi_print(word = "Welt", number = 5)
		
- return multiple values - by tuples

		def change(word1, word2):
			return (word2, word1)
			
- inner functions

		# Define echo
		def echo(n):
			"""Return the inner_echo function."""
		    
	    # Define inner_echo
	    def inner_echo(word1):
	        """Concatenate n copies of word1."""
	        echo_word = word1 * n
	        return echo_word
	
	    # Return inner_echo
	    return inner_echo
		
		# Call echo: twice
		twice = echo(2)
		
		# Call echo: thrice
		thrice = echo(3)
		
		# Call twice() and thrice() then print
		print(twice('hello'), thrice('hello'))

- flexible arguments

		def gibberish(*args): # passing arbitrary many arguments 
		    """Concatenate strings in *args together."""
		    hodgepodge = str()
		    for word in args:
		        hodgepodge += word
		    return hodgepodge


		# define report_status - passing dictionaries
		def report_status(**kwargs):
			"""Print out the status of a movie character."""
			print("\nBEGIN: REPORT\n")
			# Iterate over the key-value pairs of kwargs
			for key, value in kwargs.items():
				# Print out the keys and values, separated by a colon ':'
				print(key + ": " + value)
			print("\nEND REPORT")
		
		report_status(name='luke', affiliation='jedi', status='missing')
		report_status(name='anakin', affiliation='sith lord', status='deceased')
		
- copy/reference: parameters are passed in function
	- primitive dataypes: passing copy >>> int/boolean/strings | changes to copies in function body dont apply on original
	- datastructures: passing reference | changes to reference in function body apply on original!
	- "import copy" - "copy.copy(var)" make true copy
	- "import copy" - "copy.deepcopy(var)" make true copy - also of including variables, eg in lists

- passing datastructures as function parameters:
	- by passing the elements of a datastructure

			def f(a, b, c):
				print(a)
				print(b)
				print(c)
	    
			l = [1, 2, 3]
			
			f(l[0], l[1], l[2])
			f(*l)
		
	- by defining the parameter

			def calculate_max(*params):
			    print(params)
			    current_max = params[0]
			    for item in params:
			        if item > current_max:
			            current_max = item
			    return current_max
			    
			calculate_max(1, 2, 3)
			
	- passing functions

			def hello():
				print("hi, i am jose!")
			def otherFunction(func)
				print(func())
			otherFunction(hello)
			
	- decorators
			
				def new_decorator(func):
					def wrap_func():
						print("code here before executing func")
						func()
						print("func has been called")
					return wrap_func
								
				@ new_decorator
				def func_needs_decorator():
					print("this fct is in need of a decorator")
					
				func_needs_decorator() #now this fct is wrapped by decorator_fct
						
				# same as doing this:
				#		def func_needs_decorator():
				#			print("this fct is in need of a decorator")
				# 	func_needs_decorator = new_decorator(func_needs_decorator)
				# 	func_needs_decorator() #is wrapped by decorator
				
	- dictionaries übergeben:

			def f(**args):
			    print(args)
			    
			f(key="value", key2="Value 2") #parameters are interpreted as dictionary
			
	- dictionaries übergeben:

			def g(key, param2):
			    print(key)
			    print(param2)
			    
			d = {"key": "Ich bin der Schlüssel", "param2": "Ich bin der Parameter"}
			
			g(key=d["key"], param2=d["param2"]) #
			g(**d) #same output
			
	- practical example:

			# matplotlib inline
			import matplotlib.pyplot as plt
			
			def create_plot(**plot_params):
			    print(plot_params) #prints a dictionary
			    plt.plot([1, 2, 3], [5, 6, 5], **plot_params) #passes dict as paramas
			    plt.show()
			    
			create_plot(color="r", linewidth=10, linestyle="dashed")
			
	- lambda functions | define functions on the fly | syntax: "lambda passedParameter: returnValue"
			students = [("Max", 3), ("Monika", 2), ("Erik", 3), ("Franziska", 1)]
			students.sort(key=lambda student: student[1])
			print(students)
			
			f = lambda student: student[1]
			f(("Max", 1))
				
			
________________
<a name="python_composed_variables"/>
### composed variables
- range(m,n) creates sequence integers from m to n-1 | range(m,n,a) creates integers m, m+a, m+2a, ... as long as <n
- str list tuple set dict
	- assigning var2 = var1 means assigning the reference, not the value!
	- make really new one: "var2 = list(var1)" or "var2 = var1[:]"
- sequences: str list tuple >>> iteratable
	- lists: "\[]" "\[3,5,6,3]" "\['spam',[1,2,3],3.141592]"
	- s[i] selects i-th entry of sequence s | i=-1 is last element
	- ['ab','xy'][-1][0] selects 'x' of sequence s
	- s[i:j] selects connected interval of sequence s from i to j-1 | sequence slicing
	- s[:j] selects connected interval of sequence s from beginning to j-1
	- s[i:] selects connected interval of sequence s from i to end
	- s[:] selects all
	- s[i:j:k] selects connected interval of sequence s from i smaller j-1 with  step k | negative steps possible
		- "d_l_r_o_w_ _o_l_l_e_h"[-1::-2] >>> "hello world"
	- zuweisungen:
		- l=['x']*6 >>> [x,x,x,x,x,x]
		- l[::2]=[0]*3 >>> [0,x,0,x,0,x]
		- l=list(range(7)) >>> [0,1,2,3,4,5]
		- l[-3::-1] list(range(5)) >>> [4,3,2,1,0,5,6]
	- functions for sequences - seq s:
		- len(s) length of s
		- min(s) minimal element of s
		- max(s) maximal element of s
		- sum(s) sum of all elements of s
		- del(s[i]) | del(s[i:j:k]) deletes entries of s
		- "var in l" returns boolean: ask whether variable is element of seq
		- "var not in l" returns boolean: ask wether variable is not element of seq
- lists - list l - different types
	- definition l=[] | l=[x,y,z,"bla",5]
	- multiple assignment; "x,y,z = [var1, var2, var3]
	- l = m + n merging two lists
	- l.append(x) inserts element x in the end of l
	- l.sort() sorts entries of l increasingly
		- l.sort(reverse=True) decreasingly
		- sorting functions
				def get_length(item):
					return len(item)
				l = ["Max", "Monika", "Erik", "Franziska"]
				l.sort(key=get_length) # passing a function, not  value, 
				#simpler: pass "key=len"
				print(l)
		- print(sorted(l, reverse = True)): make a sorted copy of the list
	- l.reverse() reverses entries of l
	- l.insert(i,x) inserts x after ith element
	- l.count(x) returns number of entries with 'x'
	- l.index(x) returns index of entry 'x'
	- l.remove(x) removes first entry with x eg "stringExample" >>> value
	- del l[i] deletes i-th entry >>> index
	- l.pop() returns and removes last entry from list
	- seperate by string str: str.join(l)
	- separate string into listelements by stringlelements: str.split(strElements) | eg "This - is - a - String".split("-")
	- l2 = [ <expr(x)> for x in l1] list comprehension | eg l1 = [1,2,3,4] l2= [x*x for x in l1]
	- matrix syntax:
		- matrix = [[1,2,3],[4,5,6],[7,8,9]]
		- first_row = [row[0] for row in matrix]
- tupel - like lists, but cannot be modified
	- mutable l vs immutable t | in python keine variablen übergeben sondern referenzen
	- definition t=()
	- t=('Das','ist','ein','Tupel')
	- t[2][0] >>> 'e'
	- t=("david","39","Munich") | name, age, city = t | depack tuple
	- tuples "t1=(Mark, 34) t2=..." in a list "students":
			for name, age in students:
				print(name)
				print(age)
- dictionaries >>> hash tables | no order | mutable
	- definition d = {}
	- d = {key1:value1, key2:value2, ...}
		- only immutable keys! -> no sequences etc
		- d = {'key1': list_value, 'key2': static_value}
	- d[key1] returns value1 -> alternatively:
	- "key1 in d" returns boolean
	- d.get(key1, fallbackvalue)) returns value1 if key1 is existing, else fallbackvalue | no hard error if not existing | bad for debugging
	- d.setdefault(key1, altValue) sets altValue for key1 if key1 does not exist - returns value for key1 (either value1 of altValue)
	- d[key1] = newValue assigns a new/first value to this existing/new key
	- del(d[key1]) deletes entry associated with this key
	- iterate dictionary d
			for x in d 
				print x
	- d.keys() returns all keys as kind of sequence (type dict_keys)
	- d.values() returns all values al kind of sequence (type dict_values)
	- d.items() returns all items as tuples (type dict_items)
	- dics in a loop:
			  for key in d
				  value = d[key]
	  or simpler
			  for key, value in d.items(): # d.item() returns tuple
	- d = {key1:{subkey11:value11, subbkey12:value12}, key2:{subkey21:value21, subbkey22:value22}} # 2d dictionaries
	- d['key']['subkey']
	- zip(listOfLabels, listOfValues) # returns iterator of tuples - just runs ONE time
	- pprint.pprint(d) - pretty printing - import pprint
	- pprint.pformat(d) - returns pretty string
		  
- strings - string s
	- s.find(s1) returns offset to first time finding string s1 in s
	- s.replace(s1,s2) replaces s1 by s2
	- s.startswith(s2) returns True/False
	- s.endswith(s2) returns True/False
	- s.split(split) returns list of strings with splitter split
	- s.partition(sep) returns tuple of (head,sep,tail)
	- s.join(strs) joins strings from list strs with splitter s
	- s.capitalize() first letter capital
	- s.upper() all letters upper
	- s.lower() all letters lower
	- s.strip() string without control characters
	- s.count(s1) counting the string s1 in s
	- escape characters: eg \' \" \t \n \\
	- control character:
		- new line "\n"
	- string formatting: '%' and string.format() -> https://pyformat.info

				n = 5
				print("Ich habe " + str(n) + " Hunde")
				print("I got " + str(n) + " dogs")
				print("Ich habe {0} Hunde".format(n))
				
				translations = {
				    "number_of_dogs": "Ich habe {0} Hunde"
				}
				
				print(translations["number_of_dogs"].format(n))
				
				# new version .format()  
				print("Ich habe {1} {0}x".format(5, "Katzen"))
				print("So viele Katzen habe ich: {0:f}".format(5))
				print("So viele Katzen habe ich: {0:.2f}".format(5))
				print("Pi hat den Wert: {0:.3f}".format(3.141529))
				print("Ich habe {num:.3f} {anim}".format(num = 5, anim = "Hunde"))
				print("Ich habe {x:.3f} {y}".format(x = 5, y = "Hunde"))
				
				# old version '%'
				'mein Lieblingstier ist ein(e) %s' % tierString
				'%s %s' % ('one', 'two')
				'%d %d' % (1, 2)
	- raw strings: r"this string ignores all escape characters \n it prints all backslashes" good for regular expressions
	- string.isupper() - string.islower() - string.isalpha() - string.isalnum() - string.isdecimal() - string.istitle() - string.isspace()
	- string.upper() - string.lower()
	- string.startswith(stringA) - string.endswith(stringB)
	- string.join(listOfStrings) - listOfString = "this-is-a-test-strin-".split('-')
	- padding: "Hello".rjust(10,'*') >>> "*****Hello" - "Hello".ljust(10,'*') >>> "Hello*****" - "Hello".center(10,'=') >>> "==Hello==="
	- rm padding: string = "   Hello   " - string.strip() >>> "Hello" string.lstrip() >>> "Hello   " - string.rstrip() >>> "   Hello"
	- copy/paste: "pyperclip.copy("Hello")" - "pyperclip.paste()" >>> "Hello" - import pyperclip - works with clipboard outside

- set - in general not ordered | no redundancy!/duplicates! | every entry just once in the set | uniqueness of elements
	- s = set()
	- definition s = {entry1, entry2, ...}
	- s = {1,2,2,2,3,4,5} >>> s = {1,2,3,4,5}
	- s.add(entry3)

- queue | opposite to stack | pull entry from beginning | put entry in the end

				import queue
				q = queue.Queue()
				q.put(entry1)
				q.put(entry2)
				q.get() #entry1
				q.get() #entry2
				
- priority queue | entries are tuples with priority (+/- integers) and actual entry

				import queue
				q = queue.PriorityQueue()
				q.put((10, entry1))
				q.put((15, entry2))
				q.put((5, entry3))
				
				q.get() #entry3 least priority
				q.get() #entry1 2nd least prio
				q.get() #entry2 highest priority
				
- data structure
	- encapsulated lists

				liste = [
			    ["Berlin", "München", "Köln"],
			    ["Budapest", "Pécs", "Sopron"]
				]
				liste[0][0]
	- encapsulated dictionaries

				students = {
			    "Informatik": ["Max", "Monika"],
			    "BWL": ["Erik", "Franziska"]
				}
				print(students["Informatik"])
				print(students["BWL"])
				
- generator: creates entries on the fly / when asked >>> "yield"

				def gen_generator():
					for i in range(0, 10):
					print("gen: " + str(i))
					yield i
					
				for element in gen_generator():
					print("for: " + str(element))
				
- list comprehension - collaps for loops for building lists into single line - components are: (1) iterable, (2) iterator variable representing members of iterable, (3) output expression

		#new_list = [fct(i) for i in set(i) if condition(i)]
		new_list = [i**2 for i in range(0,10)]
		
- list comprehension - nested

		# Create a 5 x 5 matrix using a list of lists: matrix
		matrix = [[col for col in range(0,5)] for row in range(0,5)]
		# Print the matrix
		for row in matrix: print(row)

- conditions in comprehensions	

		new_list = [i**2 for i in range(0,10) if i%2 == 0]#
		
		# Create a list of strings: fellowship
		fellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']
		# Create list comprehension: new_fellowship
		new_fellowship = [member for member in fellowship if len(member) >= 7]
		# Create list comprehension: new_fellowship
		new_fellowship = [member if len(member)>=7 else '' for member in fellowship]		

- dictionary comprehensions

		pos_neg = {num: -num for num in range(0,10)}
		
- generator expressions - not stored in memory, not created but can be iterated over (is iterator) - lazy evaluation

		(2*num for num in range(1,10**1000000)) # create generator object
		
		result = (num for num in range(0,31)) # create generator object
		print(next(result)) # Print the first 5 values
		print(next(result))
		print(next(result))
		print(next(result))
		print(next(result))
		for value in result: # Print the rest of the values
		    print(value)
		
- generator functions - eg .items() or range() return generators - like generator expressions, yield a series of values, instead of returning a single value -  defined as you do a regular function, but whenever it generates a value, it uses the keyword yield instead of return

		# Create a list of strings
		lannister = ['cersei', 'jaime', 'tywin', 'tyrion', 'joffrey']
		
		# Define generator function get_lengths
		def get_lengths(input_list):
		    """Generator function that yields the
		    length of the strings in input_list."""
		
		    # Yield the length of a string
		    for person in input_list:
		        yield len(person)
		
		# Print the values generated by get_lengths()
		for value in get_lengths(lannister):
		    print(value)

________________
<a name="python_references"/>
### references
- pointers to same memory allocation >>> for composed variables, not for simple ones like int/bool/float
- references 

		x = [1,2,3]
		y = x
		x.append(4)
		print y
		
________________
<a name="python_functional_programming"/>
### functional programming

- not imperative like C, C++, Java, C#, 
- list-comprehension - like math set notation

		[num**2 for num in [1,2,3,4,5]]

		[2*x for x in range(1,21) if x%3==0 or x%7==0]
		
		[(x,y) for x in range(1,10) for y in range(1,10)]
		
		strglist = ['Ich bin rein', 'und so klein', 'ja das ist fein']
		[w for s in strglist for w in s.split() if len(w)<3]
		
- many "for terms" possible | every "for term" can be followed by if term
- map function - higher-order-function - will be created finally if asked for it/forced

		map(fct, seq_1, seq_2, ...)
		map(fct x,y,z,...: fct(x,y,z,...), [x_0, x_1, ...], [y_0, y_1, ...], ...)
		m = map (lambda x,y: x+y, [1,3,5], [10, 100, 1000])
		list(m) # map obj needs to be converted to list obj

- filter-function - filter elements from list/sequence - filter-function must return boolean value

		filter(f, seq)
		filter(lambda x: x%2 != 0 and x%3 != 0, range(1,25))

- reduce-function

		from functools import reduce
		reduce(fct, seq)
		reduce(f x,y: x+y, range(1,10)) # sums all numbers from 1 to 10
		
________________
<a name="python_files_directories"/>
### files and directories

- libraries:
	- sys Bietet Zugriff auf Umgebungsvariablen, Standardströme (stdin, stdout, stderr), die Kommandozeile 
	- os Ist die plattformunabhängige27 Schnittstelle zum Betriebssystem. 
	- os.path Ist ein „Untermodul“ von os und bietet Möglichkeiten zur Bearbeitung von Pfaden.
- import modules:

		import moduleName
		import sys
		import os
		import os.path
		dir(moduleName) # returns all functions as a string

- file objects - creating a file object accessing a file with reading/writing/appending mode (r,w,a)

		file = open (fileString, 'w')
		file.write(codeString)
		file.writelines([string1, string2])
		print(file.closed) # check whether file is closed
		file.close()
		print(file.closed)
		
		file = open (fileString, 'r')
		text1 = file.read()[:10]
		text2 = file.readlines()[:-2]
		for line in file #file iterator: reads only lines that are actually used
			print line
		file.close()
			

- python can manage files itself >>> closes ec: "with open() as file" - "with" creates context > is a context-manager

		with open("lesen.txt", "r") as file: 
			for line in file:
			print(line)
			
		open(fileString,w).write('first line','second line')
		
		with open("datei.csv") as file:
			for line in file:
			data = line.strip().split(";")
			print(data[0] + ": " + data[1])
		    
		with open("datei.csv") as file:
			for line in file:
				data = line.strip().split(";")
				if int(data[1]) < 2000000:
				    continue
				if data[2] == "BUD":
				    continue
				print(data)
				#if data[2] == "BER" or data[2] == "BUD":
				#    print(data[2])
				#    print(data)
		
		with open('datei.csv) as file
			data = file.read()
			line = file.readline()
				        
- regarding character chart apart from ascii:

		with open(filename, "r", encoding="utf-8") as file:
		    for line in file:
		        print(line)

		with open(filename_out, "w", encoding="utf-8") as file:
			file.write("Müller")
	    
- file manipulation - list comprehensions
	- todo
- directories
	- todo

________________
<a name="python_objects"/>
### objects

- in python: every variable instance of an object
	- for any function of a variable func(x) exist a method x.__method__()
	- possible to inherit this Class and overwrite >>> see later
- convention
	- classNamers: PascalCase (IchBesteheAusMehrerenWoertern)
	- not used: sneak_case (ich_bestehe_aus_mehreren_woertern)
	- variables/methods: camelCase (ichBesteheAusMehrerenWoertern)
- objects are of a special class >> data structure
- methods:
	- commands/functions provided by a class/object of a class
	- if exist many classes with same methodName >> able to create functions dealing with all these types of classes: eg getInfoOf(obj) >>> create modular code!!!
- class variables/properties: variables provided by a class/object of a class
- defining a new class:

			class ClassName():
				property_name1 = value1 # static variable
				property_name2 = value2 # static variable
				def method_name(self): # 1st arg must be itself! > eg list.append(3)
					commands method...
				command 3
				command 4
				...
				
- generating a new instance
			a_new_instance = ClassName()
			

- Generators: methods generating an instance:
			class ClassName():
				number = 0 # static variable
				def __init__(self, t, c): # initialization method
					self.type = t # instance property, only for this instance
					self.color = c # instance property, only for this instance
					ClassName.number += 1 # class attribute, for all instances the same!
				def __del__(self):
					ClassName.number -= 1 # class attribute, for all instances the same!
				def set_color(self,newColor):
					self.color = newColor
				def set_type(self,newType):
					self.type = newType
				def description:
					print("I am a %s %s, There are %d types of me.", % \(self.color, self.type, ClassName.number))
			mustang = ClassName("Ford","red")

- inheritance:
	- definition

			class ClassName(InherritedClass):
				def __init__(a,b,c):
					InherritedClass.__init__(a,b)
					# alternatively: super().__init(a,b)
					self.c = c
	
	- overwriting functions...
		- overwriting/implementing special python functions
		- eg "__mul__" function >>> works then as well for "*"
	- type(x): returns exact type of an instance
	- isinstance(x, CertainClass): is type of an instance of CertainClass?
				

- encapsulation: private/public methods/variable
	- convention: "pseudo" private variables/property/method begin with subbar: _privateProperty
	- actually private property/method: begin with 2 subbars: __privateProperty
	- access to private properties by getter and setter

- special methods:
	- x = SomeClass | print(x) | overwrite this function by def __str__(self):
	- x = SomeClass | x | overwrite this function by def __repr__(self):
	- Länge len(x) | overwrite this function by def __len__(self):

________________
<a name="python_regular_expressions"/>
### regular expressions
- import re
- regular expressions - https://docs.python.org/2/howto/regex.html - kap 4.2.1ff in "Häberlein, Informatik: Eine praktische Einführung mit Bash und Python"
	- use raw strings with an "r" before "r'testString'"
- character classes
	- "." wild card - all characters!
	- "\" is a normal character | 
	- "\s" whitespace
	- "\S" nonwhitespace
	- "\d" digits
	- "\D" nondigits
	- "\w" letters and numbers (alphanumeric)
	- "\W" non-alphanumeric
	- "[c]" makeing your own character classes eg "[aeiouAEIOU]" - eg "[0-9]" classes of eg numbers | the same: "[0123456789]"
	- "[^c]" means NOT c - negatige character class - "[^aeiouAEIOU]"
	- "*" repeating meta characters "[a-z]*" "(boa)*" sequences of letters zero/more times - escape with "\*"
	- "+" repeating meta characters "[a-z]+" "(boa)+" sequences of letters at least one, as many as possible - escape with "\*"
	- "?" optional characters "[a-z]?" zero/one times - optional paranthese (bla)? - escape with "\?"
	- "{}"optional characters "[a-z]{2,5}" 2/3/4/5 times - optional paranthese (bla){2,3} - greedy by default - "{2,5}?" ungreedy
	- "^bla" only matches in beginning of line
	- "bla$" only matches at end of line

- create regex objects:
			phoneNumRegex = re.compile(r'\d\d\d-\d\d\d-\d\d\d\d')
			matchObjects = phoneNumRegex.search("my number is 415-555-4242")
			print('Phone number found: ' + matchObjects.group())
- building groups:
			phoneNumRegex = re.compile(r'(\d\d\d)-(\d\d\d-\d\d\d\d)') # grouping by using parantheses
			matchObjects = phoneNumRegex.search("my number is 415-555-4242")
			matchObjects.group(1) >>> 415 - matchObjects.group() >>> 415-555-4242 - just returns match of ith paranthese
			matchObjects.groups() >>> tuple ('415', '555-4242')

- pipes:
			r"Beate|Christa" gets all strings matching Beate or Christa

			batRegEx = re.compile(r'Bat(man|mobile|copter|bat)')
			mo = batRegEx.search('Batmobile lost a wheel')
			mo.group()
			mo.gourp(1)
- findall()
			phoneNumRegex = re.compile(r'\d\d\d-\d\d\d-\d\d\d\d')
			mo = phoneNumRegex.search("cell: 415-555-9999 work: 212-555-0000")
			mo.group() >>> just one entry, one string
			
			phoneNumRegex = re.compile(r'\d\d\d-\d\d\d-\d\d\d\d')
			mo = phoneNumRegex.findall("cell: 415-555-9999 work: 212-555-0000")
			mo.group() >>> list of all matches

			phoneNumRegex = re.compile(r'(\d\d\d)-(\d\d\d)-(\d\d\d\d)') grouped regex
			mo = phoneNumRegex.findall("cell: 415-555-9999 work: 212-555-0000")
			mo.group() >>> list of tuples of strings!

- dot star ".*" - is always greedy quantifier - ungreedy with ".*?"
			nameRegex = re.compile(r'First Name: (.*) Last Name: (,*)')
			mo = nameRegex.search('First Name: Al, Last Name: Sweigart')
			mo.group(1) >>> "Al" - mo.group(2) >>> "Sweigart)

- references to expressions in brackets "r'(bla)'" by numbers "r'\1'"

			re.findall(〈regexp〉, 〈string〉 [,〈flags〉])
			re.findall(r'[0-9]+ ', '99 Luftballons in 10 Zimmern in 1nem Haus')
			re.findall(r'[a-z]+\s+[a-z]+', 'erstes Wortpaar zweites Wortpaar')
			
			re.sub(〈regexp1〉, 〈replacement〉, 〈string〉 [,〈flags〉])
			re.sub(r'\n+', '\n', 'Dies\nist ein \n\n\n String')
			re.sub(r'([a-zA-Z]+)\s+\1\b', r'\1', 'hallo hallo ich ich stottere')
			
			re.search(〈 regexp〉, 〈string〉, [,〈 flags〉]) #returns a match object
- complex expressions: introducing comments and linebreaks in regular expressions: with "r'''expression'''" and flag "re.X"

			m = re.search(r'''\[ #list start
					([^,\]]+) # 1. element | all chars untill , or ] coming
					((,[^,\]]+)*) # rest of the list
					\] # end of the list
					''', string, re.X)
			
- match ojects methods:
	- "group()" returns string of matches | "m.group(i)" access to i-th element by
	- "start()"/"end()" returns start/end position

- greedy/non-greedy
	- greedy: maximal sequence | "default option"
	- non-greedy: minimal sequence, as little as possible | non-greedy version of greedy ones: use "?" after a greedy quantifyer: "*", "+", "?", "{n,m}"
			
- lookahead - todo

- regular expression vs list comprehension - todo
________________
<a name="python_debugging"/>
### debugging | errors | exeptions
- e.g. when writing functions -> handle errors
- pro of errors -> show what to debug
- exceptions:
	- errors detected during execution - not unconditionally fatal
	- CRITICAL errors >>> program ends standardized
				print("name) >>> SyntaxError
				print(mylist) # w/o initializing mylist >>> NameError
	- in some case want to intercept and react by an exception - link into process - react to critical errors - program may continue

			try:
				with open("datei.xyz", "r") as file:
					print(file)
				print(5 / 0)
			except ZeroDivisionError: #if not specified prints on ANY error
				print("Du darfst nicht durch 0 teilen")
			except FileNotFoundError:
				print("FileNotFoundError ist aufgetreten")
			else:
					print("SUCCESS!")
						
- raise an error

			# Raise an error with raise
	    if echo < 0:
        raise ValueError('echo must be greater than 0')
						
- error classes
	- eg ZeroDivisionError, FileNotFoungError
	- own error classes:

				class InvalidEmailError(Exception):
				    pass
				
				def send_mail(email, subject, content):
				    if not "@" in email:
				        raise InvalidEmailError("email does not contain an @")
				        
				try:     
				    send_mail("hallo", "Betreff", "Inhalt")
				except InvalidEmailError:
				    print("Bitte gebe eine gültige E-Mail ein")
				    

- clean up after unexpected errors: "finally" good for i/o methods to give free resources

				try:
				    file = open("existiert.txt", "r")
				    print(file)
				    print(5 / 0)
				except FileNotFoundError:
				    print("Datei wurde nicht gefunden")
				finally:
				    print("FINALLY!!!")
				    print("dieser Befehl wird IMMER bearbeitet")
				    file.close()

________________
<a name="python_modules"/>
### modules
- std libraries: https://docs.python.org/3.6/library/index.html
- encapsulate/develop code parts - like puzzles
- import
	- import example_module | import complete code from "example_module.py"
	- import example_module as em | abbreviation
	- from example_module.py import fct1, fct2 # imports fct from "example_module.py"
	- from example_module.py import *  # all functions
	- import complex_example_module as cem | rename module in code
	- import module.submodule | include sumodules
- create modul folder:
	- name folder after module
	- create file __init__.py in it

			__all__ = ["datei"] # enables *-notation below
			from . import datei # enables "moduleName.datei.fct()" notation
	- create file datei.py in it
	- write code in datei.py
	- "from folderName import datei"
	- "from folderName import *"
#### data science modules
- numpy: dealing with arrays and functions on them | filters |

			import numpy as np
			# central concept: array and operationse on it - see later...
				
- pandas: csv, excell files/data -> dataFrames!
	
			import pandas as pd
			df = pd.read._csv("../data/file.csv", delimiter=",") #dataframe
			df #print datafram
			df.head() #print head of dataframe
			len(df)
			df["Name"] #get a row of the table
			entry = df.iloc[0] #getting lines by indices: index location
			entry["Name"] #access single rows
			df.iloc[4:6] #list slicing
			for row in df.iterrows() #iterate over rows - returns tuple
				print(row)
				pos = row[0] #index
				data = row[1] #data - simpler: pos, data = row
			for pos, data in df.iterrows() #most simple
				print(data["Name"])
			df[df["Year"] < 1769] #selection with a filter
			df.sort_values("Name", ascending=False) #sorting dateframes
			
			see more later...
				
- matplot.lib

			df = pd.read_excel("data.xlsx")
			%matplotlib inline
			import matplotlib.pyplot as plt
			year = df["Year"]
			sales = df["Sales"]
			plt.plot(year, sales, color="#ff0000", linestyle="dashed", marker="o", label="Sales")
			plt.legend()
			plt.show()
			
			#diagrams
			plt.pie(sales) #pie diagram
			plt.show()
			plt.bar(year, sales) #bar diagram
			plt.show()
			plt.scatter(year, sales) #point diagram
			plt.show()
			
			see more later...
			
	- pickle

			import pickle
			with open('data.pkl', 'rb') as file: # Open pickle file and load data to d
		    d = pickle.load(file)
		    
			print(d)
			print(type(d))
					
####  other example modules
- re: regular expressions
- math:
		math.pi # pi
		math.radians
- scipy.linalg as 
		inv()
- 
- pickle: writing byle stream of objects into file
- shelve: keeping persistent data as dictionarie
- csv: reading files.csv | comma separated values
- requests: request html code from an html server
- beautyfulsoup: extract html code from files
- matplotlib

		%matplotlib inline
		import matplotlib.pyplot as plt
		
		xs = [1, 2, 5]
		ys = [4, 7, 5]
		
		plt.plot(xs, ys)
		plt.show()
		
- datetime: ISO 8601 - yyyy-mm-dd hh:mm:ss

			from datetime import datetime, date, time, timedelta
			now datetime.now()
			birthday datetime(1979, 05, 19, 4, 0, 0)
			print(now) 
			print(birthday)
			#acces now.year, now.month, now.day, now.hour now.minute, now.second
			
			#calculate differences with unix timestamp| seconds since 1.1.1970
			livingTime = now.timestamp - birthday.timestamp
			
			#separate time and date
			dateNow = date.now() #without hours ec | create by date()
			timeNow = time.now() #without days ec | create by time()
			#transform datetime >> time/date: egDateTime.time() | egDateTime.date() 
			#combine time/date: egDateTime = datetime.combine(egDate, egTime)

			#formated datetime:
			now.strftime("%d.%m.%Y")
			# https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior
			#timedelta objects:
			print(now + timedelta(days = 20, hours = 4, minutes = 3, seconds = 1))
			day = datetime(2017, 8, 20)
			td = day - now # is a timedelta object
			print(td)
			print(datetime(2018, 1, 1) + td)
			
- defaultdict | default-dict: value "0" for unknown keys | initiates itself on the run

			from collections import defaultdict
			
- sys - pass parameters to python program

			import sys
			print(sys.argv)
			
			#pass a text file:
			import sys
			if len(sys.argv) >= 2:
				filename = sys.argv[1]
				file = open(filename, "r")
				counter = 0
				for line in file:
				    counter = counter + 1
				print(counter)
			else:
				print("Filename missing")
				
- os - communication with operating system

			import os
			print(os.listdir("."))
			#absolute path to running python file: __file__
			#dir for this absolute file path: os.path.dirname(__file__)
			#eg for opening/reading/writing a file
			os.path.join(os.path.dirname(__file__), datei.txt) #"/" or "\" in os?
			#join many subdirectories by many parameters in os.path.join()
			#join dir with "exampleDirectory", "..", 
			os.path.isdir(file_path) #check wether dir or not
			
			wd = os.getcwd()
			os.listdir(wd)
			
- opencv - image editing tool
	- face recognizer
- qt - framework desktop applications
	- originally for c++
	- qt creator
	- pyQt [proprietary] | PySide (LGPL) [open source] | qtpy good for both options

________________
<a name="python_numpy"/>
### numpy

- central concept: ARRAY
	- ...and natural operations on it
	- ...with more performance
	- can contain only one single type
	- same operations on np.arrays and python.lists can have very different meanings: "a1+a2" <-> "l1+l2"
- general:
	- "import numpy"
	- create arrays from lists: "a = numpy.array(l)"
	- "import numpy as np" -> "a = np.array(l)"
	- "from numpy import array" -> "a = array(l)"
- infos about np.arrays

		p.ndim # dimension of array p
		p.shape # stucture of an array
		len(p) # dim lenght
		p.dtype # data type
		
- arrays: fixed data type, immutable with fixed length

		a = np.array([1,2,3,4,5])
		a = np.linspace(1,5,5) # same rslt: linspace(start, end, entries)
		b = np.array([6,7,8,9,10])
		c = np.empty_like(a) #  new array with same shape and type
		d = np.copy(c) # arrays are immutable
		
		a * r # multiplies every entry - also: +-/#**
		a + b # adds entries pairwise - also: +-/#**
		a[i] # access ith element
		a > 4 # array with booleans -> create filters
		a[a>4] # filtered array
		c = array([[1,2,3],[4,5,6]]) # 2dim array
		c.shape() # structure of array > (2,3)
		c[1][2] # 6 - access elements - outer > inner (row > column)
		c[1,2] # alternatively
		c[0,[:2]] # select subarray
		c = np.column_stack((a,b)) # makes nx2 from 1xn and 1xn array
		
		# np.arange(): evenly div interv - reshape(): 1d>2d
		A = np.arange(8).reshape(2,4) + 0.1 
		B = np.arange(6).reshape(2,3) + 0.2
		C = np.arange(12).reshape(3,4) + 0.3
		
		# stacking horizontally - must have same number of rows
		np.hstack([B,A])
		np.concatenate([B,A], axis=1)
		
		# stacking vertically - must have same number of columns
		np.vstack([A,C])
		np.concatenate([A,C], axis=0)
		
		# creating methods
		np.empty([2,3])
		np.eye(2, dtype=np.int) # identity matrix
		np.ones(5)
		np.ones_like(p)
		np.zeros(5)
		np.zeros_like(p)
		np.full((2,2),3,dtype=np.int)
		np.full_like(p)
		
				
- functions for arrays:

		np.mean(a) # mean
		np.median(a) # median
		np.corrcoef(a,b) # correlation
		np.std(a) # std-deviation
		np.sum()
		np.sort()
		
		
- logical operators:

		np.logical_and(expression1, expression2)
		np.logical_or(expression1, expression2)
		np.logical_not(expression)
		np.array_equal(a, b)
		
- iteration in arrays

		for var in np_array:
		    expression
		
		for var in np.nditer(np_array): # iterate over all element in n-dim array
				expression
				
- random numbers

		np.random.seed() # set seed for random number
		np.random.rand() # returns random number in [0,1[
		np.random.randint(a,b) # returns random int number	 >=a AND <b
		
- importing text files

		import numpy as np
		filename = 'MNIST.txt'
		# tends to break down for mixed data types -> data frames
		data = np.loadtext(filename, delimiter=',')
		data = np.loadtext(filename, delimiter=',', skiprows=1, usecols=[0,2])
		data = np.loadtext(filename, delimiter=',', dtype=str)
		
		# structured array - handling mixed data types
		data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)
		data = np.recfromcsv('titanic.csv') # same as above with props as default
		
________________
<a name="python_mathplot"/>
### mathplot

- plotting np.arrays, pd.series, pd.dataFrames

		import matplotlib.pyplot as plt
		year = [1950, 1970, 1990, 2010]
		pop = [2.519, 3.692, 5.263, 6.972]
	
- plotting points/lines/scatterplot

		plt.plot(year, pop) # configure what and how to plot - here plot <> line
		plt.show() # just displays
		plt.scatter(year, pop) # just points w/o line
		plt.scatter(x, y, z, c=col, alpha = 0.8) # 3d with bubbles, c=color-dict
		plt.xscale('log') # other scale
	
- histograms

		plt.hist(a,numberOfBars) # plot a histogram
		plt.show()
		plt.clf() # clean up plt
	
- customization

		plt.plot(year,pop)
		plt.xlabel('Year') # set axes name
		plt.xlabel('Population') # set axes name
		plt.title('World Population Projection') # set title name
		plt.yticks([0,2,4,6,8,10],['0','4B','6B','8B','10B']) # set marks & their name
	
		plt.text(specificXvalue, specificYvalue, 'text') # add text into the grid
		plt.grid(True) # add a grid

________________
<a name="python_pandas"/>
### pandas

- introduction - all about data frames - similar to arrays - different types possible
- pandas data structure:
	- indexes: sequences of labels - immutable - homogeneous data type
	- series: 1d array with index
	- dataframe: 2d array with series as cols, sharing common indexes
- create dataframes

		import pandas as pd
		df = pd.DataFrame(d) # create dataframe from dictionary d
		df.index = rowLabels # setting the row labels
		d = dict(list(zip(list_labels, list_cols))) # e.g. build dict from lists
		d = {'height':heights, 'sex':'M'} # e.g. built dict on the fly
		df = pd.read_csv('file.sth') # read csv file
		# more on importing see below
				
- assign values in dataFrame

		df['newColumn'] = value # broadcast to entire column
		
- methods and attributes - 2d labeled array with columns as pd.series

		df.head() # default is 5
		df.tail()
		df.describe() # summary
		df.info()
		df.dtypes # data types
		df.unique() # returns array of distinct categorical entries
		df.nunique('colName') # returns number of distinct categories
		df.values().counts() # same for numerical datatypes
		df.value_counts()
		df.sort_values('colName', ascending=False) #sorting dateframes
		
		df2 = df.copy()
		
		# aggregation/reduction - receives pd.series, returns single val
		df.count() # returns series of counts
		df.mean() # average
		df.mean(axis='columns') 
		df.sum()
		df.sum(axis='columns') # sums over entries in each column
		df.std()
		df.median()
		df.quantile(q) # with q in (0,1)
		df.quantile(ql) # with ql list with elements in (0,1) > pd.series
		df.first()
		df.last()
		df.range()
		df.min()
		df.max()
		df.idxmax() # returns row label of max entry in col
		df.idxmin()
		df.idxmax(axis='columns') # returns col label of max entry in row
		
		# group data and aggregate/reduce
		df.groupby('colNameToGroup').count() # split into groups of rows
		df.groupby('colNameToGroup') # default: count in other columns
		df.groupby(colNameList).count()
		df.groupby(level=colNameList).count()
		df.groupby('colNameToGroup')[colNameAggregate].count()
		df.groupby('colNameToGroup')[listOfColumnNames].count()
		df.groupby(pandaSeriesToGroup).count() # use pd.Series to group

		# mutliple aggregation
		df.groupby('colName')[listOfColumnNames].agg(['max','sum'])
		d = {'addition':'sum', 'maximum':'max'} # keys are column names
		df.groupby('colName')[listOfColumnNames].agg(d)
		df.agg(['max','median']) # insert list of aggregation functions
		# access collumns with multi-level selection, e.g.
		df.loc[:, ('colName','max')]
		# own def z-score
		def zscore(series)
			return (series - series.mean())/series.std()
		# transform method applies fct element wise to groups
		df.groupby('colNameToGroup')[listOfColumnNames].transform(zscore)
		# apply method: more complex functions - see below

		# labels and index
		df.columns # returns/writes column labels
		df.index # returns/writes index labels
		df.index.name # returns/writes label of the index, e.g. 'date'
		df.index.names # for multiindex - many cols as index
		df = df.set_index(['col1', 'col2']) # order matters!
		df = df.sort_index() # sort df (according multi-index)
		df = df.sort_index(level=0) # by the first index-level
		df = df.reindex(['col1', 'col2'])
		df = df.reindex(['col1', 'col2']).dropna() # if df hasnt so many indices
		df = df.reset_index() # reset index by numbers
		df.values # transform dataframe into np.array
		del df['columnName'] # deletes 'columnName' in df
		df.shape # number of rows and columns
		df.columnName.value_counts(dropna=False)
		df.columnName.value_counts(dropna=False).head() # only first 5 counts
		df['columnName'].value_counts(dropna=False) # alternatively

		df.apply(np.mean, axis=0) # apply functions on dataFrames over columns
		df.apply(np.mean, axis=1) # apply functions on dataFrames over rows
		df.drop_duplicates()
		
		# zero numbers, nan, nans, missing data
		df.all() # excludes columns with all entries non-zero
		df.any() # excludes columns with no non-zero entries
		df.isnull() # filtering NaN entries
		df.notnull() # filtering non-NaN entries
		df.isnull().any() # returns columns with any NaN values
		df.notnull().all() # returns columns with no NaN values
		df.dropna(how='any') # drops columns with 'any' NaNs - alt 'all'
		df.dropna(thresh=1000, axis='columns') # drop col #nan > thresh 
		
		df.plot(color='r', style='.-', legend=True) # plotting method
		df[columnList].plot(subplots=True) # several plots
		df.plot(x='x-axisName', y='y-axisName')
		df.plot(x='x-axisName', y=list_y-axisNames)
		df.plot(kind='scatter') # alt: 'box' | 'hist' | 
		# histogram options: bins, range, normed, cumulative
		# scatter options: s=sizes
		plt.axis(('2001', '2002', 0, 100))
		plt.title('titelName')
		plt.xlabel('x-axisName')
		plt.ylabel('y-axisName')
		plt.yscale('log')
		plt.savefig('figure.png')
		plt.show()
		
- panda series type - 1d numpy array with labeled index - hybrid np.array, dictionary - slice like python lists

		ps = pd.Series(list) # with range index
		ps = pd.Series(list, index=listOfStrings) # custom index
		
		ps.index # only read since immutable
		ps.index = listOfStrings # excep: write ALL at once
		ps.name # label of the column
		ps.head()
		ps.tail()
		ps.describe()
		ps.unique() returns array with distinct values
		ps.values
		ps.count() # returns scalar int
		ps.mean()
		plt.plot(ps) # series can be plotted with mathplot
		plt.show
		ps.plot() # integrated method
		plt.show
		
- groupby object

		splitting = df.groupby('columnName')
		type(splitting) # pandas.core.groupby.DataFrameGroupBy
		type(splitting.goups) # dict
		print(splitting.groups.keys()) # returns group names
		
		# iterate over splitting 1
		for group_name, group in splitting:
			avg = group['colName'].mean()
			print(group_name, avg)
			
		# iterate over splitting 2
		for group_name, group in splitting:
			avg = group.loc[group['name].str.contains('something'), 'mean'].mean()
			print(group_name, avg)
			
- access to subframes by slicing - 2 parameters of possible types: string, list (4:6, ['bla1', 'bla2']), number - or filter with boolean series

		df["col1", "col2"] # returns pandas.core.series.Series for columns
		df['RowName']['columnName']
		df.columnName['RowName']
		df[0:3] # return col with index 0,1,2
		df[["col1", "col2"]] # returns pandas dataFrame for columns
		df[[0:3]] # return col with index 0,1,2
		df[[3:0:-1]] # return col with index 3,2,1
		
		# accessors: .loc/.iloc - label/index - also possible to WRITE data in data frame
		
		df.loc["label", "col"]	 # access single value
		df.loc["label"] # single row access -> new data frame
		df.loc[i] # single row with i-th index
		df.loc[["label1", "label2", "label3"]] # row access -> new data frame
		df.loc[:, ["col1", "col2"]] # column access -> new data frame
		df.loc[:, ["col1":"coln"]] # slicing labels - includes coln!!!
		df.loc[["label1", "label2", "label3"], ["col1", "col2"]] # row&col access...

		df.iloc[[0,2,13]] # row access with indices - using list
		df.iloc[-5:,:]] # row&col access with indices
		df.iloc[::3, -1] = np.nan # writing values 'not a number'
		df.iloc[:,[4,5]] # column access with indices
		df.iloc[[0,2,13],[4,5]] # row&col access with indices
		
		# fancy indexing - for hierarchychally indexed dfs - using tuples
		df.loc[('index1Name', 'index2Name')]
		df.loc[('index1Name', 'index2Name'), 'labelName']
		df.loc['index2Name'] # slicing on outer index
		df.loc['index2Name1':'index2NameN'] # range of symbols
		df.loc[(['index1Name1','index1Name2'],'index2Name'),:] # fancy
		df.loc[(['index1Name1','index1Name2'],'index2Name'),'labelName']
		df.loc[('index1Name', ['index2Name1', 'index2Name2']),:]
		
		# slicing in multi index
		df.loc[(slice(None), ['index2Name1', 'index2Name2']),:]
		df.loc[(slice('A1', 'A3'), slice(None), ['C1', 'C3']),:]

		# create an alias for pd.IndexSlice: idx
		idx = pd.IndexSlice
		df.loc[idx[:, :, ['C1', 'C3']], idx[:, 'foo']]
		
- operations/arithmetics - transform dataFrames element wise - merging

		df1 + df2 # may result in NaN's for not common indices
		df1.add(df2) # sames result
		df1.add(df2, fill_value=0) # adds 0 for entries nonexist indices
		
		df.multiply(ps) # multiplying df with ps
		
		df / ps # doesnt work nice - for non-common indices 
		df.divide(ps, axis='rows') # deviding df by ps
		df.floordiv(12) # converting to dozens unit
		np.floor_devide(df, 12) # converting to dozens unit
		
		# percent change wrt previous value
		s = pd.Series([90, 91, 85])
		s.pct_change()
												# 0         NaN
												# 1    0.011111
												# 2   -0.065934
												#	dtype: float64

		# apply function
		def dozens(n):
			return n//12
		df.apply(dozens)
		df.apply(lambda n: n//12)
		
		# map function - ig 'apply' for dictionaries
		df.index = df.index.str.upper() # index with capital letters
		df.index = df.index.map(str.lower) # 'apply' for the index: map()
		# Create the dictionary: red_vs_blue
		red_vs_blue = {'Obama':'blue', 'Romney':'red'}		
		# Use the dictionary to map the 'winner' column to the new column: election['color']
		election['color'] = election['winner'].map(red_vs_blue)
		
		df['col3'] = df.col1 + df.col2 # adding cols into new col	
		
		# vectorized functions - better performance
		# Import zscore from scipy.stats
		from scipy.stats import zscore 
		# Call zscore with election['turnout'] as input: turnout_zscore
		turnout_zscore = zscore(election['turnout'])
		# Assign turnout_zscore to a new column: election['turnout_zscore']
		election['turnout_zscore'] = turnout_zscore
		# Print the output of election.head()
		print(election.head())	
		
- merging: appending/concatenating series/dataframes

		ps1.append(ps2) # stack rows of ps2 underneath ps1 w/o reindexing
		df1.append(df2).reset_index(drop=True) # for df with reindexing
		# fills with NaNs for disjoint column labels and indices
		
		pd.concat([ps1, ps2, ps3]) # concat vertically
		pd.concat([df1, df2, df3], ignore_index=True)
		pd.concat([df1, df2, df3], axis=0) # vertically - alt 'rows'
		pd.concat([df1, df2],axis=0,keys=['il1','il2']) # outer ind label
		
		# horizontally: outer join
		pd.concat([df1, df2, df3], axis=1) # horizontally - alt 'columns'
		pd.concat([df1, df2],axis=1,keys=['cl1', 'cl2']) # column labels
		pd.concat({'cl1':df1, 'cl1':df2, 'cl3':df3}, axis=1) # use dict
		
- merging inner/outer joins
	- outer join: fills NaN for missing rows - all labels, no repets
	- inner joins: intersection of index labels, not union

			pd.concat([df1, df2, df3], axis=1, join='outer')
			pd.concat([df1, df2, df3], axis=1, join='inner')
		
- merging - merge(), merge_ordered()

		pd.merge(df1, df2) # default: wrt common col labels, inner join
		pd.merge(df1, df2, how='inner') # alt: 'left', 'right', 'outer'
		
		pd.merge(df1, df2, how='left')
		# keeps all left rows in merged df
		# rows in left df with matches in right df: non-join cols of right are appended
		# rows in left df without matches in right df: non-join cols filled with NaNs (only NaNs in right cols)
		
		pd.merge(df2, df1) # order is important
		pd.merge(df1, df2, on='colName') # only wrt matching in 'colName'
		pd.merge(df1, df2, on=colNameList)
		
		# adding suffixes to redundant column names
		pd.merge(df1, df2, on=colNameList, suffixes=['_df1', '_df2'])
		
		pd.merge(df1, df2, left_on='label1' right_on='label2')
		
		# when columns can be ordered
		pd.merge_ordered(df1, df2) # default: outer join
		pd.merge_ordered(df1, df2, on=colList, suffixes=['_df1','_df_2'])
		pd.merge_ordered(df1, df2, on=colList, fill_method='ffill')
		
		
		
- merging - join()

		df1.join(df2) # default: left join
		df1.join(df2, how='left')
		df1.join(df2, how='right')
		
- filter - map

		df.filter(lambda g:g['Units'].sum() > 35)
		
		# Create the Boolean Series: under10
		under10 = (titanic['age'] < 10).map({True:'under 10', False:'over 10'})
		# Group by under10 and compute the survival rate
		survived_mean_1 = titanic.groupby(under10)['survived'].mean()
		print(survived_mean_1)
		# Group by under10 and pclass and compute the survival rate
		survived_mean_2 = titanic.groupby([under10, 'pclass'])['survived'].mean()
		print(survived_mean_2)
		
		
- comparison operators

		ps = df['row'] # build panada series for comparision
		filter = np.logical_and(ps > 5, ps < 10) # build filter
		df_new = df[filter] # apply filter
		
		filter2 = df['CountryCode']=='CEB'
		df_new = df.loc[filter2,:]
		
		
- iteration

		for column in pd_dataFrame: # iterate over columns names
		    expression
		
		# iterate over rows & their label - iterrows() returns a pd series
		for label, row in pd_dataFrame.iterrows()
				expression # "row" is here a series, elements accessable by col names
				
- iterate over chunks of data:

		# Define count_entries()
		def count_entries(csv_file, c_size, colname):
		    """Return a dictionary with counts of
		    occurrences as value for each key."""
		    
		    # Initialize an empty dictionary: counts_dict
		    counts_dict = {}
		
		    # Iterate over the file chunk by chunk
		    for chunk in pd.read_csv(csv_file, chunksize=c_size):
		
		        # Iterate over the column in DataFrame
		        for entry in chunk[colname]:
		            if entry in counts_dict.keys():
		                counts_dict[entry] += 1
		            else:
		                counts_dict[entry] = 1
		
		    # Return counts_dict
		    return counts_dict
		
		# Call count_entries(): result_counts
		result_counts = count_entries('tweets.csv', 10, 'lang')
		
		# Print result_counts
		print(result_counts)
		
		
		# __________________________________________
		# Import the pandas package
		import pandas as pd
		
		# Initialize reader object: df_reader
		df_reader = pd.read_csv("ind_pop.csv", chunksize=10)
		
		# Print two chunks
		print(next(df_reader))
		print(next(df_reader))

- importing flat files

		import pandas as pd
		filename = 'winequality-red.csv'
		df = pd.read_csv(filename)
		df = pd.read_csv('file.sth', header=None) # labels are numbers by default
		df = pd.read_csv('file.sth', header=None, names=list_columns)
		df = pd.read_csv('file.sth', index_col=0) # omit first column
		df = pd.read_csv('file.sth', chunksize=10) # for big files just load chunks
		df = pd.read_csv('file.sth', na_values='-1') # set nan replacement
		df = pd.read_csv(file_messy, delimiter=' ', header=3, comment='#')
		df = pd.read_csv('file.sth', parse_dates=True) # parse to date time objects
		df = pd.read_csv(filename, index_col='blabla')
		df = pd.read_csv(filename, nrows=5, header=None)
		df = pd.read_csv(file, sep='\t', comment='#', na_values='Nothing')
		data_array = df.values
		
		filenames = ['1.csv', '2.csv']
		# using glob
		from glob import glob
		filenames = glob('sales*.csv')
		# with a loop
		dataframes = []
		for f in filenames:
			dataframes.append(pd.read_csv(f))
		# with list comprehension
		dataframes = [pd.read_csv(f) for f in filenames]		
		
- importing excel files

		import pandas as pd		
		file = 'battledeath.xlsx'
		xl = pd.ExcelFile(file) # Load spreadsheet
		print(xl.sheet_names) # Print sheet names
		
		# alternatively
		xl = pd.read_excel(url, sheetname=None)
		print(xl.keys()) # Print the sheetnames to the shell
		print(xl['1700'].head() # Print head of first sheet (using name, NOT index)
		
		# Load a sheet into a DataFrame by name: df1
		df1 = xl.parse('2004')
		print(df1.head())
		
		# Parse the first sheet and rename the columns:
		df2 = xl.parse(0, skiprows=[0], names=['Country', 'AAM due to War (2002)'])
		df3 = xl.parse(0, parse_cols=[0], skiprows=[0], names=['Country'])
		
		print(df2.head())
		print(df3.head())
		
- importing sas files

		import pandas as pd
		from sas7bdat import SAS7BDAT
		with SAS7BDAT('urbanpop.sas7bdat') as file:
			df_sas = file.to_data_frame()
			
- importing stata files

		import pandas as pd
		data = pd.read_stata('urbanpop.dta')
		
- importing hdf5 files

		import h5py
		data = h5py.File(filename, 'r') # respectively w for write
		for key in data.keys():
			print(key) # >>> contains 'meta', 'quality' and 'strain'
		
- importing matlab files

		import scipy.io
		mat = scipy.io.loadmat(filename) # > dict, keys=varNames, values=objects
				
- sql databases

		from sqlalchemy import create_engine
		import pandas as pd
		engine = create_engine('sqlite:///Northwind.sqlite')
		table_names = engine.table_names() # returns listc
		
		con = engine.connect()
		rs = con.execute("SELECLT * FROM table") # result
		df = pd.DataFrame(rs.fetchall())
		con.close()
		
		with engine.connect() as con: # context manager
			rs = con.execute("SELECT * FROM table") # result
			df = pd.DataFrame(rs.fetchmany(size=5))
			df.columns = rs.keys()
			
			# alternatively
			df = pd.read_sql_query("SELECT * FROM table", engine)
			
- fetching data from www -> web files

		from urllib.request import urlretrieve
		url = "http://www.../winequality-white.csv"
		urlretrieve(url, 'winequality-white.csv')
		
- fetching data from www -> http -> BeautifulSoup

		from urllib.request import urlopen, Request
		import requests
		from bs4 import BeautifulSoup
		
		url = "http://www.wikipedia.com"
		request = Request(url)
		response = urlopen(request) # return of http response object
		html = response.read()
		print(html)
		response.close()
		
		url = "http://www.datacamp.com/teach/documentation"
		r = requests.get(url)
		html_doc = r.text
		
		soup = BeautifulSoup(html_doc)
		print(soup.printify())
		print(soup.title)
		print(soup.get_text())
		for link in soup.find_all('a'):
			print(link.get('href'))
			
- importing from json

		import json
		with open('snakes.json', 'r') as json_file:
			json_data = json.load(json_file) # returns dict
		for key, value in json_data.items():
			print(key + ' : ' + value)

- api's

		import requests
		url = "http://www.omdbapi.com/?t=hackers" # with query string "?t=hackers"
		r = requests.get(url)
		json_data = r.json()
		for key, value in json_data.items():
			print(key + ' : ' + value)
			
		url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'
		r = requests.get(url)
		json_data = r.json()
		pizza_extract = json_data['query']['pages']['24768']['extract']
		print(pizza_extract)
		
- twitter api

		import tweepy, json
		access_token = "***"
		access_token_secret = "***"
		consumer_key = "***"
		consumer_secret = "***"
		auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
		auth.set_access_token(access_token, access_token_secret)
		
		l = MyStreamListener() # Initialize Stream listener
		stream = tweepy.Stream(auth, l) # Create Stream object
		# Filter Twitter Streams to capture data by the keywords:
		stream.filter(track=['clinton', 'trump', 'sanders', 'cruz'])
		
- exporting df to files	
		
		df.to_csv['clean_data.csv']
		df.to_csv['clean_data.csv', sep='\t’]
		df.to_excell['clean_data.xlsx']
		df.to_csv(file_clean, index=False) # no index row
		
- datetime and pandas - iso 8601 format
	check datetime index

			df.info()
			
	construct pd series with datetime as index

			# Prepare a format string: time_format
			time_format = '%Y-%m-%d %H:%M'
			
			# Convert date_list into a datetime object: my_datetimes
			my_datetimes = pd.to_datetime(date_list, format=time_format)  
			
			# Construct a pandas Series using temperature_list and my_datetimes: time_series
			time_series = pd.Series(temperature_list, index=my_datetimes)
			
	partial datetime string selection/indexing - eg using datetime as index_col
		
			df.loc['2015-2-5 11:00:00', 'Company'] # specific datetime
			df.loc['2015-2-5'] # whole day - returns table
			df.loc['2015-Feb-5']
			df.loc['2015-2] # specific month
			df.loc['2015] # specific year
			df.loc['2015-2-16':'2015-2-20'] # slicing
			df.loc['February 5, 2015']
			
	converting strings to datetime

			dt = pd.to_datetime(['2015-2-11 20:00', '2015-2-11 21:00', '2015-2-11 22:00'])
			
	reindexing dataFrame

			# returns new df for specific datetime index
			df_new = df.reindex(dt) 
			# in case of not being available > filled with NaN's by default
			df_new = df.reindex(dt, method='ffill') # forward fill
			# filling with nearest preceding (lower) non-null fill
			df_new = df.reindex(dt, method='bfill') # backward fill
			# opposite of ffill
			# Reindex df2 with index of df2 without fill method: df3
			df3 = df2.reindex(df1.index)
			# Reindex with fill method, using forward fill: df4
			df4 = df2.reindex(df1.index, method='ffill')
			
			# Extract temperature data for August: august
			august = df.loc['2010-08', 'Temperature']
			# Downsample to obtain only the daily highest temperatures in August: august_highs
			august_highs = august.resample('D').max()
			# Extract temperature data for February: february
			february = df.loc['2010-02', 'Temperature']
			# Downsample to obtain the daily lowest temperatures in February: february_lows
			february_lows = february.resample('D').min()
			
	resampling - statistical methods over different time intervals: mean(), sum(), count(), var(), std(), etc.

			daily_mean = sales.resample('D').mean()
			# 1 method resample needs string to specify frequency: 'min'/'T', 'H'our, 'D'ay, 'B'uisnesday, 'W'eek, 'M'onth, 'Q'uarter, 'Y'ear, ... adding numbers: 2W two week intervals
			# 2 resample() method is chained with mean() method
			# 3 default: missing days are filled with NaN

	downsampling - reduce datetime rows to slower frequency - e.g. dayly > weekly

			# Downsample to 6 hour data and aggregate by mean: df1
			df1 = df.loc[:,'Temperature'].resample('6h').mean()
			
			# Downsample to daily data, count number of data points: df2
			df2 = df.loc[:,'Temperature'].resample('D').count()

	upsampling - increase datetime rows to faster frequency
	
			two_days.resample('4H').ffill() # fwd fill null entries
			
			# first() fills NaN for years in between
			population.resample('A').first()
			
			# linear interpolation - missing data!
			population.resample('A').first().interpolate('linear')
			
	rolling means - to smoothen fluctuations by the 'window' parameter - use method chaining

			# Extract data from 2010-Aug-01 to 2010-Aug-15: unsmoothed
			unsmoothed = df['Temperature']['2010-08-01':'2010-08-15']
			# Apply a rolling mean with a 24 hour window: smoothed
			smoothed = unsmoothed.rolling(window=24).mean()
			# Create a new DataFrame with columns smoothed and unsmoothed: august
			august = pd.DataFrame({'smoothed':smoothed, 'unsmoothed':unsmoothed})
			# Plot both smoothed and unsmoothed data using august.plot().
			august.plot()
			plt.show()
			
			# Extract the August 2010 data: august
			august = df['Temperature']['2010-08']
			
			# Resample to daily data, aggregating by max: daily_highs
			daily_highs = august.resample('D').max()
			
			# Use a rolling 7-day window with method chaining to smooth the daily high temperatures in August
			daily_highs_smoothed = august.resample('D').max().rolling(window=7).mean()
			print(daily_highs_smoothed)
			
- string methods

	 	# changes strings
	 	sales['Company'].str.upper() 
		
		# substr matching returns filter/boolean dataFrame
		sales['Product'].str.contains('ware')
		sales['Product'].str.contains('ware').sum() # adds booleans T+T=2
		
		# strip whitespace

		# Extract rows for which 'NOC' is either 'USA' or 'URS': is_usa_urs
		is_usa_urs = medals.NOC.isin(['USA', 'URS'])
		
- datetime methods

		# extract hour of the day
		sales['Date'].dt.hour 
		
		# transfer between timezones
		central = sales['Date'].dt.tz_localize('US/Central') # set
		central = sales['Date'].dt.tz_convert('US/Eastern') # convert
		sales['Date'].dt.tz_localize('US/Central')dt.tz_convert('US/Eastern') # chain method
		
- missing data - interpolation

		# Reset index of ts2 to ts1, use lin interpol to fill in the NaNs
		ts2_interp = ts2.reindex(ts1.index).interpolate(how='linear')
		# Compute the absolute difference of ts1 and ts2_interp
		differences = np.abs(ts1 - ts2_interp)
		print(differences.describe())
		
- visualization
	plot types: attribute 'kind=' area, histogram, ...
  subplots, separate plots: '(...).plot(subplots=True)'
  style format string:
  - color (k:black, g:green, r:red, c:cyan) 
  - marker (.:dot, o:circle, *:star, s:square, +:plus)
  - line type (-:solid, :dotted, -:dashed)

		# Plot the raw data before setting the datetime index
		df.plot()
		plt.show()
		
		# Convert the 'Date' column into a collection of datetime objects: df.Date
		df.Date = pd.to_datetime(df['Date'])
		
		# Set the index to be the converted 'Date' column
		df.set_index('Date', inplace=True)
		
		# Re-plot the DataFrame to see that the axis is now datetime aware!
		df.plot()
		plt.show()
		
- weather project

		# Import pandas
		import pandas as pd
		
		# Read in the data file: df
		df = pd.read_csv(data_file)
		# Print the output of df.head()
		print(df.head())
		# Read in the data file with header=None: df_headers
		df_headers = pd.read_csv(data_file, header=None)
		# Print the output of df_headers.head()
		print(df_headers.head())
		
		# Clean and Tidy
		# Split on the comma to create a list: column_labels_list
		column_labels_list = column_labels.split(',')
		# Assign the new column labels to the DataFrame: df.columns
		df.columns = column_labels_list
		# Remove the appropriate columns: df_dropped
		df_dropped = df.drop(list_to_drop, axis='columns')
		# Print the output of df_dropped.head()
		print(df_dropped.head())
		
		# Clean and transform dateTime columns
		# Convert the date column to string: df_dropped['date']
		df_dropped['date'] = df_dropped['date'].astype(str)
		# Pad leading zeros to the Time column: df_dropped['Time']
		df_dropped['Time'] = df_dropped['Time'].apply(lambda x:'{:0>4}'.format(x))
		# Concatenate the new date and Time columns: date_string
		date_string = df_dropped['date'] + df_dropped['Time']
		# Convert the date_string Series to datetime: date_times
		date_times = pd.to_datetime(date_string, format='%Y%m%d%H%M')
		# Set the index to be the new date_times container: df_clean
		df_clean = df_dropped.set_index(date_times)
		# Print the output of df_clean.head()
		print(df_clean.head())
		
		# Cleaning the numeric columns - missing values with 'M'
		# Print the dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011
		print(df_clean.loc['2011-06-20 08:00:00':'2011-06-20 09:00:00', 'dry_bulb_faren'])
		# Convert the dry_bulb_faren column to numeric values: df_clean['dry_bulb_faren']
		df_clean['dry_bulb_faren'] = pd.to_numeric(df_clean['dry_bulb_faren'], errors='coerce') # forces strings to be interpreted as NaN
		# Print the transformed dry_bulb_faren temperature between 8 AM and 9 AM on June 20, 2011
		print(df_clean.loc['2011-06-20 08:00:00':'2011-06-20 09:00:00', 'dry_bulb_faren'])
		# Convert the wind_speed and dew_point_faren columns to numeric values
		df_clean['wind_speed'] = pd.to_numeric(df_clean['wind_speed'], errors='coerce')
		df_clean['dew_point_faren'] = pd.to_numeric(df_clean['dew_point_faren'], errors='coerce')
		
		# Signal Max/Min/Median
		# Print the median of the dry_bulb_faren column
		print(df_clean['dry_bulb_faren'].median())
		# median of dry_bulb_faren col time range '2011-Apr':'2011-Jun'
		print(df_clean.loc['2011-04':'2011-06', 'dry_bulb_faren'].median())
		# Print median of the dry_bulb_faren column for month January
		print(df_clean.loc['2011-01', 'dry_bulb_faren'].median())
		
		# Signal variance
		# Downsample df_clean by day and aggregate by mean
		daily_mean_2011 = df_clean.resample('D').mean()
		# Extract dry_bulb_faren col from daily_mean_2011 using .values
		daily_temp_2011 = daily_mean_2011['dry_bulb_faren'].values
		# Downsample df_climate by day and aggregate by mean
		daily_climate = df_climate.resample('D').mean()
		# Extract Temperature col from daily_climate using .reset_index()
		daily_temp_climate = daily_climate.reset_index()['Temperature']
		# difference between the two arrays - print the mean difference
		difference = daily_temp_2011 - daily_temp_climate
		print(difference.mean())
		
		# Sunny or cloudy
		# Using df_clean, when is sky_condition 'CLR'?
		is_sky_clear = df_clean['sky_condition']=='CLR'
		# Filter df_clean using is_sky_clear
		sunny = df_clean.loc[is_sky_clear]
		# Resample sunny by day then calculate the max
		sunny_daily_max = sunny.resample('D').max()
		# See the result
		sunny_daily_max.head()
		# Using df_clean, when does sky_condition contain 'OVC'?
		is_sky_overcast = df_clean['sky_condition'].str.contains('OVC')
		# Filter df_clean using is_sky_overcast
		overcast = df_clean.loc[is_sky_overcast]
		# Resample overcast by day then calculate the max
		overcast_daily_max = overcast.resample('D').max()
		# See the result
		overcast_daily_max.head()
		# From previous steps
		is_sky_clear = df_clean['sky_condition']=='CLR'
		sunny = df_clean.loc[is_sky_clear]
		sunny_daily_max = sunny.resample('D').max()
		is_sky_overcast = df_clean['sky_condition'].str.contains('OVC')
		overcast = df_clean.loc[is_sky_overcast]
		overcast_daily_max = overcast.resample('D').max()
		# Calculate the mean of sunny_daily_max
		sunny_daily_max_mean = sunny_daily_max.mean()
		# Calculate the mean of overcast_daily_max
		overcast_daily_max_mean = overcast_daily_max.mean()
		# Print the difference (sunny minus overcast)
		print(sunny_daily_max_mean - overcast_daily_max_mean)
		
		# Visual EDA
		# Import matplotlib.pyplot as plt
		import matplotlib.pyplot as plt
		# Select the visibility and dry_bulb_faren columns and resample them: weekly_mean
		weekly_mean = df_clean[['visibility','dry_bulb_faren']].resample('W').mean()
		# Print the output of weekly_mean.corr()
		print(weekly_mean.corr())
		# Plot weekly_mean with subplots=True
		weekly_mean.plot(subplots=True)
		plt.show()
		# Sunny Hours
		is_sky_clear = df_clean['sky_condition'] == 'CLR'
		resampled = is_sky_clear.resample('D')
		sunny_hours = resampled.sum()
		total_hours = resampled.count()
		sunny_fraction = sunny_hours / total_hours
		# Make a box plot of sunny_fraction
		sunny_fraction.plot(kind='box')
		plt.show()	
		# Heat of Humidity
		# Resample dew_point_faren and dry_bulb_faren by Month, aggregating the maximum values: monthly_max
		monthly_max = df_clean[['dew_point_faren', 'dry_bulb_faren']].resample('M').max()
		# Generate a histogram with bins=8, alpha=0.5, subplots=True
		monthly_max.plot(kind='hist', bins=8, alpha=0.5, subplots=True)
		# Show the plot
		plt.show()
		# Probability of high temperatures
		# Extract the maximum temperature in August 2010 from df_climate: august_max
		august_max = df_climate.loc['2010-Aug','Temperature'].max()
		print(august_max)
		# Resample August 2011 temps in df_clean by day, aggregate max value
		august_2011 = df_clean.loc['2011-Aug','dry_bulb_faren'].resample('D').max()
		# Filter for days in august_2011 where the value exceeds august_max
		august_2011_high = august_2011.loc[august_2011 > august_max]
		# Construct a CDF of august_2011_high
		august_2011_high.plot(kind='hist', normed=True, cumulative=True, bins=25)
		# Display the plot
		plt.show()
		
		
		
		
		
		

		
					

________________
<a name="python_cleaning_data"/>
### cleaning data

- see also: pd.methods and attributes
- types of data, constistency > "df.info()"
- column names, capital letters > "df.columns"
- nan-values > "df.info()"
- frequency count, constistency, categorical data > "df.columnName.value_counts(dropna=False)"
- frequency count > "df['columnName'].value_counts(dropna=False).head()"

- outliers > "df.describe()" summary
- find outlier in visualisation: bar-plots <> discrete | histograms <> continuous
- histogram: 
		
		import matplolib.pyplot as plt
		df.columnName.plot('hist')
		plt.show() # outliers over 1000000000
 		df[df.population > 1000000000]

- boxplots

		import matplolib.pyplot as plt
		df.boxplot(column='population', by='continent')
		plt.show()
		
- scatterplot - rlt-ship btw two variables - flag potentially bad data if one variable is not enough (barplot/histogram) to find bad data

		df.plot(kind='scatter', x='initial_cost', y='total_est_fee', rot=70)
		plt.show()
		
- multi-index - hierachical columns
- tidy data - principles:
	- columns represent separate variables
	- rows represent individual observations
	- observational units form a table
- there are different purposes (analysis, reporting) for diffnt shapes of data
- to transfer untidy into tidy data, use pd.melt() <-inverse-> df.pivot()
	- convert many columns into one - i.e. turn columns into rows
	- melt() i.g. every entry is one row of the new table with two columns: 1: columnName, 2: value

			# specify further remaining columns by id_vars
			# default column names: variable, value
			df_melt = pd.melt(frame= df, id_vars='name', value_vars=['treatment a', 'treatment b'])
						
			df_melt = pd.melt(frame= df, id_vars='name', value_vars=['treatment a', 'treatment b'], var_name='treatment', value_name='result')
			
			df_melt = pd.melt(df, col_level=0) # omit all index cols

	- pivoting data - turn rows into columns - turn one COLUMN into separate ones (for each category in the former col) with VALUES of another column

			df_tidy = df.pivot(index='date', columns='element', values='value')
			# problem: duplicate entries
			# want to summarize them statistically
			
			df_tidy = df.pivot_table(index='date', columns='element', value='value', aggfunc=np.mean)
			# mean value for duplicate entries
			# default: average. other: 'count', sum, len, ...
			
			df_tidy = df.pivot_table(index='date', columns='element', value='value', aggfunc=sum, margins=True)
			# adds 'ALL' row in the end
			
			print(df_pivot.index)
			# Reset the (Mulit-)index of df_pivot: df_pivot_reset
			df_pivot_reset = df_pivot.reset_index()
			print(df_pivot_reset.index)
			print(df_pivot_reset.head())
			
	- melting and creating new variables:

			tb_melt = pd.melt(tb, id_vars=['country', 'year']) # Melt tb: tb_melt
			tb_melt['gender'] = tb_melt.variable.str[0] # Create 'gender' column
			tb_melt['age_group'] = tb_melt.variable.str[1:] # Create 'age_group' col
			print(tb_melt.head()) # Print the head of tb_melt
			
			# Melt ebola: ebola_melt
			ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')
			# Create the 'str_split' column
			ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')
			# Create the 'type' column
			ebola_melt['type'] = ebola_melt.str_split.str.get(0)
			# Create the 'country' column
			ebola_melt['country'] = ebola_melt.str_split.str.get(1)
			# Print the head of ebola_melt
			print(ebola_melt.head())
			
			# Melt gapminder: gapminder_melt
			gapminder_melt = pd.melt(gapminder, id_vars='Life expectancy')
			# Rename the columns
			gapminder_melt.columns = ['country', 'year', 'life_expectancy']
			
	- unstacking multi-index - stacking hierachical columns

			# put an index to hierachical columns - similar to pivot method
			df.unstack(level='indexName')
			df.unstack(level=1) # unstack second index (first is 0)
			
			# stacking hierachically columns - make wide df thinner/longer
			df.stack(level='columnName')
			
	- swapping levels: inner index <> outer index

			df.swaplevel(0,1) # swapping 1st and 2nd index level
			df.sort_index # then sort for swapped indices
			
			
- combining/concatenate and cleaning huge datasets

		import pandas as pd
		
		# concatinating rows
		concatenated = pd.concat(['weather_p1', 'weather_p2'])
		# row index keeps original row index label -> reset index lable
		concatenated = pd.concat(['weather_p1', 'weather_p2'], ignore_index=True)
		
		# concatinating columns
		ebola_tidy = pd.concat([ebola_meld, status_country], axis=1)
		print(ebola_tidy.shape)
		print(ebola_tidy.head())

- globbing - pattern matching for file names

		import glob
		csv_files = glob.glob('*.csv')
		print(csv_files)
		list_data = [] # list of dataFrames
		for filename in csv_files:
			data = pd.read_csv(filename)
			list_data.apend(data)
		pd.concat(list_data)
			
- merging data - joining tables - many types of merge: one-to-one | one-to-many/many-to-one | many-to-many depending on duplicates

		import pandas as pd
		pd.merge(left='state_populations', right='state_code', on='None', left_on='state', right_on='name') # with common or not_common key
			
- data type conversion

		df.dtypes # data types		

		# Convert the sex column to type 'category'
		# uses less memory - speeds up operations like groupby()
		tips.sex = tips.sex.astype('category')
		# Convert the smoker column to type 'category'
		tips.smoker = tips.smoker.astype('category')
		# Print the info of tips
		print(tips.info())
		
		# Convert 'total_bill' to a numeric dtype
		tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')
		# Convert 'tip' to a numeric dtype
		tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')			
		# Print the info of tips
		print(tips.info())
			
- string manipulation - regular expressions <> pattern matching

		import re
		
		pattern = re.compile('\$\d*\.\d{2}')
		result = pattern.match($17.89)
		print(bool(result))
		
		prog = re.compile('\d{3}-\d{3}-\d{4}')
		result = prog.match('123-456-7890')
		print(bool(result))
		result2 = prog.match('1123-456-7890')
		print(bool(result2))
		
		matches = re.findall('\d+', 'the recipe calls for 10 strawberries and 1 banana') # returns a list
		print(matches)
		
		pattern1 = bool(re.match(pattern='\d{3}-\d{3}-\d{4}', string='123-456-7890'))
		print(pattern1)
		
		pattern2 = bool(re.match(pattern='\$\d*\.\d{2}', string='$123.45'))
		print(pattern2)
		 
		pattern3 = bool(re.match(pattern='[A-Z]\w*', string='Australia'))
		print(pattern3) # A capital letter, followed by an arbitrary number of alphanumeric character
		
		countries = gapminder.country
		countries = countries.drop_duplicates()
		pattern = '^[A-Za-z\s\.]*$' # Write the regular expression: pattern
		mask = countries.str.contains(pattern) #  Create the Boolean vector: mask
		mask_inverse = ~mask # Invert the mask: mask_inverse
		invalid_countries = countries.loc[mask_inverse]
		print(invalid_countries)
			
- using functions

		df.apply(np.mean, axis=0) # mean over columns
		df.apply(np.mean, axis=1) # mean over rows
		
		import re
		from numpy import NaN
		patter = re.compile('^\$\d*\.\d{2}$')
		
		def diff_money(row, pattern)
			icost = row['Initial Cost']
			tef = row['Total Est. Fee']
			if bool(pattern.match.icost) and bool(pattern.match.tef):
				icost = icost.replace("$", "")
				tef = tef.replace("$", "")
				icost = float(icost)
				tef = float(tef)
				return icost - tef
			else:
				return NaN
		
		df_subset['diff'] = df_subset.apply(diff_money, axis=1, pattern=pattern)
		
		# Write the lambda function using replace
		tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))
		# Write the lambda function using regular expressions
		tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\d+\.\d+', x)[0])
		# Print the head of tips
		print(tips.head())
			
- duplicates, missing value

		df.drop_duplicates()
		df.dropna() # drop missing data > get complete case, losing a  lot
		df[column] = df[column].fillna('missing')
		df[column] = df[column].fillna(0)
		
		df_new = df.fillna(df[column].mean())
		
		
	- missing data: leave as it is | drop them | fill them
	- "fd.info()" >> overview over missing data
	- "df.dropna()" dropping rows
	- dropping columnx also possible
	- "df_new = df.fillna('missing')" filling in
	- "df_new = df.fillna(df[column].mean())" filling in with test statistics

- assert statements - true if true, error if false

		# Assert that there are no missing values
		assert ebola.notnull().all().all() # all means logical and over all entries
		
		# Assert that all values are >= 0
		assert (ebola >= 0).all().all()
		
		# Convert the year column to numeric
		gapminder.year = pd.to_numeric(gapminder.year)
		# Test if country is of type object
		assert gapminder.country.dtypes == np.object
		# Test if year is of type int64
		assert gapminder.year.dtypes == np.int64
		# Test if life_expectancy is of type float64
		assert gapminder.life_expectancy.dtypes == np.float64


- write finally to file

		df.to_csv['clean_data.csv']
		df.to_csv['clean_data.csv', sep='\t’]
		df.to_excell['clean_data.xlsx']
		
________________
<a name="python_visualization"/>
### 	

#### data visualization - reminder

- line plots

		import numpy as np
		import matplotlib.pyplot as plt
		x = np.linspace(0,1,201)
		y = np.sin((2*np.pi*x)**2)
		plt.plot(x,y,'red')
		plt.show()
		
- scatter plots

		import numpy as np
		import matplotlib.pyplot as plt
		x = 10*np.random.rand(200,1)
		y = (0.2 + 0.8*x) * np.sin(2*np.pi*x) + \
			np.random.randn(200,1)
		plt.scatter(x,y)
		plt.show()
		
- histograms

		import numpy as np
		import matplotlib.pyplot as plt
		x = 10*np.random.rand(200,1)
		y = (0.2 + 0.8*x) * \
			np.sin(2*np.pi*x) + \
			np.random.randn(200,1)
		counts, bins, patches = plt.hist(y, bins=20) # returns arrays!
		plt:show()
		
#### data visualization - introduction

- possible arguments of plots: np.arrays, lists, pd.series
- strategies: plotting graphs on common axes - creating axes within figure - creating subplots within a figure

		import matplotlib.pyplot as plt
		plt.plot(t, temperature, 'r') # not drawn yet, just in memory
		plt.show(t, dewpoint, 'b') # appears on same axis
		plt.xlabel('Date') # x-axis label
		plt.title('Temperature & Dew Point') # title
		plt.show() # renders plot objects to screen

- multiple axes in same figure - axes([x_lo, y_lo, width, height]), units between 0 and 1 (figure dimensions)

		plt.axes([0.05, 0.05, 0.425, 0.9]) # left side active
		plt.plot(t, temperature, color='r') # alt: omit 'color='
		plt.xlabel('Date')
		plt.title('Temperature')
		plt.axes([0.525, 0.05, 0.425, 0.9]) # right side active
		plt.plot(t, dewpoint, 'b')
		plt.xlabel('Date')
		plt.title('Dew Point')
		plt.show()
		
- subplots - subplot(nrows, ncols, nsubplot) - subplot grid ordering: row-wise top>left, indexed from 1

		plt.subplot(2,1,1) # top plot active
		plt.plot(t, temperature, 'r')
		plt.xlabel('Date')
		plt.title('Temperature')
		plt.subplot(2,1,2) # bottom plot active
		plt.plot(t, dewpoint, 'b')
		plt.xlabel('Date')
		plt.title('Dew Point')
		plt.tight_layout() # pads spaces between plots > no overlap
		plt.show()

- control axis extents/range

		plt.axis([xmin, xmax, ymin, ymax])
		plt.axis('off') # turns off axis lines, labels
		plt.axis('equal') # euqal x/y scaling
		plt.axis('square') # forces quare plot
		plt.axis('tight') # sets xlim(), ylim() to show all data
		
		plt.xlim([xmin, xmax]) # separate
		plt.ylim([ymin, ymax])
		plt.xlim((-2,-3)) # use tuples
		plt.xlim([-2,-3]) # use lists
		
- save image

		plt.savefig('graphic.png')
		
- legends - box with labels - instead strings: numbers...

		plt.plot(x_data, y_data, label='labelName') # content of legend
		plt.xlabel('sepal length (cm)') # content of legend
		plt.ylabel('sepal width (cm)') # content of legend
		plt.xticks(x_vals, x_labs, rotation=60) # rot labels x-axis out
		plt.title('Iris data') # content of legend
		
		plt.legend(loc='upper right') # location of the legend
		plt.legend(loc='lower left') # variation
		plt.legend(loc='center center') # variation
		plt.legend(loc='best')
		plt.legend(loc='right')
		
- plot annotations - text labels/arrows using annotate() method - flexible specification of coordinates - options: 's':text, 'xy':coordinates to annotate, 'xytext': coordinates of label, 'arrowprops':controls drawing of arrow  - keyword arrowprops: dict of arrow properties: width, color, ...
		
		# without arrows
		plt.annotate('sertosa', xy=(5.0,3.5))
		plt.annotate('virfinica', xy=(7.25,3.5))
		plt.annotate('versicolor', xy=(5.0,2.0))
		
		# for arrows
		plt.annotate(
			'sertosa',
			xy=(5.0,3.5)
			xytext=(4.25,4.0),
 			arrowprops={'color':'red'})
		plt.annotate(
			'virfinica',
			xy=(7.25,3.5)
			xytext=(6.5,4.0),
 			arrowprops={'color':'blue'})
		plt.annotate(
			'versicolor',
			xy=(5.0,2.0)
			xytext=(5.5,1.75),
 			arrowprops={'color':'green'})
 			
- plot styles - style sheets in Matplotlib - default lines/points/backgrounds/...

		plt.style.use('ggplot') # switch styles globally
		plt.style.use('fivethirtyeight')
		plt.style.available # list of styles

#### data visualization - 2d arrays

- numy review -> see above
	- homogeneous in type
	- calculations all at once - without loops
	- indexing with brackets
		
			a[i0] # 1d array
			a[i0][i1] # 2d array
			a[s0][s1] # slicing arrays w slices s0/s1 eg start:stop:stride
			
- pseudo color plot with plt.pcolor() and np.meshgrid()

		import numpy as np
		import matplotlib.pyplot as plt
		
		# example 1 - use np.meshgrid
		u = np.linspace(-2, 2, 3)
		v = np.linspace(-1, 1, 5)
		X,Y = np.meshgrid(u, v) # construct 2d array
		Z1 = X**2/25 + Y**2/4
		
		# example 2 - warning: pcolor(Z) draws Z bottom row > top row
 		Z2 = np.array([[1,2,3],[4,5,6]])
 		
 		# example 3
 		u = np.linspace(-2, 2, 41)
		v = np.linspace(-1, 1, 21)
		X,Y = np.meshgrid(u,v)
		Z3 = np.sin(3*np.sqrt(X**2 + Y**2)) 
		print('Z3:\n', Z3)
		
		# use pcolor() for pseudo color plot 
		Z = Z1
		plt.set_cmap('grayscale') # add color map
		plt.pcolor(Z)
		plt.pcolor(Z, cmap='gray') # jet, coolwarm, magma, viridis
		plt.pcolor(Z, cmap='autumn') # summer, winter, spring
		plt.pcolor(Z, cmap='Greens') # Blues, Reds, Purples
		plt.pcolor(X, Y, Z) # use meshgrid X, Y
		plt.colorbar() # adds vertical colorbar to the right
		plt.axis('tight') # no empty space around pseudocolor plot
		plt.show()
		
		# use contour() for contour lines
		plt.contour(Z)
		plt.contour(Z, 30) # add number of contour lines
		plt.contour(X, Y, Z, 30) # use meshgrid X, Y
		plt.contourf(X, Y, Z, 30) # filled in between contour plots
		plt.colorbar()
		plt.show()
		
- 2d points/scatter plot > distribution - create 2d histo from x&y
- many shapes available: rectangles, hexagons, ...

		# rectagles
		plt.hist2d(x,y, bins=(10,20), ) # x,y 1d arrays of same lenght
		plt.hist2d(x,y, bins=(10,20), range=((xmin, xmax), (ymin, ymax)))
		
		# hexagons
		plt.hexbin(x,y, gridsize=(15,10)) # similar function
		plt.hexbin(x,y, gridsize=(15,12), extent=(40, 235, 8, 48))
		
		plt.colorbar()
		plt.xlabel('weight ($\mathrm{kg}$)')
		plt.ylabel('acceleration ($\mathrm{ms}^{-2}$)')
		plt.show()
		
- image
	- grayscale images - rectangular 2d arrays
	- color images - typically: 3 rectangular 2d arrays - rgb
	- channel values: 0...1 (float) or 0...255 (8 bit)
- loading images

		img = plt.imread('sunflower.jpg') # reading files
		print(img.shape) # returns size and number of channels
		
		plt.imshow(img) # displaying images
		plt.axis('off') # hide axes when displaying images
		plt.show()
		
- reduction to gray-scale image

		collapsed = img.mean(axis=2) # average over rgb channels > axis=2
		print(collapsed.shape)
		
		plt.set_cmap('gray') # want gray scale color
		plt.imshow(collapsed, cmap='gray')
		plt.axis('off')
		plt.show()
		
- pixel shape

		uneven = collapsed[::4, ::2] # nonuniform subsample
		print(uneven.shape)
		
		plt.imshow(uneven) # displays distorted image
		plt.imshow(uneven, aspect=2.0) # scaling ration of pixels
		plt.imshow(uneven, cmap='gray', extent=(0,640,0,480)) # l>r & b>t
		
		plt.axis('off')
		plt.show()
		
#### data visualization - seaborn

- library for statistical visualization - works best with pd.dataFrames
- reminder pd.dataFrames:
	- labelled tabular data structure
	- labels on rows: 'index' - labels on columns: 'columns'
	- columns are pd.series - columns of same type
	- rows of mixed types
- linear regression - relation of two columns

		import pandas as pd
		import matplotlib.pyplot as plt
		import seaborn as sns
		
		# mandatory parameters
		df_tips = sns.load_dataset('tips') # sns.load.dataset()
		sns.lmplot(x='totalbill', y='tip', data=df_tips) # sns.lmplot()
		plt.show()
		
		# grouping factors in same plot - using color-palette 'palette='
		sns.lmplot(x='to', y='ti', data=df, hue='sex', palette='set1')
		
		# congressmen affiliation to reps/democrates?
		plt.figure()
		sns.countplot(x='education', hue='party', data=df, palette='RdBu')
		plt.xticks([0,1], ['No', 'Yes'])
		plt.show()
		
		# grouping factors in subplots - col>horizontal, row>vertical
		sns.lmplot(x='totalbill', y='tip', data=df_tips, col='sex')
		sns.lmplot(x='totalbill', y='tip', data=df_tips, row='sex')
		
		# calculate residuals - difference to regr-line
		sns.residplot(x='age', y='fare', data=df, color='indianred')
		# more powerfull than lmplot - x,y arrays/strings - data optional
		
		# higher order polynomial regressions
		plt.scatter(auto['weight'], auto['mpg'], label='data', color='red', marker='o')
		sns.regplot(x='weight', y='mpg', data=auto, scatter=None, color='blue', label='order 1')
		sns.regplot(x='weight', y='mpg', data=auto, color='green', scatter=None, order=2, label='order 2')
		plt.legend(loc = 'upper right')
		plt.show()
		
- univariate data > strip/swarm/box/violin plots

		# strip plot
		sns.stripplot(y='tip', data=df_tips)
		sns.stripplot(x='day', y='tip', data=df_tips) # grouping via 'x'
		plt.ylabel('tip (&)')
		plt.show()
		
		# spread plot
		sns.stripplot(x='day', y='tip', data=df, size=4, jitter=True)
		
		# swarm plot	
		sns.swarmplot(x='day', y='tip', data=df)
		sns.swarmplot(x='day', y='tip', data=df, hue='sex') # grouping
		sns.swarmplot(x='day', y='tip', data=df, orient='h') # orientat.
		
		# box plot 1
		import matplolib.pyplot as plt
		df.boxplot(column='population', by='continent')
		plt.show()
		
		# box plot 2
		df.plot(kind='box')
		plt.show()	
		
		# box plot 3
		sns.boxplot(x='day', y='tip', data=df_tips)
		
		# violin plot
		sns.violinplot(x='day', y='tip', data=df_tips)
		
		# mixture of violin/spread plot
		sns.violinplot(x='day', y='tip', data=df_tips, inner=None)
		sns.stripplot(x='day', y='tip', data=df, size=4, jitter=True)
		
- bi/multivariate data - visualize relationship beetween data - join/pair plots, heat maps

		# joint plot - scatter plot + histogram in axes
		sns.jointplot(x='total_bill', y='tip', data=df_tips)
		
		# joint plot - using kde - smooth distribution - kernel dens dist
		sns.jointplot(x='total_bill', y='tip', data=df_tips, kind='kde')
		# other options 'kind': 'scatter', 'reg', 'resid', 'hex'
		
		# pair plot - diag: histo, off-diag: scatter/grouped strip plots
		sns.pairplot(df_tips)
		sns.pairplot(df_tips, hue='sex') # grouping using different color
		sns.pairplot(df_tips, kind='reg') # different off-diag plot
		
		# heat map - similar pseudo color plot of matrix
		# add functionality for pd.dataFrame
		sns.heatmap(df_covariance)
		
#### data visualization - misc

- time series
	- pandas time series: datetime as index
	- datetime: represents periods or time-stamps
	- datetime: specialized slicing
	
			weather['2010-07-04']
			weather['2010-03':'2010-o4']
			s = now.strftime("%d.%m.%Y") # formated datetime
				
	- moving window calc: average, medians, std over time-window - smoothing out oscillations 

- sharpen contrast in images -> rescaling

		# image histogram
		orig = plt.imread('low-contrast-moon.jpg')
		pixels = orig.flatten() # 2d -> 1d array
		
		plt.hist(pixels, bins=256, range=(0,256), normed=True, color='blue', alpha=0.3)
		
		# rescaling - using more the range of intensity values
		minval, maxval = orig.min(), orig.max()
		rescaled = (255/(maxval-minval)) * (pixels - minval)
		
		plt.hist(rescaling, bins=256, range=(0,256), normed=True, color='green', alpha=0.3)		plt.axis('off')
		plt.show
		
		# use cdf to sharpen image - cumulated density function
		plt.hist(pixels, bins=256, range=(0,256), normed=True, color='blue', alpha=0.3)
		plt.twinx() # two vertical axis left/right
		orig_cdf, bins, patches = plt.hist(pixels, cumulatice=True bins=256, range=(0,256), normed=True, color='red', alpha=0.3)
		
		# equalizing intensity values
		new_pixels = np.interp(pixels, bins[:-1], orig_cdf*255)
		new = new_pixels.reshape(orig.shape) # 1d -> 2d
		
		# plot
		plt.imshow(new)
		plt.acis('off')
		plt.title('Equalized image')
		plt.show()

________________
<a name="python_interactive_visualization"/>
### interactive data visualization

- bokeh framework
- html/js visualisation
		
#### bokeh - plotting interface
- glyphs:
	- visual shapes drawn on screen
		- circles, squares, triangles
		- rectangles, lines, wedges
	- with properties attached to data
		- coordinates (x,y)
		- size, color, transparency
		- single values, lists, arrays, pd.series, tuples, ...
		- single values: like list with const entries
- circles

		from bokeh.io import output_file, show # alt: output_notebook
		from bokeh.plotting import figure
		
		plot = figure(plot_width=400, tools='pan,box_zoom', x_axis_label='xLabel', y_axis_label='yLabel')
		# tools > comma sep string
		
		plot.circle([1,2,3,4,5], [8,6,5,2,3])
		plot.circle(x=10, y=[2,5,8,12], size=[10,20,30,40])
		# other std markers: asterisk(), circle(), circle_cross(), circle_x(), cross(), diamond(), diamond_cross(), inverted_triangle(), square(), square_cross(), square_x(), triangle(), x()
		# other arguments: color, size, alpha, fill_color
		
		output_file('circle.html')
		show(plot)
		
- lines

		from bokeh.io import output_file, show # alt: output_notebook
		from bokeh.plotting import figure
		
		x = [1,2,3,4,5]
		y = [8,6,5,2,3]
		
		plot = figure()
		plot.line(x, y, line_width=2)
		plot.circle(x, y, fill_color='white', size=2) # if want together
		
		output_file('line.html')
		show(plot)
		
- patches - useful for showing geographic regions - data given as "list of lists" - like patch-work

		from bokeh.io import output_file, show # alt: output_notebook
		from bokeh.plotting import figure
		
		xs = [[1,1,2,2], [2,2,4], [2,2,3,3]]
		ys = [[2,5,5,2], [3,5,5], [2,3,4,2]]
		plot = figure()
		plot.patches(
			xs, ys,
			fill_color=['red', 'blue', 'green'],
			line_color='white')
		output_file('patches.html')
		show(plot)
		
- other gyphs: annulus(), annular_wedge(), wedge(), rect(), quad(), vbar(), hbar(), image(), image_rgba(), image_url(), patch(), patches(), line(), multi_line(), circle(), oval(), ellipse(), arc(), quadratic(), bezier()

- use np.arrays

		from bokeh.io import output_file, show # alt: output_notebook
		from bokeh.plotting import figure
		import numpy as np
		
		# def np.arrays
		x = np.linspace(0, 10, 1000)
		y = np.sin(x) + np.random.random(1000) * 0.2
		
		plot = figure()
		plot.line(x,y)
		output_file('numpy.html')
		show(plot)
		
- use pd.dataFrames

		from bokeh.io import output_file, show # alt: output_notebook
		from bokeh.plotting import figure
		from bokeh.sampledata.iris import flowers # flowers is a pd.df
		
		plot = figure()

		plot.circle(
			flowers['petal_length'], # insert column 1 of df
			flowers['sepal_length'], # insert column 2 of df
			size = 10)

		plot.circle(df['hp'], df['mpg'], color=df['color'], size=10)

		output_file('pandas.html')
		show(plot)
		
- column data source 
	- data structure is central in bokeh
	- python > javaScript
	- similar but simpler to pd.df
	- maps string column names to sequences of data
	- often created automatically for you
	- can be shared between glyphs
	- extra columns can be used with hover tooltips

			from bokeh.models import ColumnDataSource
			
			source = ColumnDataSource(data={
				'x': [1,2,3,4,5], # list values in data must be same length
				'y': [8,6,5,2,3]})
			source.data
			
			# get columnDataSource from dataFrame
			source = ColumnDataSource(df)
			
			# scatter plot of two columns applying color column
			p.circle('Year', 'Time', source=source, size=8, color='color')
			
- customizing glyphs

		# appearance of selected/non-selected pts
		plot = figure(tools='box_select, lasso_select') 
		plot.circle(
			petal_length, sepal_length,
			selection_color='red',
			nonselection_alpha=0.2,
			nonselection_fill_color='grey')
		
		# hover appearance
		from bokeh.models import HoverTool
		hover = HoverTool(tooltips=None, mode='hline') # all pts on hline
		plot = figure(tools=[hover, 'crosshair'])
		plot.circle(x_data, y_data, size=15, hover_color='red')
		plot.add_tools(hover) # or adding tools afterwards
		
		# color mapping - two arguments:lst of values to map, lst of clrs
		from bokeh.models import CategoricalColorMapper
		mapper = CategoricalColorMapper(
			factors=['setosa', 'virginica', 'versicolor']
			palette=['red', 'green', 'blue'])
		plot = figure(
			x_axis_label='petal_length',
			y_axis_label='sepal_length')
		plot.circle(
			'petal_length',
			'sepal_length',
			size=10,
			source=source,
			legend='species'
			color={ # color argument > pass a dictionary
				'field': 'species', # value name of ???
				'transform': mapper}) # value is colormap
	
#### bokeh - layouts

- arrange multiple plots/contols on a page
- rows, columns, nested

		from bokeh.layouts import row, column
		
		layout = row(p1, p2, p3) # arguments: plots
		output_file('row.html')
		show(layout)
		
		layout = column(p1, p2, p3)
		output_file('column.html')
		show(layout)
		
		layout = row(column(p1, p2), p3)
		output_file('nested.html')
		show(layout)
		
		row2 = column([mpg_hp, mpg_weight], sizing_mode='scale_width')
		layout = row([avg_mpg, row2], sizing_mode='scale_width')
		output_file('layout_custom.html')
		show(layout)

- grid arrangements

		from bokeh.layouts import gridplot
		layout = gridplot(
			[[None, p1], [p2, p3]], # list of rows
			toolbar_location=None)
		output_file('nested.html')
		show(layout)
		
- tabbed layouts

		from bokeh.models.widgets import Tabs, Panel
		
		# create panel with title for each tab
		first = Panel(child=row(p1,p2), title='first')
		second = Panel(child=row(p3), title='second')
		
		# put the panels in a tab object
		tabs = Tabs(tabs=[first, second])
		output_file('tabbed.html')
		show(tabs)

#### linking plots together - eg linked panning - const while int.acting
- linking axes

		p3.x_range = p2.x_range = p1.x_range
		p3.y_range = p2.y_range = p1.y_range
		
- linking selections - data source is shared!

		p1 = figure(
			x_axis_label='fertility (children per woman)', y_axis_label='female literacy (% population)',
			tools='box_select,lasso_select')
		p1.circle(
			'fertility',
			'female literacy',
			source=source)
		p2 = figure(
			x_axis_label='fertility (children per woman)', y_axis_label='population (millions)',
			tools='box_select,lasso_select')
		p2.circle(
			'fertility',
			'population',
			source=source) # here source is shared!!
		layout = row(p1, p2)
		
#### annotations/guides
- help relate scale information to the viewer
	- axes, grids (default on most plots)
- explain visual encodings that are used
	- legends
- drill down into details not visible in the plot
	- hover tooltips

			# legends
			plot.circle('petal_length', 'sepal_length',
				size=10,
				source=source,
				legend='species'
				color={ # color argument > pass a dictionary
					'field': 'species', # value name of ???
					'transform': mapper}) # value is colormap
			plot.legend.location = 'top_left'
			plot.legend.background_fill_color = 'lightgray'
			
			# hover tooltips
			from bokeh.models import HoverTool
			hover = HoverTool(tooltips=[
				('species name','@species'), # diplayed when hovering a point
				('petal length','@petal_length'), # tuples (title, @column)
				('sepal length','@sepal_length')])
			plot = figure(tools=[hover, 'pan', 'wheel_zoom'])
			plot.circle(x_data, y_data, size=15, hover_color='red')
			plot.add_tools(hover) # or adding tools afterwards
			
#### bokeh - server

- basic app outline

		from bokeh.io import curdoc # doc holds plots/controls/layouts
		
		# create plots and widgets
		# add callbacks: fcts running after some event
		# arrange plots and widgets in layouts
		
		curdoc().add_root(layout)

- running bokeh applications - 

		bokeh serve --show myapp.py # run single module app shell/prompt
		bokeh serve --show myappdir/ # directory style app run similarly
		
- example 1

		from bokeh.io import curdoc
		from bokeh.plotting import figure
		plot = figure()
		plot.line([1,2,3,4,5], [2,5,4,6,7])
		curdoc().add_root(plot)
		
- example 2

		from bokeh.io import curdoc
		from bokeh.layouts import widgetbox
		from bokeh.models import Slider
		slider1 = Slider(title='slider1', start=0, end=10, step=0.1, value=2)
		slider2 = Slider(title='slider2', start=10, end=100, step=1, value=20)
		layout = widgetbox(slider1, slider2)
		curdoc().add_root(layout)
		
- connecting slider to plots

		from bokeh.io import curdoc
		from bokeh.layouts import column
		from bokeh.models import ColumnDataSource, Slider
		from bokeh.plotting import figure
		from numpy.random import random
		
		N = 300
		source = ColumnDataSource(data = {
		    'x': random(N),
		    'y': random(N)})
		
		plot = figure()
		plot.circle(x='x', y='y', source=source)
		
		slider = Slider(
		    start=100, end=1000, value=N, step=10,
		    title='Number of points')
		
		def callback(attr, old, new): # update source.data
		    N = slider.value
		    source.data = {'x': random(N), 'y': random(N)} 
	
		slider.on_change('value', callback) # 1st param: prop like2 watch
		
		layout = column(slider, plot)
		curdoc().add_root(layout)
		
- updating plots from dropdown menus

		from bokeh.io import curdoc
		from bokeh.layouts import column
		from bokeh.models import ColumnDataSource, Select
		from bokeh.plotting import figure
		from numpy.random import random, normal, lognormal
		
		N = 1000
		source = ColumnDataSource(data = {
		    'x': random(N),
		    'y': random(N)})
		
		plot = figure()
		plot.circle(x='x', y='y', source=source)
		
		menu = Select(
		    options=['uniform', 'normal', 'lognormal'],
		    value='uniform', # initial value
		    title='Distribution')
		
		def callback(attr, old, new): # update source.data
				if menu.value == 'uniform': f = random
				elif menu.value == 'normal': f = normal
				else: f = lognormal
				source.data = {'x': f(size=N), 'y': f(size=N)} 

		menu.on_change('value', callback) # 1st param: prop i like2 watch
		layout = column(menu, plot)
		curdoc().add_root(layout)
		
- button callbacks

		from bokeh.models import Button
		button = Button(label='press me')
		def update():
			# do something interesting
		botton.on_click(update)
		
		#several types of buttons
		from bokeh.models import CheckboxGroup, RadioGroup, Toggle
		toggle = Toggle(label='Some on/off', button_type='success')
		checkbox = CheckboxGroup(labels=['foo', 'bar', 'baz'])
		radio = RadioGroup(labels=['2000', '2010', '2020'])
		
		def callback(active)
			# active tells wich button is active

- hosting applications
	- https://anaconda.org
	- https://bokeh.pydata.org/en/latest/

________________
<a name="python_statistics"/>
### statistics

- exploratory data analysis - eda
- process of organizing/plotting/summarizing a data set - START WITH GRAPHICAL EDA!!! - exploratory data analysis > john tukey - godfather - get the whole story!

#### graphical eda

- generating histograms - interesting: to which side is the weight? outliers - interpretation <> how are bins chosen? -> arbitrary, binning bias	

		import matplotlib.pyplot as plt
		_ = plt.hist(df['colName']) # returns three arrays -> dummy var _
		_ = plt.xlabel('XLabel') # NEVER FORGET TO LABEL THE AXES!
		_ = plt.ylabel('YLabel')
		plt.show()
		
		# rule of thumbs for the bins
		n_bins = n_data = n_bins = int(np.sqrt(len(dataSet)))
		bin_edges = [0,10,20,30,40,50,60,70,80,90,100]
		_ = plt.hist(df['colName'], bins=n_bins) # evenly spaced bins
		_ = plt.hist(df['colName'], bins=bin_edges) # pass list to 'bins'
		
		# with seaborn styling
		import seaborn as sns
		sns.set() # set seaborn styling
		_ = plt.hist(df['colName']) # returns three arrays -> dummy var _
		_ = plt.xlabel('XLabel') # always label your axes!
		_ = plt.ylabel('YLabel')
		plt.show()
					
- binning bias -> better: plot ALL of your data -> swarm plot, bee swarm plot - compare distributions of specific grouped data - limit of swarm plots: gets confusing when too many points/BIG data sets comared side by side -> ecdf

		# grouped by x-var 'state'
		_ = sns.swarmplot(x='state', y='dem_share', data=df_swing)
		_ = plt.xlabel('state')
		_ = plt.ylabel('percent of vote for Obama')
		plt.show()
		
- ecdf - empirical cumulative distribution function - how many percent of data points have certain value or less/more - x-axis is sorted data - good overview over distribution

		import numpy as np
		x = np.sort(df_swing['dem_share']) # array sort wrt vals of a col
		y = np.arange(1, len(x)+1) / len(x) # evenly spaced data points

		_ = plt.plot(x, y, marker='.', linestyle='none')
		_ = plt.xlabel('percent of vote for Obama')
		_ = plt.ylabel('ECDF')
		plt.margins(0.02) # keeps data off plot edges
		plt.show()
		
		# general ecdf function
		def ecdf(data):
	    """Compute ECDF for a one-dimensional array of measurements."""
	    n = len(data)
	    x = np.sort(data)
	    y = np.arange(1,n+1) / n # evenly distributed points
	    return x, y
	    
		# apply on flowers data
		x_set, y_set = ecdf(setosa_petal_length)
		x_vers, y_vers = ecdf(versicolor_petal_length)
		x_virg, y_virg = ecdf(virginica_petal_length)
		
		_ = plt.plot(x_set, y_set, marker='.', linestyle='none')
		_ = plt.plot(x_vers, y_vers, marker='.', linestyle='none')
		_ = plt.plot(x_virg, y_virg, marker='.', linestyle='none')
		
		plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
		_ = plt.xlabel('petal length (cm)')
		_ = plt.ylabel('ECDF')
		
		plt.show()
		
#### quantitative eda
		
- summary statistics -> sample mean/average - median
- mean/average: problem: outliers big weight
- median: immune to extreme data - sorted by ranking, not by value
- percentiles: generalization of median - visualized in edf

		import numpy as np
		np.mean(array)
		np.median(array)
		np.percentile(array, percentNumber) # percent, not fractions!
		np.percentile(array, percentList)
		
		_ = plt.plot(x_vers, y_vers, '.') # Plot the ECDF
		_ = plt.xlabel('petal length (cm)')
		_ = plt.ylabel('ECDF')
		_ = plt.plot( # Overlay percentiles as red diamonds.
					ptiles_vers, percentiles/100,
					marker='D', color='red', linestyle='none')
		plt.show()
		
- boxplots -> visualizing percentiles [25,50,75] - box is iqr: inter quantile range |p(75)-p(25)| - antenna: 2x iqr from median - alternative to swarm plots
- outliers: here determined by iqr - outlierst extreme, not errorious

		import matplotlib.pyplot as plt
		import seaborn as sns
		_ = sns.boxplot(x='east_west', y='dem_share', data=df_all_states)
		_ = plt.xlabel('region')
		_ = plt.ylabel('percent of vote for Obama')
		plt.show()
		
		_ = sns.boxplot(x='species', y='petal length (cm)', data=df)
		_ = plt.xlabel('species')
		_ = plt.ylabel('petal length (cm)')
		plt.show()
		
- variance/standard deviation - measure of spread

		np.var(array) # mean of the squared distances from mean
		np.mean((np.mean(array) - array)**2) # alternatively

		np.std(array) # square root of variance
		np.sqrt(np.var(array))

- correlation: scatter plot, compare two properties, want number <> covariance - normed: pearson correlation coefficient
		
		# scatter plot
		_ = plt.plot(
					total_votes/1000, dem_share,
					marker='.', linestyle='none')
		_ = plt.xlabel('total votes (thousands)')
		_ = plt.ylabel('percent of vote for Obama')
		
		# covariance: mean of product of differences to means for x and y
		# x,y pos correlated, if covariance is pos high
		# x,y neg correlated, if covariance is neg high
		# x,y not correlated, if covariance is near zero
		np.cov(array1, array2) # returns 2x2 matrix with var on diag
		
		# pearson correlation coefficient: divide cov by std deviations
		# normed to 1 with values in [-1, 1]
		np.corrcoef(array1, array2) # returns 2x2 matrix with 1 on diag
		
		# pearson correlation coefficient
		def pearson_r(array1, array2):
	    """Pearson correlation coefficient between two arrays."""
	    corr_mat = np.corrcoef(array1, array2)
	    return corr_mat[0,1]

#### thinking probabilistically - discrete variables

- statistical inference - sample > population
	- measuring different samples - each sample <> mean(sample) -> different means etc -> distribution of mean over samples
	- use concepts of probability "your data speak in language of probability"
- probability
	- random number generators - np.random module
	- seed gives random numbers deterministically -> pseudo-randomness

			np.random.seed() # specifies seed for reproducibility
			
- simulate repeated measurements using python <> hacker statistics with computers is easy - hacker stats vs penAndPaper stats > getting hard very fast
- hacker stats: (1) det how to simulate data - (2) sim data many many times - (3) compute fraction of trials with outcome of interest
- you have the power of a computer: if you simulate the story, you can get the distribution

		# simulate coin flips
		import numpy as np
		
		# (1) one 4-times-flip, interesting outcome: get all heads
		np.random.seed(42)
		rn = np.random.random(size=4) # draw rnd num eq distr in [0,1]
		heads = rn < 0.5
		n_heads = np.sum(heads)
		# (2) many 4-times-flips, count trials of interest
		n_all_heads = 0 # test for event "all-heads"
		n_trials = 10000
		for _ in range(n_trials)
			heads = np.random.random(size=4) < 0.5
			n_heads = np.sum(heads)
			if n_heads == 4:
				n_all_heads += 1
		# (3) fraction of trials with outcome of interest
		n_all_heads / n_trials
		
		# (1) bernoulli trials, get certain number of heads for n trials
		def perform_bernoulli_trials(n, p):
	    """n bernoulli trials, success prob p, return num of success"""
	    n_success = 0
	    for i in range(n):
		    random_number = np.random.random()
		    if random_number < p:
			    n_success += 1
			return n_success
		# (2)	sim data many times
		np.random.seed(42)
		n_defaults = np.empty(1000)
		for i in range(1000):
			n_defaults[i] = perform_bernoulli_trials(100, 0.05)
		# (3) compute fraction of trials with specific outcomes
		_ = plt.hist(n_defaults, normed=True)
		_ = plt.xlabel('number of defaults out of 100 loans')
		_ = plt.ylabel('probability')
		plt.show()

- probability distributions and stories
- pmf - probability mass function - set of probabilities of discrete outcomes
- example: discrete uniform pmf <> tossing dice
- example: the binomial distribution <> outcomes of n bernoulli trials with success prob p are binomial distributed, e.g. coin flips

		np.random.binomial(4, 0.5) # num of heads while tossing 4 times
		np.random.binomial(4, 0.5, size=10) # sample 10 nums from b-dist
		samples = np.random.binomial(60, 0.5, size=10000) is bin-distr
		
		# plot cdf
		import matplotlib.pyplot as plt
		import seaborn as sns
		sns.set()
		x,y = ecdf(samples)
		_ = plt.plot(x, y, marker='.', linestyle='none')
		plt.margins(0.02)
		_ = plt.xlabel('number of successes')
		_ = plt.ylabel('cdf')
		plt.show()
		
		# plot pmf with histogram
		bins = np.arange(0, max(samples) + 2) - 0.5
		_ = plt.hist(samples, normed=True, bins=bins)
		_ = plt.xlabel('outcomes')
		_ = plt.ylabel('pmf')
		plt.show()

- poisson processes - poisson distribution: rare events independent of each other
- example: bus stop, rare event: bus is arriving
- examples: natural births in given hospital, hits on a website (6 hits per hour, lambda=6), meteor strikes, molecular collision in a gas, aviation incidents, buses in poissonville
- poisson distribution: one parameter - lambda
- number r of arrivals of a poisson process in a given amount of time with average rate of lambda arrivals per interval is poisson distributed
- poission dist is limit of binomial distribution for n->infty, p->0 hence rare events

		samples = np.random.poisson(6, size=10000)
		x,y = ecdf(samples)
		_ = plt.plot(x, y, marker='.', linestyle='none')
		plt.margin(0.02)
		_ = plt.xlabel('number of successes')
		_ = plt.ylabel('cdf')
		plt.show()
		
		# compare limit of binomial distr <> poisson distr
		samples_poisson = np.random.poisson(10, size=10000)
		print('Poisson:     ', np.mean(samples_poisson),
		                       np.std(samples_poisson))
		# Specify values of n and p to consider for Binomial: n, p
		n = [20, 100, 1000, 10000]
		p = [0.5, 0.1, 0.01, 0.001]
		for i in range(4):
		    samples_binomial = np.random.binomial(n[i], p[i], size=10000)
		    print('n =', n[i], 'Binom:', np.mean(samples_binomial),
		                                 np.std(samples_binomial))
		                                 
		# poisson process of a "7 or more no-hitter season"
		# Draw 10,000 samples out of Poisson distribution: 
		n_nohitters = np.random.poisson(251/115, size=10000)
		n_large = np.sum(n_nohitters >= 7)
		p_large = n_large / 10000
		print('Probability of seven or more no-hitters:', p_large)

#### thinking probabilistically - continous variables

- property can be measured on continous scale, not dice but size of a human
- pdf probability density function: continuous analog of pmf - mathematical description of relative likelihood of observing a value of a continuous variable - single values don't make sense - concept: measureable sets like intervals - probability: area under curve
- cdf cumulative density function: integral over pdf - probability of measuring equal or less a specific value
- normal distribution - THE continuous distribution - pdf: bell shape - symmetric with one peak - completely det by mean and std
- important: don't confuse mean/std of distribution with empirical mean/std

		# assume real distr is normal distr estimated by sample distr
		import numpy as np
		mean = np.mean(michaelson_speed_or_light)
		std = np.std(michaelson_speed_or_light)
		samples = np.random.normal(mean, std, size=10000) # estimation
		x, y = ecdf(michaelson_speed_or_light)
		x_theor, y_theor = ecdf(samples)	
		# compare cdf <> ecdf plots
		import matplotlib.pyplot as plt
		import seaborn as sns
		sns.set()
		_ = plt.plot(x_theor, y_theor)
		_ = plt.plot(x, y, marker='.', linestyle='none')
		_ = plt.xlabel('speed of light (km/s')
		_ = plt.ylabel('cdf')
		plt.show()
		
		# compare normal distributions w several std: pdf, cdf
		samples_std1 = np.random.normal(20, 1, size=100000)
		samples_std3 = np.random.normal(20, 3, size=100000)
		samples_std10 = np.random.normal(20, 10, size=100000)
		_ = plt.hist(samples_std1, bins=100, normed=True, histtype='step')
		_ = plt.hist(samples_std3, bins=100, normed=True, histtype='step')
		_ = plt.hist(samples_std10, bins=100, normed=True, histtype='step')
		_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))
		plt.ylim(-0.01, 0.42)
		plt.show()
		x_std1, y_std1 = ecdf(samples_std1)
		x_std3, y_std3 = ecdf(samples_std3)
		x_std10, y_std10 = ecdf(samples_std10)
		_ = plt.plot(x_std1, y_std1, marker='.', linestyle='none')
		_ = plt.plot(x_std3, y_std3, marker='.', linestyle='none')
		_ = plt.plot(x_std10, y_std10, marker='.', linestyle='none')
		_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc='lower right')
		plt.show()
		
- normal distribution - properties/warnings - for many symmetric peaked data - for many statistical procedures there are normality assumptions about the data - ubiquit in nature
- warning - length/mass of large mouth bass - length yes, mass not - in normal distribution: outliers extremely unlikely <> not realistic?

- exponential distribution
	- story: waiting time between arrivals of a poisson process is exponentially distributed
	- falling strong monotonically
	- single parameter: mean waiting time
	- examples: nuclear incidents are poisson processes - time between processes is exp distr

			# exponetiall inter-incident times
			mean = np.mean(inter_times)
			samples = no.random.exponential(mean, size=10000)
			x, y = ecdf(inter_times)
			x_theor, y_theor = ecdf(samples)
			_ = plt.plot(x_theor, y_theor)
			_ = plt.plot(x, y, marker='.', linestyle='none')
			_ = plt.xlabel('time (days)')
			_ = plt.ylabel('cdf')
			plt.show()
			
	- example
			
#### parameter estimation by optimization

- michaeleson_speed_of_light: how do we know, that mean/std of data are appropriate estimates for the normal parameters?
- if we believe that the process that generates our data gets normally distributed results, the set of parameters that brings the model - here the normal distribution - in closest aggreement with the data, uses the mean and the std computed directly from the data - these are the optimal parameters
- optimal parameters: parameter values that bring model in closest aggreement with data - only optimal for the model we chose for your data - when model is wrong, optimal parameters are not meaningful
- finding optima parameters: not always easy -> see later linear reg
- optimization: optimizing parameters <> python packages for statistical inference: "scipy.stats", "statsmodels" - here: hacker statistics with numpy -> swiss army knife
- hackers stat: compare ecdf and cdf! good way to campare model and data

		np.random.seed(42)		
		tau = np.mean(nohitter_times) # mean no-hitter time
		inter_nohitter_time = np.random.exponential(tau, 100000)
		_ = plt.hist(inter_nohitter_time,
		             bins=50, normed=True, histtype='step')
		_ = plt.xlabel('Games between no-hitters')
		_ = plt.ylabel('PDF')
		plt.show()
		
		x, y = ecdf(nohitter_times) # real data
		x_theor, y_theor = ecdf(inter_nohitter_time) # model
		plt.plot(x_theor, y_theor)
		plt.plot(x, y, marker='.', linestyle='none')
		plt.margins(0.02)
		plt.xlabel('Games between no-hitters')
		plt.ylabel('CDF')

		# take samples with half/double tau - to illustrate optimal tau
		samples_half = np.random.exponential(tau/2, size=10000)
		samples_double = np.random.exponential(tau*2, size=10000)
		x_half, y_half = ecdf(samples_half)
		x_double, y_double = ecdf(samples_double)
		
		_ = plt.plot(x_half, y_half)
		_ = plt.plot(x_double, y_double)
		plt.show()

- linear regression by least squares - compare two columns - parameter of line: slope, intercept - optimization: distances from line to points are minimal - least squares - use np.polyfit with 1st degree polynomials

		slope, intercept = np.polyfit(total_votes, dem_share, 1)

- example literacy/fertility data

		# eda of literacy/fertility data
		_ = plt.plot(fertility, illiteracy, marker='.', linestyle='none')	plt.margins(0.02)
		_ = plt.xlabel('percent illiterate')
		_ = plt.ylabel('fertility')
		plt.show()
		print(pearson_r(illiteracy, fertility)) # pearson corr coeff
		
		# linear regression using np.polyfit(): a, b
		a, b = np.polyfit(illiteracy, fertility, 1)		
		print('slope =', a, 'children per woman / percent illiterate')
		print('intercept =', b, 'children per woman')
		x = np.array([0,100]) # two points are enough
		y = a * x + b
		_ = plt.plot(x, y)
		plt.show()
		
		# analyse rss: residual sum of squares vs several slopes
		# Specify slopes to consider: a_vals
		a_vals = np.linspace(0, 0.1, 200)
		# Initialize sum of square of residuals: rss
		rss = np.empty_like(a_vals) # 
		# Compute sum of square of residuals for each value of a_vals
		for i, a in enumerate(a_vals):
		    rss[i] = np.sum((fertility - a*illiteracy - b)**2)
		plt.plot(a_vals, rss, '-')
		plt.xlabel('slope (children per woman / percent illiterate)')
		plt.ylabel('sum of square of residuals')
		plt.show()

- anscombe's quartet - all with same parameters wrt linear regression - equal averages wrt x resp y - very similar rss
- look before you leap: regressions w/o outliers? other (nonlin) model? -> EXPLORE DATA FIRST - exploratory data analysis should be the first step in an analysis of data (after getting your data imported and cleaned, of course)

		# anscombe data set
		# Perform linear regression: a, b on first quartet
		a, b = np.polyfit(x, y, 1)
		print(a, b)
		
		x_theor = np.array([3, 15]) # first and last x val
		y_theor = x_theor * a + b
		
		# Plot the Anscombe data and theoretical line
		_ = plt.plot(x, y, marker='.', linestyle='none')
		_ = plt.plot(x_theor, y_theor)
		plt.xlabel('x')
		plt.ylabel('y')
		plt.show()
		
		# iterate through all anscombe data set and calc each parameters
		for x, y in zip(anscombe_x, anscombe_y):
		    a, b = np.polyfit(x,y,1)
		    print('slope:', a, 'intercept:', b)

#### bootstrap confidence

- think probabilistically - another measurement (data set) wouldn't result in same parameter (mean, median, std, ...) - not interested in single measurement - interested in general statements
- hacker stat: simulate new measurement from one measurement
- resampling: sampling from data set with replacement
- distribution of parameters (mean, median, std, ...)
- bootstrap sample: each resampled array
- bootstrap replicate: a statistic computed from b sample - single value

		np.random.choice([1,2,3,4,5], size=5) # resampling 5 times
		np.random.choice(array, size=len(array)) # bootstrapping
		
- examples

		# speed of light
		bs_sample = np.random.choice(michelson_speed_of_light,size=100)
		np.mean(bs_sample)
		np.median(bs_sample)
		np.std(bs_sample)
		
		# rainfall (Sheffield Weather Station UK 1883 - 2015)
		for _ in range(50): # plot bootstrap data
		    bs_sample = np.random.choice(rainfall, size=len(rainfall))
		    x, y = ecdf(bs_sample)
		    _ = plt.plot(x, y, marker='.', linestyle='none',
		                 color='gray', alpha=0.1)
		x, y = ecdf(rainfall) # original data
		_ = plt.plot(x, y, marker='.')
		plt.margins(0.02)
		_ = plt.xlabel('yearly rainfall (mm)')
		_ = plt.ylabel('ECDF')
		plt.show()
		
- bootstrap replicate function of 1d data

		def bootstrap_replicate_1d(data, func):
			"""generate bootstrap replicate of 1d data."""
			bs_sample = np.random.choice(data, len(data))
			return func(bs_sample)
			
		# apply on speed of light			
		bs_replicates = np.empty(10000)
		for i in rante(10000)
			bs_replicates[i] = bootstrap_replicate_1d(
				michelson_speed_or_light,
				np.mean)
		
		_ = plt.hist(bs_replicates, bins=30, normed=True)
		_ = plt.xlabel('mean speed of light (km/s)')
		_ = plt.ylabel('pdf')
		plt.show()

- estimate of the mean: p% confidence interval (e.g. p=95)
- conf interval: inner range of repl distribution

		# 95% conf interval for mean speed of light
		conf_int = np.percentile(bs_replicates, [2.5, 97.5])
		
- array of bootstrap replicates - encapsulated in function 

		def draw_bs_reps(data, func, size=1):
	    """Draw bootstrap replicates."""
	    bs_replicates = np.empty(size)
	    for i in range(size):
	        bs_replicates[i] = bootstrap_replicate_1d(data, func)
	    return bs_replicates
	    
		# bootstrap replicates of the mean | the sem (std error of mean)
		bs_replicates = draw_bs_reps(rainfall, np.mean, size=10000)
		sem = np.std(rainfall) / np.sqrt(len(rainfall)) 
		print(sem) # sem = approx std/sqrt(n)
		bs_std = np.std(bs_replicates)
		print(bs_std)
		_ = plt.hist(bs_replicates, bins=50, normed=True)
		_ = plt.xlabel('mean annual rainfall (mm)')
		_ = plt.ylabel('PDF')
		plt.show() # value of mean will be approx normally distributed
		
		# confidence interval
		print(np.percentile(bs_replicates, [2.5, 97.5]))
		
		# other statistics: variance
		bs_replicates
		bs_replicates = draw_bs_reps(rainfall, np.var, 10000)
		bs_replicates /= 100
		_ = plt.hist(bs_replicates, normed=True, bins=50)
		_ = plt.xlabel('variance of annual rainfall (sq. cm)')
		_ = plt.ylabel('PDF')
		plt.show()
		
- parametric estimates - e.g. lin reg - statistics have other val for other replicates - thinking probabilistically distribution
- nonparametric inference

- pairs or bootstaps - 2d data set, 2 columns - for linear regression: bs replicates of parameters slope and intercept

		# bootstrap total_votes
		inds = np.arange(len(total_votes)) # indices of arrays
		bs_inds = np.random.choice(inds, len(inds)) # bootstrap indices
		bs_total_votes = total_votes[bs_inds] # bs 1. array
		bs_dem_share = dem_share[bs_inds] # bs 2. array
		bs_slope, bs_intercept = np.polyfit( # get lin reg parameter
			bs_total_votes,
			bs_dem_share,
			1)
			
		# a function to do pairs bootstrap
		def draw_bs_pairs_linreg(x, y, size=1):
	    """Perform pairs bootstrap for linear regression."""
	    inds = np.arange(len(x))
	    bs_slope_reps = np.empty(size)
	    bs_intercept_reps = np.empty(size)
	    for i in range(size):
	        bs_inds = np.random.choice(inds, size=len(inds))
	        bs_x, bs_y = x[bs_inds], y[bs_inds]
	        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
	    return bs_slope_reps, bs_intercept_reps
	    
		# pairs bootstrap of literacy/fertility data
		bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000)
		print(np.percentile(bs_slope_reps, [2.5, 97.5]))
		_ = plt.hist(bs_slope_reps, bins=50, normed=True)
		_ = plt.xlabel('slope')
		_ = plt.ylabel('PDF')
		plt.show()
		
		# plotting bootstrap regressions
		x = np.array([0,100])
		for i in range(0,100):
			_ = plt.plot(x, 
			             bs_slope_reps[i]*x + bs_intercept_reps[i],
			             linewidth=0.5, alpha=0.2, color='red')
		_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')
		_ = plt.xlabel('illiteracy')
		_ = plt.ylabel('fertility')
		plt.margins(0.02)
		plt.show()
		
#### hypothesis testing

- how reasonable is it, to observe data described by the model? - 
- example: how reasonable is it, to measure the county_level_voting of a state (ohio) in it's neighbour state (pansylvania)? test this hypothesis with voting data
- observe how reasonable observed data are assuming the hypothesis is true - hypothesis we are testing: null hypothesis
- might plot two ecdf of two count level votes - compare some summary statistics - not enough, even when almost identical
- permutation: putting two data sets together, randomly reordering/permute entries, dividing into two new sets - test hypothesis with assumption, that two quantities/data sets are equally distributed

		# function generating permutation sample
		def permutation_sample(data1, data2):
	    """Generate a permutation sample from two data sets."""
	    data = np.concatenate((data1, data2))
	    permuted_data = np.random.permutation(data)
	    perm_sample_1 = permuted_data[:len(data1)]
	    perm_sample_2 = permuted_data[len(data1):]
	    return perm_sample_1, perm_sample_2
	    
			
		# generate permutation samples, create and plot ecdf's
		for i in range(50):
			perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november)
			x_1, y_1 = ecdf(perm_sample_1)
			x_2, y_2 = ecdf(perm_sample_2)
			_ = plt.plot(x_1, y_1, marker='.', linestyle='none',
			             color='red', alpha=0.02)
			_ = plt.plot(x_2, y_2, marker='.', linestyle='none',
			             color='blue', alpha=0.02)
		# create and plot ecdf's from original data
		x_1, y_1 = ecdf(rain_june)
		x_2, y_2 = ecdf(rain_november)
		_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
		_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')
		plt.margins(0.02)
		_ = plt.xlabel('monthly rainfall (mm)')
		_ = plt.ylabel('ECDF')
		plt.show()
		
- test statistics and p-value
- example: null hypothesis: county level voting is identical distributed between two states
- remember def: hypothesis testing < assessment of how reasonable the obeserved data are assuming a hypothesis is true
- def is a bit vague: 
	- what about the data do you assess?
	- how do we quantify the assessment?
	- answer > concept: test statistic
- test statistic:
	- single number, computed from observed data, from data you simulate under the null hypothesis
	- serves as basis of comparison beetween the two: what does hypothesis predict, what actually was observed
	- chose test statistic to be sth trying to answer with hypothesis test
- example: identical states => same mean vote share for obama, difference should be zero - test statistic: difference of means

		# comparing test statistic of original and permutation data
		np.mean(perm_sample_PA) - np.mean(perm_sample_OH) # perm replicat
		np.mean(dem_share_PA) - np.mean(dem_share_OH) # actual statistic
		
- compare original statistic with distribution of permutation replicates (compare original value (measured value) and histogram) - histogram represents null hypothesis: states are identical, real statistic lies in confidence interval
- p value: probability, that - under assumption null hypothesis is true <> given histogram - original (or a more extreme) statistic is observed.
- example: under assumption of identical states: p value of original statistic is 23%
- no statement about null hypothesis being true!!! just about observed test statistic
- statistical significance: data are different, if p-value is small
- null hypothesis significance testing (nhst) - what we are doing in this chapter
- warning:
	- stat significance is just a label: yes/no
	- p value is a quantity!
	- practical significance: whether or not the difference of the data matters for practical considerations

			# function: generating permutation replicates
			def draw_perm_reps(data_1, data_2, func, size=1):
				"""Generate multiple permutation replicates."""	
				perm_replicates = np.empty(size)	
				for i in range(size):
				    perm_sample_1, perm_sample_2 = \
				    permutation_sample(data_1, data_2)
				    perm_replicates[i] = func(perm_sample_1, perm_sample_2)	
				return perm_replicates	
				
			# example: south american horned frogs - force of tongue strike
			# Kleinteich and Gorb (Sci. Rep., 4, 5225, 2014)
			_ = sns.swarmplot('ID', 'impact_force', data=df)
			_ = plt.xlabel('frog')
			_ = plt.ylabel('impact force (N)')
			plt.show() # look before you leap: eda before hypothesis testng
			# permutation test on frog data
			def diff_of_means(data_1, data_2):
			    """Difference in means of two arrays."""
			    diff = np.mean(data_1) - np.mean(data_2)
			    return diff
			empirical_diff_means
			empirical_diff_means = diff_of_means(force_a, force_b)
			perm_replicates = \
					draw_perm_reps(force_a, force_b, diff_of_means, size=10000)
			p = np.sum(perm_replicates >= empirical_diff_means) /len(perm_replicates)
			print('p-value =', p)			
			
- pipeline of hypothesis testing
	- clearly sate hypothesis
	- define your test statistic
	- assuming the null hypothesis is true: generate many sets of simulated data
	- compute  test statistic for each simulated data set > distribution
	- p value: fraction of simulated data sets for which the test statistic is at least as extreme as for the real data

- example: speed or light - michelson vs newcomb - are two outcomes (mean speed of light) different - don't have newcombs data, only michelson
- null hypothesis: the true speed of light in michelson's experiment was actually newcomb's reported value - compare data set with value! - ONE SAMPLE TEST - shift michelson's data and assume both data sets are equal - former way: TWO SAMPLE TEST
- example of ONE SAMPLE TEST

		newcomb_value = 299860 # km/s
		michelson_shifted = michelson_speed_of_light \ # shifted data set
				- np.mean(michelson_speed_of_light) + newcomb_value
		def diff_from_newcomb(data, newcomb_value=299860):
				return np.mean(data) - newcomb_value # test statistic
		diff_obs = diff_from_newcomb(michelson_speed_of_light)
		bs_replicates = draw_bs_reps(
				michelson_shifted, diff_from_newcomb, 10000) # get the p val
		p_value = np.sum(bs_replicates <= diff_observed) / 10000
		
- example: hypothesis mean strike force of Frog B and Frog C equal

		# a one-sample bootstrap hypothesis test
		translated_force_b = force_b - np.mean(force_b) + 0.55
		bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)
		p = np.sum(bs_replicates <= np.mean(force_b)) / 10000 # p value
		print('p = ', p)
		# a two-sample bootstrap hypothesis test for difference of means	
		mean_force = np.mean(forces_concat)
		force_a_shifted = force_a - np.mean(force_a) + mean_force
		force_b_shifted = force_b - np.mean(force_b) + mean_force
		bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, size=10000)
		bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, size=10000)
		bs_replicates = bs_replicates_a - bs_replicates_b
		p = np.sum(bs_replicates >= empirical_diff_means) / len(bs_replicates)
		print('p-value =', p)
		
- A/B testing: A old strategy, B new strategy - null hypothesis: test statistic in impervious to the change - low p-value: strategy change lead to significant change in performance
- example: A/B: click-through rate (yes/no) with new/old design - hypothesis: click-through rate not affected by redesign - good: permutation test - simulate result as if redesign hat no effect on click-through rate

		import numpy as np
		# clickthrough_A, clickthrough_B: array of 1s ans 0s - yes/no

		# test statistic function - difference of click-through rates
		def diff_frac(data_A, data_b): 
				frac_A = np.sum(data_A) / len(data_A)
				frac_B = np.sum(data_B) / len(data_B)
				return frac_A - frac_B
			
		diff_frac_obs = diff_frac( 
				clickthrough_A, clickthrough_B) # test statistic observation
		perm_replicates = np.empty(10000) # test statistic permutations
		for i in range(10000)
				perm_replicates[i] = permutation_replicate(
						clickthrough_A, clickthrough_B, diff_frac)
				
		p_value = np.sum(perm_replicates >= diff_frac_ops) / 10000
		# smapp -> redesign is real improvement
				
				
- example A/B: the vote for the civil rights act in 1964

		# Construct arrays of data: dems, reps
		dems = np.array([True] * 153 + [False] * 91)
		reps = np.array([True] * 136 + [False] * 35)
		def frac_yea_dems(dems, reps):
		    """Compute fraction of Democrat yea votes."""
		    frac = np.sum(dems) / len(dems)
		    return frac
		perm_replicates = draw_perm_reps(
				dems, reps, frac_yea_dems, 10000)
		p = np.sum(perm_replicates <= 153/244) / len(perm_replicates)
		print('p-value =', p)

- example A/B: measure number of people who click on an ad on your company's website before/after changing its color.

- example: check out whether the rule changes in 1920 changed the rate of no-hitters
		
		nht_diff_obs = diff_of_means(nht_dead, nht_live) # test stat obs
		perm_replicates = draw_perm_reps( # test stat permutations
				nht_dead, nht_live, diff_of_means, 10000)
		p = np.sum(perm_replicates <= nht_diff_obs) / 10000 # p-value
		print('p-val =', p)
		
- correlation - hypothesis test of correlation - state null hypothesis: two variables are completely uncorrelated - simulate data assuming hypothesis is true - test statistic: rho = pearson corr coeff  - p value: fraction of replicates that have rho at least at large as observed

- example: fertility vs illiteracy

		r_obs = pearson_r(illiteracy, fertility)
		perm_replicates = np.empty(10000)
		for i in range(10000):
			illiteracy_permuted = np.random.permutation(illiteracy)
			perm_replicates[i] = pearson_r(illiteracy_permuted, fertility)
		p = np.sum(perm_replicates >= r_obs) / len(perm_replicates)
		print('p-val =', p)

- example: do neonicotinoid insecticides have unintended consequences

		x_control, y_control = ecdf(control)
		x_treated, y_treated = ecdf(treated)
		plt.plot(x_control, y_control, marker='.', linestyle='none')
		plt.plot(x_treated, y_treated, marker='.', linestyle='none')
		plt.margins(0.02)
		plt.legend(('control', 'treated'), loc='lower right')
		plt.xlabel('millions of alive sperm per mL')
		plt.ylabel('ECDF')
		plt.show()
		
		diff_means = diff_of_means(control, treated) # test statistic
		mean_count = np.mean(np.concatenate((control, treated)))
		control_shifted = control - np.mean(control) + mean_count
		treated_shifted = treated - np.mean(treated) + mean_count
		bs_reps_control = draw_bs_reps(control_shifted,
		                       np.mean, size=10000)
		bs_reps_treated = draw_bs_reps(treated_shifted,
		                       np.mean, size=10000)
		bs_replicates = bs_reps_control - bs_reps_treated
		p = np.sum(
				bs_replicates >= np.mean(control) - np.mean(treated)) \
				/ len(bs_replicates)
		print('p-value =', p) # very small => stat signific difference!

#### darwin finches - beak length/depth of species/generations

- toolbox
	- graphical/quanititative eda
	- parameter estimation
	- confidence interval calculation
	- hypothesis testing

			# eda
			_ = sns.swarmplot('year', 'beak_depth', data=df)
			_ = plt.xlabel('year')
			_ = plt.ylabel('beak depth (mm)')
			plt.show()
			x_1975, y_1975 = ecdf(bd_1975)
			x_2012, y_2012 = ecdf(bd_2012)
			_ = plt.plot(x_1975, y_1975, marker='.', linestyle='none')
			_ = plt.plot(x_2012, y_2012, marker='.', linestyle='none')
			plt.margins(0.02)
			_ = plt.xlabel('beak depth (mm)')
			_ = plt.ylabel('ECDF')
			_ = plt.legend(('1975', '2012'), loc='lower right')
			plt.show()
			
			# parameter estimates of beak depths
			mean_diff = np.mean(bd_2012) - np.mean(bd_1975)
			bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, 10000)
			bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, 10000)
			bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975
			conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5])
			print('difference of means =', mean_diff, 'mm')
			print('95% confidence interval =', conf_int, 'mm')
			
			# hypothesis test: Are beaks deeper in 2012?
			combined_mean = np.mean(np.concatenate((bd_1975, bd_2012)))
			bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean
			bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean
			bs_replicates_1975 = draw_bs_reps(
					bd_1975_shifted, np.mean, 10000)
			bs_replicates_2012 = draw_bs_reps(
					bd_2012_shifted, np.mean, 10000)
			bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975
			p = np.sum(bs_diff_replicates >= mean_diff) / len(bs_diff_replicates)
			print('p =', p) # 0.0034
			
			# eda of beak length and depth
			_ = plt.plot(bl_1975, bd_1975, marker='.',
			             linestyle='none', color='blue', alpha=.5)
			_ = plt.plot(bl_2012, bd_2012, marker='.',
			             linestyle='none', color='red', alpha=.5)
			_ = plt.xlabel('beak length (mm)')
			_ = plt.ylabel('beak depth (mm)')
			_ = plt.legend(('1975', '2012'), loc='upper left')
			plt.show()
			
			# linear regressions
			slope_1975, intercept_1975 = np.polyfit(bl_1975, bd_1975, 1)
			slope_2012, intercept_2012 = np.polyfit(bl_2012, bd_2012, 1)
			bs_slope_reps_1975, bs_intercept_reps_1975 = \
			        draw_bs_pairs_linreg(bl_1975, bd_1975, 1000)
			bs_slope_reps_2012, bs_intercept_reps_2012 = \
			        draw_bs_pairs_linreg(bl_2012, bd_2012, 1000)
			slope_conf_int_1975 = np.percentile(
							bs_slope_reps_1975, [2.5, 97.5])
			slope_conf_int_2012 = np.percentile(
							bs_slope_reps_2012, [2.5, 97.5])
			intercept_conf_int_1975 = np.percentile(
							bs_intercept_reps_1975, [2.5, 97.5])
			intercept_conf_int_2012 = np.percentile(
							bs_intercept_reps_2012, [2.5, 97.5])
			print('1975: slope =', slope_1975,
			      'conf int =', slope_conf_int_1975)
			print('1975: intercept =', intercept_1975,
			      'conf int =', intercept_conf_int_1975)
			print('2012: slope =', slope_2012,
			      'conf int =', slope_conf_int_2012)
			print('2012: intercept =', intercept_2012,
			      'conf int =', intercept_conf_int_2012)
			
			# displaying the linear regression results
			_ = plt.plot(bl_1975, bd_1975, marker='.',
			             linestyle='none', color='blue', alpha=0.5)
			_ = plt.plot(bl_2012, bd_2012, marker='.',
			             linestyle='none', color='red', alpha=0.5)
			_ = plt.xlabel('beak length (mm)')
			_ = plt.ylabel('beak depth (mm)')
			_ = plt.legend(('1975', '2012'), loc='upper left')
			x = np.array([10, 17])
			for i in range(100): # plot only 100 lines
			    plt.plot(x, bs_slope_reps_1975[i] * x \
			    + bs_intercept_reps_1975[i],
			    linewidth=0.5, alpha=0.2, color='blue')
			    plt.plot(x, bs_slope_reps_2012[i] * x \
			    + bs_intercept_reps_2012[i],
			    linewidth=0.5, alpha=0.2, color='red')
			plt.show()
			
			# beak length to depth ratio
			ratio_1975 = bl_1975 / bd_1975
			ratio_2012 = bl_2012 / bd_2012
			mean_ratio_1975 = np.mean(ratio_1975)
			mean_ratio_2012 = np.mean(ratio_2012)
			bs_replicates_1975 = draw_bs_reps(
				ratio_1975, np.mean, 10000)
			bs_replicates_2012 = draw_bs_reps(
				ratio_2012, np.mean, 10000)
			conf_int_1975 = np.percentile(
				bs_replicates_1975, [0.5, 99.5])
			conf_int_2012 = np.percentile(
				bs_replicates_2012, [0.5, 99.5])
			print('1975: mean ratio =', mean_ratio_1975,
				  'conf int =', conf_int_1975)
			print('2012: mean ratio =', mean_ratio_2012,
				  'conf int =', conf_int_2012)
						
- The mean beak length-to-depth ratio decreased by about 0.1, or 7%, from 1975 to 2012. The 99% confidence intervals are not even close to overlapping, so this is a real change. The beak shape changed.

			# eda of heritability
			_ = plt.plot(bd_parent_fortis, bd_offspring_fortis,
			             marker='.', linestyle='none', color='blue', alpha=.5)
			_ = plt.plot(bd_parent_scandens, bd_offspring_scandens,
			             marker='.', linestyle='none', color='red', alpha=.5)
			_ = plt.xlabel('parental beak depth (mm)')
			_ = plt.ylabel('offspring beak depth (mm)')
			_ = plt.legend(('G. fortis', 'G. scandens'), loc='lower right')
			plt.show()

			# correlation of offspring and parental data
			def draw_bs_pairs(x, y, func, size=1):
			    """Perform pairs bootstrap for a single statistic."""
			    inds = np.arange(len(x))
			    bs_replicates = np.empty(size)
			    for i in range(size): # bootstrapping
			        bs_inds = np.random.choice(inds, len(inds))
			        bs_x, bs_y = x[bs_inds], y[bs_inds]
			        bs_replicates[i] = func(bs_x, bs_y)
			    return bs_replicates
			    
			# pearson correlation of offspring and parental data
			r_scandens = pearson_r(
					    bd_parent_scandens, bd_offspring_scandens)
			r_fortis = pearson_r(
					    bd_parent_fortis, bd_offspring_fortis)
			bs_replicates_scandens = draw_bs_pairs(bd_parent_scandens, bd_offspring_scandens, pearson_r, 1000)
			bs_replicates_fortis = draw_bs_pairs(bd_parent_fortis, bd_offspring_fortis, pearson_r, 1000)
			conf_int_scandens = np.percentile(
							bs_replicates_scandens, [2.5, 97.5])
			conf_int_fortis = np.percentile(
							bs_replicates_fortis, [2.5, 97.5])
			print('G. scandens:', r_scandens, conf_int_scandens)
			print('G. fortis:', r_fortis, conf_int_fortis)	
			
			# measuring heritability
			def heritability(parents, offspring):
							"""Compute the heritability from parent and offspring samples."""
							covariance_matrix = np.cov(parents, offspring)
							return covariance_matrix[0,1] / covariance_matrix[0,0]
			heritability_scandens = heritability(
							bd_parent_scandens, bd_offspring_scandens)
			heritability_fortis = heritability(
			        bd_parent_fortis, bd_offspring_fortis)
			replicates_scandens = draw_bs_pairs(
			        bd_parent_scandens, bd_offspring_scandens,
			        heritability, size=1000)			        
			replicates_fortis = draw_bs_pairs(
			        bd_parent_fortis, bd_offspring_fortis,
			        heritability, size=1000)
			conf_int_scandens = np.percentile(
			        replicates_scandens, [2.5, 97.5])
			conf_int_fortis = np.percentile(
			        replicates_fortis, [2.5, 97.5])
			print('G. scandens:', heritability_scandens, conf_int_scandens)
			print('G. fortis:', heritability_fortis, conf_int_fortis)
			
			# is beak depth heritable at all in G. scandens?
			perm_replicates = np.empty(10000)
			for i in range(10000):
				bd_parent_permuted = np.random.permutation(
				    bd_parent_scandens)
				perm_replicates[i] = heritability(
				    bd_parent_permuted, bd_offspring_scandens)
			p = np.sum(perm_replicates >= heritability_scandens) / len(perm_replicates)
			print('p-val =', p)


		
________________
<a name="python_data_bases"/>
### data bases

- tool here: SQLAlchemy - two parts:
	- core (releational model focused)
	- orm object relational model (user data model focused)

- connecting to a database by engine
- engine: common interface to the db from sqlalchemy

		from  sqlalchemy import create_engine
		engine = create_engine('sqlite:///census_nyc.sqlite')
		engine = create_engine('sqlite:///census.sqlite')
		# 'sqlite': db driver+dialect - 'census_...': local filename
		connection = engine.connect()
		
- mysql database

		from  sqlalchemy import create_engine
		engine = create_engine(
			'mysql+pymysql://'+ # driver + dialect
			'student:datacamp'+ # user + pass - @host:port/
			'@courses.csrrinzqubik.us-east-1.rds.amazonaws.com:3306/'+
			'census') # connection string
				
		
- amazon web service - aws - with postgreSQL db

		from sqlalchemy import create_engine			
		engine = create_engine(
		'postgresql+psycopg2://' # dialect and driver
		+'student:datacamp' # user/pass
		+'@postgresql.csrrinzqubik.us-east-1.rds.amazonaws.com'#host/port
		+':5432/census') # string for connection to db
		
- what's in your database > e.g. what tables?

		print(engine.table_names())
		
- reflection: loads table from db, builds sqlalchemy table objects
- MetaData obj: meta data container, catalogue storing db informations, e.g. access dict metadata.tables via key 'census'

		from sqlalchemy import MetaData, Table
		metadata = MetaData()
		
		census = Table('census', metadata, autoload=True, autoload_with=engine) # reflection
		
		print(repr(census))	# print details of census
		print(census.columns.keys()) # column names
		print(repr(metadata.tables['census'])) # details of census
		
- sql query language - proxy: vertreter/bevollmächtigter - fetch from proxy all or only what you desire

		connection = engine.connect()
		
		stmt = 'SELECT * FROM census' # sql query
		stmt = select([census]) # same as above - select requires list of tables/columns
		
		result_proxy = connection.execute(stmt) # get resProxy for query
		results = result_proxy.fetchall() # get resultSet from Proxy
		
		results = connection.execute(stmt).fetchall() # short cut
		
		first_row = results[0]
		print(first_row.keys())
		print(results[0].keys()) # print labels of columns
		print(first_row.colName) # first_row[0], first_row['colName']
		print(results[:10]) # first 10 rows
				
- where clauses - restrict by boolean condition, compare values in different columns, use comparisan operators

		stmt = select([census])
		stmt = stmt.where([census.columns.state == 'California'])

		results = connection.execute(stmt).fetchall()
		for result in results: # loop over rows
			print(result.state, result.age)
		
		stmt = select([census]) # pass a list!
		stmt = stmt.where(census.columns.state.startswith('New'))
		
		# resultProxy can be used in loop - need no fetchall()
		for result in connection.execute(stmt):
			print(result.state, result.pop2000)
		
- more complex conditions: in_(), like(), between(), ...
- conjunctions: and(), not(), or_(), ...

		from sqlalchemy import or_, in_, and_, ...
		
		stmt = select([census])
		stmt = stmt.where(
			and_(census.columns.state == 'New York',
				or_(census.columns.age == 21,
          census.columns.age == 37)))
		stmt = stmt.where(census.columns.state.in_(states))

- order_by()

		from sqlalchemy import desc
		
		stmt = select([census])
		stmt = stmt.order_by(census.columns.state) # ascending
		stmt = stmt.order_by(desc(census.columns.state)) # descending
		
		stmt = stmt.order_by(listOfColNames)# order by multiple

- sql functions: aggregate data - more efficient than processing in python - count, sum, ...
- dist(), limit()
- group by - multiple grouping columns possible

		from sqlalchemy import func
		stmt = select([func.sum(census.columns.pop2008)])
		results = connection.execute(stmt).scalar()
		# getting just the value of a query that returns only one row/col
		
		# group by
		stmt = select([
			census.columns.sex, # group by this column
			func.sum(census.columns.pop2008) # calc func for each group
			func.sum(census.columns.pop2008.distinct())]) # count dist vals
		stmt.group_by(census.columns.sex)
		
		# multiple groups
		stmt = select([
			census.columns.sex,
			census.columns.age,
			func.sum(census.columns.pop2008)])
		stmt.group_by(census.columns.sex, census.columns.age)
		
		# set labels for function columns - default: 'func_1' etc
				stmt = select([
			census.columns.sex,
			func.sum(census.columns.pop2008).label('pop2008_sum')])
		
		stmt = stmt.limit(5) # top five results
		
- use pandas, matplotlib

		import pandas as pd
		import matplotlib.pyplot as plt

		df = pd.DataFrame(results)
		df.columns = results[0].keys()
		
		df[10:20].plot.barh() # horizontal bar plot
		plt.show()
		
- math operations on columns - +-*/% - different each data type

		stmst = select([
			census.columns.age
			(census.columns.pop2008 -
			census.columns.pop2000).label('pop_change')])
		stmt = stmt.group_by(census.columns.age)
		stmt = stmt.order_by(desc('pop_change'))
		stmt = stmt.limit(5)
		
- case statement - treat data differently based on condition

		from sqlalchemy import case
		stmt = select ([
			func.sum(
				case([
					census.columns.state == 'New York', # condition
					census.columns.pop2008 # output
				], else_=0))]) # output in all other cases
		
		# useful for converting data types: cast()
		from sqlalchemy import case, cast, Float
		stmt = select ([
			(
				func.sum(
					case([
						(census.columns.state == 'New York') # condition
						census.columns.pop2008) # output
					], else_=0) # output in all other cases
				) /
				cast(func.sum(census.columns.pop2008), Float) * 100
			).label('ny_percent')
		])
		
- relationships between data:
	- avoid dublicate data
	- make easy to change things in one place
	- useful: break out information we don't need often
	- automatic joins - relation: comparing indexes

			stmt = select([tabel1.columns.name1, tabel2.columns.name2])
			
	- join: need expression if prefefined setting not wanted - location: select() <expression> where()/order_by()/groupby()
	- select_from(): replaces default derived FROM clause with a join - wraps join() clause

			stmt = select([func.sum(table1.columns.colName1)]) # select
			stmt = stmt.select_from(table1.join(table2)) # from join
			stmt = stmt.where(table2.columns.colName2 == '10') # where
			
	- join tables w/o predef relationship - only joins data matching between two columns

			stmt = select([func.sum(table1.columns.colName1)]) # select
			stmt = stmt.select_from( # from ... join
				table1.join(
					table2, # joining table
					table1.columns.colName1 == table2.columns.colName2))#j-cond
			stmt = stmt.where(table2.columns.colName3 == 'bla') # where
			
	- hierarchical tables, self-relational tables > tables joining themselves - e.g. table with employer-id and manager-id...
	- alias() - one table viewing via two/multiple unique names

			managers = employees.alias()
			stmt = select([
				managers.columns.name.label('manager'),
				employees.columns.name.label('employee')])
			stmt = stmt.select_from(
				employees.join(
					managers,
					managers.columns.id == employees.columns.manager))
			stmt = stmt.order_by(managers.columns.name)
			
			# other example
			stmt = select(
				[managers.columns.name.label('manager'),
				employees.columns.name.label('employee')])
			stmt = stmt.where(managers.columns.id == employees.columns.mgr)
			stmt = stmt.order_by(managers.columns.name)
			
	- group_by, func - important:
		- to target group_by() at right alias
		- with what you perform functions on

				managers = employees.alias()
				stmt = select([
					managers.columns.name,
					employees.columns.sal])
				stmt = stmt.select_from(
					employees.join(
						managers,
						managers.columns.id == employees.columns.manager))
				stmt = stmt.order_by(managers.columns.name)
				
- dealing with large methods -> fetchmany() lets us specify how many rowa we want to act upon - loop over fetchmany() - returns empty list when no more records > call close() method on ResultProxy

		while more_results:
			partial_results = results_proxy.fetchmany(50)
			if partial_results == []
				more_results = False
			for row in partial_results:
        if row.state in state_count:
            state_count[row.state] += 1
        else:
            state_count[row.state] = 1		results_proxy.close()
		results_proxy.close()
		print(state_count)
		
- creating databases - here not: PostgreSQL, MySQL - here: SQLite with create_engine() creates db if not already existed

		from sqlalchemy import (Table, Column, String, Integer, Decimal, Boolean)
		
		# still use table object like for reflection - 				
		# replace autoload keyword with column objects
		employees = Table('employees', metadata,
			Columns('id', Integer()),
			Columns('name', String(255)),
			Columns('salary', Decimal()),
			Columns('active', Boolean()))

		# create tables in actual db: create_all() on metadata instance
		metadata.create_all(engine) # pass engine

		engine.table_names() # verify, that table was created
	
- handle db tables updates > use tools: Alembic, raw SQL - out of scope of this course
- additional column options - constraints:
	- uniqueness: forces data in column to be unique
	- nullable: determines if col can be empty in a row
	- default: sets default value, if one isn't supplied
	- others...

			employees = Table('employees', metadata,
				Columns('id', Integer()),
				Columns('name', String(255), unique=True),
				Columns('salary', Float(), default = 100.00),
				Columns('active', Boolean(), default = True))
				
			employees.contraints # check constraints attribute
			
- inserting data into a table - after: creating engine, establiching connection, reflecting the table

		from sqlalchemy import insert
		stmt = insert(employees).values( # insert doesn't return any rows
			id=1,
			name='Jason',
			salary=1.00,
			active=True,)
		result_proxy = connection.execute(stmt) 
		print(result_proxy.rowcount)
		
- inserting multiple rows -> insert()-stmt

		# built insert statement w/o any values
		stmt = insert(employees)
		
		# built dict representing all values clauses for rows to insert
		values_list = [
			{'id':2, 'name':'Rebecca', 'salary':2.00, 'active':True}
			{'id':3, 'name':'Bob', 'salary':0.00, 'active':False}]
			
		# alternatively
		for idx, row in enumerate(csv_reader):
	    data = {'state': row[0], 'sex': row[1], 'age': row[2], 'pop2000': row[3], 'pop2008': row[4]}
	    values_list.append(data)
			
		# pass both: stmt, values list to execute method on connection
		result_proxy = connection.execute(stmt, values_list)
		print(result_proxy.rowcount)
		
- update data in database -> update()-stmt

		from sqlalchemy import update
		
		# like insert- with where-clause: to determin record 2b updated
		stmt = update(employees)
		stmt = stmt.where(employees.columns.id == 3)
		
		# values clause only contains values we want to change
		stmt = stmt.values(active=True) # only column name
		
		result_proxy = connection.execute(stmt)
		print(result_proxy.rowcount) # check count of updated rows
		
-  update multiple rows

		stmt = update(employees)
		stmt = stmt.where(
			employees.columns.active == True)
		stmt = stmt.values(
			active=True,
			salary=0.00)		
		result_proxy = connection.execute(stmt)
		print(result_proxy.rowcount)
		
- correlated updates -> update data from select statement: defining a select statement that returns the value you want to update the record with and assigning that as the value in an update statement.

		new_salary = select([employees.columns.salary])
		new_salary = new_salary.order_by(desc(employees.columns.salary))
		new_salary = new_salary.limit(1)
		
		stmt = update(employees)
		stmt = stmt.values(salary=new_salary)
		result_proxy = connection.execute(stmt)
		print(result_proxy.rowcount)
		
- deleting data from db -> delete()-stmt - BE VERY CAREFUL!!

		from sqlalchemy import delete
		stmt = select([
			func.count(extra_emplyees.columns.id)])
		connection.execute(stmt).scalar()
		
		delete_stmt = delete(extra_employees)
		result_proxy = connection.execute(delete_stmt)
		result_proxy.rowcount
		
		# delete specific rows
		stmt = delete(employees).where(employees.columns.id == 3)
		result_proxy = connection.execute(stmt)
		result_proxy.rowcount
		
		# dropping a table completely
		extra_employees.drop(engine)
		print(extra_employees.exists(engine))
		
		# dropping a table completely
		metadata.drop_all(engine)
		engine.table_names()


### introduction to relational databases

- metadata tables (postGre, mySQL, ...)
	- information_schema.tables # get metadata
	- information_schema.columns # get metadata
	- columns: table_schema - eg with value 'public' > user def info
- tables: core of every database
	- reduce reduncancies with several tables
________________
<a name="scikit"/>
### scikit-learn	
			
see notebook file
			
			


		
		
		
		

 

		

		
		

________________
<a name="python_django"/>
### django

________
<a name="python_django_notes"/>
#### notes
- Udemy: "Python and Django Full Stack Web Developer Bootcamp"

________
<a name="python_django_backend"/>
#### Backend
High Level Overview:
- User request a URL
- urls.py > views.py <> models.py <> Database (SQLite)
- views.py > Templates HTML/CSS/JS > user request

________
<a name="python_django_venv"/>
#### virtual environment: venv
- virtual installation of python and packages with certain versions - activate environment and install certain packages with pip/conda

					conda create --name myDjangoEnv python=3.5 #creates an environment
					conda info --envs #info about all environments
					source activate myDjangoEnv
					source deactivate myDjangoEnv
					conda install django #installs django into your current environment
				
- test new features without breaking web application
- allready included in anaconda

________
<a name="python_django_project"/>
#### django project
- later discuss difference: project <> application
- now aim: run project on local server displayd in browser
- with django installation comes tool: django-Admin | good feature to run things from command line
- now run first project:
					
					source activate myDjangoEnv
					django-admin startproject first_project
					
- what are the files in the project "first_project":
	- __init__.py	| is blank | lets py know: this dir can be treated as a package
	- settings.py | all projects settings are stored in
	- urls.py | stores all url patterns | basically the different pages of the web applicatoin
	- wsgi.py | acts as the web server gate interface | later: deploy on server
	- manage.py | gonna use a lot | includes many commands as we build up our webapp
- use manage.py

					python manage.py runserver
					
	- get the local url with the port
	- warning about migrations > to do about data bases | for now ignore it
	- migration: move db from one design to another


________
<a name="python_django_apps"/>
#### django apps
- what is a django project?
	- collection of applications and configurations
	- combined together makes up full web app (website running with django)
- what is a django app
	- created to perform a particular functionality for your entire web application
	- eg: registration app | polling app | comments app
	- reusability > pluggable django applications
	- create simple application:

					python manage.py startapp first_app

- what are the failes in the app "first_app"
	- __init__.py	| is blank | lets py know: this dir can be treated as a package
	- admin.py | admin interface
	- apps.py | apps configurations
	- models.py | store applications data models
	- tests.py | store test functions
	- views.py | handle requests and return responses
	- migrations folder | store db specific info as related to the models
- link first_app to first_project
	- in first_project/settings.py INSTALLED_APPS list

					INSTALLED_APPS = [
					...
					'first_app'
					]
					
	- run server again:

					python manage.py runserver
					
	- now create first view "hello world"

					from django.http import HttpResponse #import the response fct

					# Create your views here as individual view functions
					# each view function takes at least one argument: a request object
					# each view function mus return an HttpResponse object
					def index(request):
					    return HttpResponse("<h1>Hello World!</h1>") #contains HTML
					    
	- map view to url.py file and read the examples there

					# import views.py to url.py
					from first_app import views
					(...)
					urlpatterns = [
					    path('', views.index, name='index'),
					    path('radmin/', admin.site.urls),
					]
					
	- alternatively use include() function from django.conf.url
		- allows to look for a match with regular expressions
		- link back to our applicaion's own urls.py file
		- will have to manually add in this urls.py file -> each app has its own urls.py file
		- want to keep project's urls.py file cean and modular by setting reference to the application instead of listinge them all in the main urls.py file
		- edit first_project/urls.py

					from django.conf.urls import include
					(...)
					urlpatterns = [
					    path('', views.index, name='index'),
					    path('choseANameForTheExtension/', include('first_app.urls')),
					    path('radmin/', admin.site.urls),
					]
					
		- create new file: first_app/urls.py

					from django.conf.urls import url
					from first_app import views
					urlpatterns = [
						url('', views.index, name='index'),
					]

________
<a name="python_django_templates"/>
#### django templates
idea:
- understand how django works and interacts with your website
- later: connect templates with models > diplay data from db dynamically
- templates contain static part of html page
- besides html use template tags | django template variables | have own syntax | allows to injext dynamic content that django views produce > effects final html
- create template folders for each specific app: first_project/templates/first_app
- connect templates to django project in settings.py DIR key in TEMPLATES dict > need absolute path here > BUT want project to be transfered to other computers  with different os and directories > use pythons os-module in settings.py and BASE_DIR

				BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
				TEMPLATES_DIR = os.path.join(BASE_DIR, "templates")
				(...)
				TEMPLATES = [
			    {
			        (...)
			        'DIRS': [TEMPLATE_DIR,],
			        (...)
			    },
- new template file: /templates/first_app/index.html

			<!DOCTYPE html>
			<html>
			  <head>
				<meta charset="utf-8">
				<title>First App</title>
			  </head>
			  <body>
				<h1>Hello, this is index.html!</h1>
				{{ insert_me }}
			  </body>
			</html>

- power of Django: inject db content into html by using python code (models) | therefore: use render() function | place it into index() function inside views.py file

		def index(request):
		    my_dict = {'insert_me':"Hello, I am from views.py!"}
		    return render(request, 'first_app/index.html', context=my_dict)

________
<a name="python_django_static_files"/>
#### static files: media | css | js | ...
- create folder first_project/static/images, css, javasript ec and add a file like image.jpg or mystyle.css
- add STATIC_DIR variable and STATICFILES_DIRS list in settings.py

		STATIC_DIR = os.path.join(BASE_DIR, "static")
		(...)
		STATIC_URL = '/static/'
		STATICFILES_DIRS = [
		    STATIC_DIR,
		]
		
- insert template tags in index.html

			<!DOCTYPE html>
			{% load staticfiles %}
			<html>
			  <head>
			    <meta charset="utf-8">
			    <title>Django Guitar Page</title>
			    <link rel="stylesheet" href="{% static "css/mystyle.css" %}">
			  </head>
			  <body>
			    <h1>Hello, this is a picture of Django himself!</h1>
			    <img src="{% static "images/djangoguitar.jpg" %}" alt="Uh Oh, didn't show up!">
			  </body>
			</html>

#### deploy django
- use git (version control) and github (managing git and hosting files)
	- look wether you have git installed:
	
							git --version
							
	- create a new folder for the repository
	- create a new repository:
	
							git init
							git add .
							git commit . -m "first commit"
							git remote add origin https://github.com/tomobones/django-deployment-example.git
							git push -u origin master
	
	- or push an existing repository from the command line
	
							git remote add origin https://github.com/tomobones/django-deployment-example.git
							git push -u origin master

- http://www.pythonanywhere.com - here conda is NOT available > other commands
	- look up local django version:

							python
							import django
							django.__version__
							
	- console on www.pythonanywhere.com:

							mkvirtualenv --python=python3.6 myproj # make virtual environment
							pip list # lists all installed packages
							pip install -U django==2.1
						 
	 - weiter bei lektion 156 minute 8:00











________________________________
<a name="r_statistics"/>
# R statistics

<a name="r_notes"/>
## general notes
- https://cran.r-project.org
- google search: "r reference card"
- sources:
	- udemy: r programming a-z, r for data science, kirill eremenko
	- udemy: advanced analytics in r for data science, kirill eremenko
	- udemy: Data Science and Machine Learning Bootcamp with R, Jose Portilla
- concept of lygometry: shadow measure | measure what you don't know | youtub > ted talk

## specific r notes
- comments starting with "#"
- no char in end of collumn ";"
	- "sprintf("$s is $f feet tall\n", "Sven", 7.1)" # formats $i $f $s $e $g $a
- help
	- bring up help for specific functions: ?print()
	- help("function")
	- help.search("topic")
	- apropos("topic")
	- methods(a)
	- ??vector # searching for vector

<a name="r_fundamentals"/>
## fundamentals
- assign variables: "var <- 56" | "var <- "test"" | "var <- 45L"
	- eg "var <- NA" empty, but allocating memory
	- ig r assigns numbers as a double
	- if special type needed: use letter after number: eg "L" for long (int)
	- 5 atomic types: int (long), double, complex, char, logical (T,F) or (TRUE, FALSE)
	- logical op: <,>,==,!=,<=,>=,!,|,&,isTRUE() and combinations by brackets
	- check type: "typeof(var)" | "class()"
	- also: is.int(45) >>> TRUE | is.numeric(45) >>> TRUE
	- b <- as.dataType(a) # concerting a data type a into another b
	- print a variable var just by "var"
	- print on console: "print()"
	- remove variable "rm(var)"
	- strings also work with simple quotation marks: "string <- 'tomo'"
- flow control
	- for-loop:
		- index in "for (i in vec) {print(i)}" here "i" is NO index BUT element of vec, eg "1:5" (R-specific loop)
		- if you want a real index, use "for (j in seq(1,10)) {print vec[j]}" or an external index/counter (conventional loop)
	- while-loop: "while(counter...){}"
	- if-condition: if(condition1){}else if(condition2){}else{}
- create functions
	- functions "myFct <- function(param1, param2=defaultParam){code}"
	- ananomous functions: function(a,b){a+b} # eg to put directly in lapply(vec,fct)
- packages
	- stored libraries | functions and data
	- graphical interface: use "packages"-folder in r-studio > detach/entach/delete/...
	- install package: "install.packages("package_name")"
	- activate package: "library(packagename)" or check it with a hook in the list
	- packages - examples
		- knitr -> markDown -> pdf
		- openML
		- likert -> graphs
		- ggplot2
		- ggbeeswarm -> graphs
		- ggridges -> graphs ridgeplot
		- hexbin -> graphs
- functions:
	- "abs()" absolute value

<a name="r_vectors"/>
## vectors
- vectors: arrays - index starting with 1, not 0
	- "R" is a vector driven language - a lot of things happen in vectorized form > faster algorithms
	- everything is vector - even variables!
	- assign vectors by combine fct: "vec <- c(5, 4, 5, 2, 6, 7, 3)"
	- entries have to be all of SAME TYPES
	- check variable for special type: is.integer(var) | is.numeric(var) | is.character(var)
- special functions:
	- sequence: "seq(1,15)" | also "1:15" | adding steps: "seq(1,15,2)"
	- replicate:
		- "rep(3, 50)"
		- "rep("a", 50)"
		- "rep(vec, 50)"
		- "rep(vec, times=50)" | the same as above
		- "rep(vec, each=50)"
	- put vector as entry of new vector puts entries in new vector
	- "paste(vec1,vec2)" pastes two vectors | "paste(vec1,vec2,sep=" ")" is the default separation
	- "paste0("Hello","World") without separation
	- "sort(c(1,7,4,2,8,5), decreasing=TRUE)"
	- "rev(c(1,2,3,4,5,6,7,8,9,0))" # reverses elements of vector
	- append(vec1, vec2) # appends a vector to another vector
- access of elements of vectors >>> by brackets []
	- i-th element of vec: "vec[i]"
	- all except i-th element: "newVec <- vec[-i]"
	- sequence of elements: "newVec <- vec[3:5]
	- combination of elements: "newVec <- vec[c(1,5,7)]"
	- excluding elements by combination: "newVec <- vec[c(-1,-5,-7)]" | "newVec <- vec[-3:-5]
	- subsets of vectors: eg "vec[c(1,5,6)]"
- vector operations:
	- use operations for "component-wise operations": +,-,*,/,<,boolean,logical... -> vectorized approach vs de-vectorized approach in "for (i in 1:N) {...}"
	- RECYCLING of vectors: vectors of different lenght > smaller vector is recycled/repeated
	- "rnorm(5)" vector with 5 random numbers out of normal distributed numbers
	- sample(1:100,3) #3 random numbers between 1 and 100
	- use functions with vectors, eg "sqrt(vec)"
	- other functions:
		- round()
		- mean()
		- sd()
		- max()
		- min()
		- sum()
		- prod()
		- length()
		- round(2,33434565, digits=3)
		- round(2,33434565, 3) | same as above
	- lapply(vec, function(a){a*2) # returns list with values of function applied to each element of the vector
	- sapply(vec, function(a){a*2}) # returns vector with ...
	- sapply(vec, fct, b=100) # functions with  many parameters eg function(a,b){a+b}
	- others *apply >>> help(sapply)


<a name="r_regexp"/>
## regular expression
- idea: pattern search 
	- grepl("pattern", "text with a pattern")
	- grepl("pattern", vec) # searches each entry for pattern -> boolean vector
	- grepl("pattern", vec) # searches each entry for pattern -> vector of indices that match

<a name="r_timesstamps"/>
## dates - timestamps
 - "syst.date()" current date - is own class "Date"
 - someDate = as.Date("19-05-1979")
 - someOtherDate = as.Date("Nov-03-90", format="%b-%d-%y")
 - POSIXct("11:02:03", format="%H:%M:%S") # portablae operating system interface
 - "help(strptime)" strip time function >>> documentation | basic date time conversion function
 - strptime("11:02:03", format="%H:%M:%S")

<a name="r_matrices"/>
## matrices
- in general
	- indices: row/column A_ij
	- select the i_th row: A[i,]
	- select the j_th column: A[,j]
	- only same type
- initiate matrix 
	- matrix()
		- eg 4x5 a matrix counting 1 to 20: "matrix(1:20, 4, 5)" | this is filling the columns first
		- eg 4x5 a matrix counting 1 to 20: "matrix(1:20, 4, 5, byrow=T)" | this is filling the rows first
	- by (row)vectors: rbind(rowVec1, rowVec2, rowVec3, rowVec4) | like "c(1,2,3,4)" | afterwards: clean up vectors by rm()?
	- by (column)vectors: cbind(columnVec1, columnVec2, columnVec3, columnVec4)
	- mat <- matrix(c(vec1,vec2,vec3,...,vecn), nrow=n)
- Naming dimensions: replace indices by names/chars/...
	- every vector has name property: "names(vec) = c("Eins", "Zwei", "Drei")"
	- access: "vec("DREI")" | "mat("A", "DREI")"
	- reset: "names(vec) = NULL"
	- matrices: "rownames(mat) = rowVector" and "colnames(mat) = colVector"
	- by initiating:
		- rbind(name1=rowVec1, name2=rowVec2, name3=rowVec3, name4=rowVec4)
		- cbind(name1=colVec1, name2=colVec2, name3=colVec3, name4=colVec4)
- Matrix operations: +,-,*,/,<,boolean,logical...
	- Matrix multiplication: "mat1 %*% mat2"
	- colSums() rowSums()
	- rowMeans()
	- rbind(mat,rowVec) cbind(mat,colVec) #add vectors to matrices
- MatPlot --> "?matplot()"
	- "matplot(t(ToreDerStarsJedesJahr), type="b", pch=15:18, col=c(1:4,6))"
	- plots columns of a matrix
	- maybe transpose matrix with "t()"
	- set legend next to matplot command by "legend()"
	- "legend("bottomleft", inset=0.01, legend=Players, col=c(1:4,6), pch=15:18)"
- subset of rows and columns of a matrix:
	- eg "mat[1:3, c(3,6)]"
- subsetting with/without dropping dimensions:
	- mat[2,] is a vector | just one dimension > one row
	- mat[2,,drop=F] is a matrix
	- IMPORTANT for MatPlot: only allows matrices
- encapsulate plot into function:
				myplot <- function(data, rows=1:10) {
					Data <- data[rows,,drop=F]
					matplot(t(Data), type="b", pch=15:18, col=c(1:4,6))
					legend("bottomleft", inset=0.01, 
						legend=Players[rows], col=c(1:4,6), pch=15:18)
				}
				
				myplot(Salary, 1:2)

<a name="r_dataframes"/>
## data frames
- entities (rows) have different properties (sorted in columns)
- not all data is numeric -> data frames
- import data: read.csv() reads data into data frame
	- select file manually
		- data <- read.csv(file.choose()) popup appears to choose data file
	- set working directory and read data
		- getwd() displays the permanent working directory
		- setwd(/Users/thomasvogg/Desktop...) sets the working directory (MacOS)
		- data <- read.csv("dataFile.csv") popup appears to choose data file
	- write.csv(dataFrame, file="someFile.csv")
	- excel files
		- install.packages('readxl',repos="http://cran.rstudio.com/") need package readxl
		- library(readxl) # Load the readxl package
		- excel_sheets('Sample-Sales-Data.xlsx') #list the sheets of the excel file
		- read_excel("sampleData.xlsx", sheet="Sheet1") # loading an excel file
		- write on excel files:
			- install.packages('xlsx',repos="http://cran.rstudio.com/")
			- library(xlsx)
			- write.xlsx(df, "output.xlsx")
	- sql files
		- install.packages("RODBC") 
		- library(RODBC)
		- myconn <- odbcConnect("Database_Name", uid="User_ID", pwd="password")
		- dat <- sqlFetch(myconn, "Table_Name")
		- querydat <- sqlQuery(myconn, "SELECT * FROM table")
		- close(myconn)
	- web-scraping...
- Exploring the data frame
	- nrow(data) | ncol(data) how many rows/cols
	- head(data) | tail(data) top/last 6 (default) rows
	- str(data) structure > whats going up in this data frame | "factors" means "categories"
	- runif(data) random variables distributed uniformly <> rnorm()
	- summary(data) | more detailed as str | apply on separate columns
	- level(data[,"columnName"]) outputs the different levels/factors of a column > good example to apply "$" notation
- accessing data of a data frame
	- like in matrices:
		- "data[i,j]"
		- "data[i,"columnName"]"
		- in data frames you don't have names for rows!
		- "data[1:5,]" subsetting
	- with "$" operator: Access one column by name and get vector:
		- "vec = data$columnName" | same as "data[,"columnName"]"
		- "entry = data$columnName[i]"
		- eg level(data$columnName)
- basic operations with the data frame
	- "data[i,]" subsetting | is still data frame | no need for dimension drop "drop=F"
	- "data[,j]" is a vector | need here "data[,j,drop=F]"
	- operations (+-/!&...) on dataFrameColumns data$ClumnName1 * data$ClumnName2 outputs vector
	- add new column: data$newColumnName <- vec
	- remove column: data$columnNameToRemove <- NULL
- Filtering data frames
	- filter <- data$someValue < 10 gets a logical vector
	- data[filter,] outputs only rows that satisfy condition 
	- data[data$someValue < 10,] denser code
	- use logic operations in filters & == != ! ...
	- also: newDataFrame = subset(dataFrame, subset= someColumnName <= 10)
	- ordering: orderedDataFrame = order(dataFrame["someColumnName"])
	- ordering: orderedDataFrame = order(-dataFrame["someColumnName"]) descently
- qplot()
	- install.packages("ggplot2")
	- library(ggplot2)
	- qplot(data=dataFrameName, x=columnName) plots the distribution of a column of a data frame
	- don't need $-notation eg "x=dataFrameName$columnName"
	- qplot(data=dataFrameName, x=colName1, y=colName2) plots prop1 against prop2 > data in scatter plot
	- qplot(data=dataFrameName, x=colName1, y=colName2, size=I(10), color=I("blue"), geom="boxplot")
	- qplot(data=dataFrameName, x=colName1, y=colName2, size=I(10), color=colName3) #3ed prop is distinguished by color
	- other properties:
		- shape=I(17) # eg triangelss
		- alpha=I(0.3) # transparancy
		- main="titel"
- creating data frames
	- myDataFrame <- data.frame(vec1, vec2, vec3, ...) don't forget to rename 
	- columns "colnames(myDataFrame) = c("name1", "name2", "name3", ...)"
	- "colnames(myDataFrame)[3] = "name3""
	- simpler: myDataFrame <- data.frame(name1=vec1, name2=vec2, name3=vec3, ...)
	- works also for rbind() and cbind()
- merging data frames
	- mergedDataFrame <- merge(df1, df2, by.x="commonColumnInDF1", by.y="commonColumnInDF2") # commonColumnInDF2 will be dropped
	- important: columns are really identic!!!
	- may be drop other redundant columns: mergedDataFrame$otherRedundantColumns <- NULL
- dealing with missing data:
	- any(is.na(dataFrame)) # TRUE if there is en empty entry
	- any(is.na(df$col.name.1)) # anywhere in col
	- df <- df[!is.na(df$col), ] delete selected missing data rows
	- df$col[is.na(df$col)] <- 999 # For a selected column

<a name="r_lists"/>
## lists
- allow us to store a variety of data structures under a single variable. This means we could store a vecor,matrix, data frame, etc. under a single list.
- initiate:
	- eg "li <- list(vec,mat,df)"
	- li <- list(sample_vec = v,sample_mat = m, sample_df = df)
- access data
	- li['sample_vec'] # bracket notation to access item in a list
	- li$sample_vec # Can also use $ notation
	- li[['sample_vec']] # Use double brackets to actually grab the items
	- li[['sample_vec']][1] # Second set of indexing
- Combining lists:
	- double_list <- c(li,li)

<a name="r_ggplot2"/>
## visualization with GGPlot2
- layers in visualization - ggplot works by adding specific layers:
	- data | aesthetics | geometry | statistics | facets | coordination | themes
	- data
		- factors
			- number/numerical <> factor: categorical variable
			- transfer numbers into factors: "movies$year <- factor(movies$year)"
			- factor: ordinal/nominal
			- transfer into ordinal factors: "factor.tmp <- factor(liquid$tmp, ordered=TRUE, levels=c("cold", "warm", "hot"))"
	- aesthetics: how your data maps to what you see
		- eg: ggplot(data=movies, aes(x=CriticalRating, y=AudienceRating, color=Genre, size=BudgetsMillions)) + geom_point()
	- geometry - plotting with geom_layers
		- p <- ggplot(data=movies, aes(...)) # is the starting layer, is an object! contains all data
		- p + geom_point() # adds a layer
		- p + geom_line() # adds another layer
		- p + geom_line() + geom_point() # two layers
	- aesthetics overriding
		- q = ggplot(data=movies, aes(x=CriticalRating, y=AudienceRating, color=Genre, size=BudgetsMillions))
		- q + geom_points()
		- q + geom_points(aes(size=CriticalRating)) # overriding aesthetics - not the original manipulating
		- overriding in further layer vs setup in starting layer
			- depends on what you want to be flexibile with and what are your constants
	- aesthetics: mapping vs setting
		- q + geom_points(aes(color=Genre)) # mapping
		- q + geom_points(color="DarkGreen") # setting
	- geometry - histograms | density charts
		- s <- ggplot(data=movies, aes(x=BudgetMillions))
		- s + geom_histogram(binwidth=10, aes(fill=Genre), color="Black")
		- s + geom_density(aes(fill=Genre), position="stack")
	- statistics - statistical transformations
		- u <- ggplot(data=movies, aes(x=CriticRating, y=AudienceRating, color=Genre))
		- u + geom_points() + geom_smooth(fill=NA)
		- u <- ggplot(data=movies, aes(x=Genre, y=AudienceRating, color=Genre))
		- u + geom_boxplot(size=1.2)
		- u + geom_boxplot(size=1.2) + geom_point()
		- u + geom_boxplot(size=1.2) + geom_jitter()
		- u + geom_jitter() + geom_boxplot(size=1.2, alpha=.5)
	- facets
		- v <- ggplot(data=movies, aes(x=BudgetMillions))
		- v + geom_histogram(binwidth=10, aes(fill=Genre), color="Black") + facet_grid(Genre~., scales="free") # left of ~ is rows, right of ~ is column
		- w <- u <- ggplot(data=movies, aes(x=CriticRating, y=AudienceRating, color=Genre))
		- w + geom_point()
	- coordinates: limits, zooming in/out - by coord_layers:
		- xlim(), ylim() # set borders
		- coord_cartesian(ylim=c(0,50)) # zooms in
	- themes
		- xlab("labelX"), ylab("labelY") # adding labels to axes 
		- ggtitle("TitelName")
		- theme(axis.title.x = element_text(color="Green", size=30), axis.title.y = element_text(color="Red", size=30), axis.text.x = element_text(size=20), axis.text.y = element_text(size=20), legend.title = element_text(size=30), legend.text = element_text(size=20), legend.position = c(1,1), legend.justification = c(1,1), plot.title = element_text(color="DarkBlue", size=40, family="Courier")) # label format

- for more properties of these layers -> ?layer()

<a name="r_datamanipulation"/>
## data manipulation with r
- deplyer package: install.packages("dplyr") > library(dplyr)
	- now "filter" and "slice" functions are masked - "overriden" from new library
- data set from flight 20/13 | install.packages("nycflights13") > library(nycflights13) | check: head(flights)
- focus on few functions of dplyr > easily manipulate data
	- filter() >> filter conditions
		- head(filter(flights, month==11, day==3, carrier=="AA"))
		- alt: head(flights[flights$month==11 & flights$day==3 & flights$carrier=="AA",])
	- slice() >> select rows by position
		- slice(flights, 1:10) # select rows 1 - 10
	- arrange() >> similar to filter, orders rows
		- head(arrange(flights, year, month, day, arr_time))
		- head(arrange(flights, year, month, desc(arr_time))) # descending desc()
	- select() >> select only few collums
		- head(select(flights, carrier, arr_time))
	- rename() >> rename collumns
		- head(rename(flights, airline_carrier = carrier))
	- distinct() >> select distinct values for unique values
		- distinct(select(flights,carrier)) # returns all unique airline names
	- mutate() >> creates new dataframe old one with new columns
		- head(mutate(flights, new_col = arr_delay - dep_delay))
	- transmute() >> creates dataframe with only one (new) column
		- head(transmute(flights, new_col = arr_delay - dep_delay))
	- summarise() >> collaps dataframes into single rows - using a (average) function, eg mean()
		- summarise(flights, avg_air_time = mean(air_time,na.rm = TRUE))
	- sample_n() >> random number of rows
		- sample_n(flights, 10)
	- sample_frac() >> random fraction of rows
		- sample_frac(flights, 0.1)

## pipe operator %>%
- df <- mtcars
- arrange(sample_n(filter(df, mgg>20),size=5),desc(mpg)) # nesting
- df %>% filter(mpg>20) %>% sample_n(size=5) %>% arrange(desc(mpg)) # pipe operator - since every operation has one input of dataframe

## Guide using Tidyr

todo

## introduction to machine learning
-> gareth james - an introduction to statistical learning







________________________________
<a name="sql"/>
# SQL

see data science certification - comprehension


________________________________
<a name="git"/>
# GIT version control

## youtube - corey schafer

### preparation

- "git --version" - check version of git

- "git config --global user.name 'Tomo Bones'" - configure your git setup
- "git config --global user.email 'bones@tomo.de"
- "git config --global diff.tool vimdiff
- "git config --global merge.tool vimdiff"
- "git config --list"

- "git help <verb>" - eg "git help config"
- "git <verb> --help" - eg "git add --help"

- two scenarios: (1) local existing project - (2) remotely existing project

### getting started (1) - local

- "git init" - initiate repository from existing code - just creates ".git" file
- "git status" - check state of repository - see three states...
- "touch .gitignore" -> add some files
	.DS_Store
	.project
	*.pyc
- three states/directories:
	- (W) working dir - modified/untracked files
	- (S) staging area - organize what will/not be committed
	- (C) .git directory/repository - commits <> hash
        - (W) => (S) stage fixes - "git add -A" all - "git add anyFile.txt"
	- (W) <= (S) redo staging - "git reset anyFile.txt" - "git reset" all
	- (S) => (C) commit staged changes - "git commit" - "git commit -m 'message'" detailed changes of code
	- (W) <= (C) checkout project
- "git log" 
	- "git log" see last commit
	- "git log --stat" detailed view of changes of commits
	- "git log --graph --oneline --all"
	- "git log --graph --pretty=oneline --abbrev-commit"
	- "git log --graph --decorate --pretty=oneline --abbrev-commit"

- "git reflog" - shows workflow of actual branch
- "git reflog" - shows workflow of actual branch

### getting started (2) - remote

- "git clone <url> <where to clone>"
	- "git clone ../remote_repo.git ."
	- "git clone ~/project/ ."
	- "git clone https://github.com/CoreyMSchafer/remote_repo.git ."
- view informations about remote repe
	- "git remote -v" - info of remote repository
	- "git branch -a" - info of ALL branches - remotely as well
- push changes to remote repo
	- "git diff" view differences from modified to commited files
	- "git status" view modified files
	- "git add -A"
	- "git commit -m 'message'" - committed locally - want to push to remote repo
	- "git pull origin master"
	- "git push <remoteRepoName> <branch>" eg "git push origin master"

### common workflow - (1) create branch for desired feature - (2) work on branch - (3) push branch 
	- no effect on master branch or remote repo
	- "git branch branchName" - create new branch branchName
	- "git branch" - shows me all the branch names
	- "git checkout branchName" - switch to branchName
	- change project - "git status" - "git add -A" - "git commit -m 'some message'"
	- "git puch -u origin branchName" - push branch to remote repo
	- "git branch -a" - see ALL branches

### merge a branch (with master)
	- "git checkout master" - checkout to local master branch
	- "git pull origin master" - pull all changes down
	- "git branch --merged" - branchName not appearing - not yet merged
	- "git merge branchName" - merge branchName to master
	- "git push origin master"- push changes to remote master branch
	- "git branch --merged" - branchName NOW appearing 
	- "git branch -d branchName" - delete branch locally
	- "git push origin --delete branchName" - delete branch remotely

### fixing common mistakes - undoing bad commits
	- undo local & not staged changes
		- "git status" -> modified file modFile.txt - "git diff" see changes
		- "git checkout modFile.txt"
	- modify messages of commits
		- "git commit --amend -m 'new message'" - same commit, new message, new commit hash
		- BUT: better to not change hash/history... see later...
	- add file after commit
		- "git add file2add.txt"
		- "git commit --amend" -> "git log" same commit - "git log --stat" find new file
	- move commit to other branch (master -> featureBranch)
		- "git log" get first seven chars of hash -> hashChars
		- "git checkout featureBranch"
		- "git cherry-pick hashChars" before/after "git log" new commit appears
		- "git checkout master" -> get rid of commit in master branch
		- "git log" get first seven chars of hash commit to reset -> hashChars
		- "git reset --soft hashChars" - "git status" -> keeps changes in staging directory
		- "git reset hashChars" - mixed - "git status -> keeps changes in working directory
		- "git reset --hard hashChars" - "git status" -> only keeps untracked files in work dir
		- "git clean -df" DELETES untracked (d)irectories and (f)iles
	- undo deletion of files...
		- "git reflog" shows walkthrough of workflow in actual branch -> grap hashChars
		- "git checkout hashChars" -> "git log" shows: changes are back -> detached state 
		- "git branch backup" to safe changes of detached state
	- undo commit - even when others allready puled commit
		- get hashChars to commit to be reverted
		- "git revert hashChars"
		- "git log" commit is reverted to commit before - no commit/history deleted!
	- differences between two hashs:
		- "git diff hashChars1 hashChars2"

### git stash command - keep changes > do sth different > go back to changes
	- "git stash save 'working on certain feature'" - all changes saved and gone "git status"
	- "git stash list" -> see all stashes with ids
	- "git stash apply stash@{n}" -> restores stash no n, not getting rid of saved stash 
	- "git stash pop" -> restores and drops stash on top (with lowest number)
	- "git stash drop stash@{n}" -> deletes stash number n
	- "git stash clear" deletes all stashes, redos all changes
- make changes not in master branch but in featBranch
	- "git stash save 'worked on feature'"
	- "git checkout featBranch"
	- "git stash pop" - "git add ." - "git commit" 

### tools: diff and merge 
- difftool
	- "git diff" -> eventually want to solve conflicts
	- "git difftool" - vimdiff: left <> last commit | right <> changes
- mergetool
	- "git merge feature" want to merge feature into master -> evetual: merging conflict
	- "git mergetool" - left: actual branch | middle: change trying to resolve | right: merge branch

### git add
- to be done: https://www.youtube.com/watch?v=tcd4txbTtAY






## datacamp tutorial

### 1 Basic workflow

- git diff
- git status
- git add
- git commit


#### What is version control?

A version control system is a tool that manages changes made to the files and directories in a project. Many version control systems exist; this lesson focuses on one called Git, which is used by many of the data science tools covered in our other lessons. Its strengths are:

- Nothing that is saved to Git is ever lost, so you can always go back to see which results were generated by which versions of your programs.
- Git automatically notifies you when your work conflicts with someone else's, so it's harder (but not impossible) to accidentally overwrite work.
- Git can synchronize work done by different people on different machines, so it scales as your team does.

Version control isn't just for software: books, papers, parameter sets, and anything that changes over time or needs to be shared can and should be stored and shared using something like Git.


#### Where does Git store information?

Each of your Git projects has two parts: the files and directories that you create and edit directly, and the extra information that Git records about the project's history. The combination of these two things is called a repository.

Git stores all of its extra information in a directory called .git located in the root directory of the repository. Git expects this information to be laid out in a very precise way, so you should never edit or delete anything in .git.


#### How can I check the state of a repository?

When you are using Git, you will frequently want to check the status of your repository. To do this, run the command

		$ git status
		
which displays a list of the files that have been modified since the last time changes were saved.


#### How can I tell what I have changed?

Git has a staging area in which it stores files with changes you want to save that haven't been saved yet. Putting files in the staging area is like putting things in a box, while committing those changes is like putting that box in the mail: you can add more things to the box or take things out as often as you want, but once you put it in the mail, you can't make further changes.

Staging Area

		$ git status

shows you which files are in this staging area, and which files have changes that haven't yet been put there. In order to compare the file as it currently is to what you last saved, you can use 

		$ git diff filename
		$ git diff
		
without any filenames will show you all the changes in your repository, while

		$ git diff directory
		
 will show you the changes to the files in some directory.
 
 
#### What is in a diff?

A diff is a formatted display of the differences between two sets of files. Git displays diffs like this:

		diff --git a/report.txt b/report.txt
		index e713b17..4c0742a 100644
		--- a/report.txt
		+++ b/report.txt
		@@ -1,4 +1,4 @@
		-# Seasonal Dental Surgeries 2017-18
		+# Seasonal Dental Surgeries (2017) 2017-18
		
		 TODO: write executive summary.

This shows:

- The command used to produce the output (in this case, diff --git). In it, a and b are placeholders meaning "the first version" and "the second version".
- An index line showing keys into Git's internal database of changes. We will explore these in the next chapter.

		--- a/report.txt
		+++ b/report.txt

	which indicate that lines being removed are prefixed with -, while lines being added are prefixed with +.
- A line starting with @@ that tells where the changes are being made. The pairs of numbers are start line,number of lines changed. Here, the diff output shows that 4 lines from line 1 are being removed and replaced with new lines.
- A line-by-line listing of the changes with - showing deletions and + showing additions. (We have also configured Git to show deletions in red and additions in green.) Lines that haven't changed are sometimes shown before and after the ones that have in order to give context; when they appear, they don't have either + or - in front of them.

Desktop programming tools like RStudio can turn diffs like this into a more readable side-by-side display of changes; you can also use standalone tools like DiffMerge or WinMerge.

#### What's the first step in saving changes?

You commit changes to a Git repository in two steps:

- Add one or more files to the staging area.
- Commit everything in the staging area.

To add a file to the staging area, use 

		$ git add filename.
		
#### How can I tell what's going to be committed?

To compare the state of your files with those in the staging area, you can use 

		$ git diff -r HEAD
		
The -r flag means "compare to a particular revision", and HEAD is a shortcut meaning "the most recent commit".

You can restrict the results to a single file or directory using 

		$ git diff -r HEAD path/to/file
		
where the path to the file is relative to where you are (for example, the path from the root directory of the repository).

We will explore other uses of -r and HEAD in the next chapter.

#### Interlude: how can I edit a file?

Unix has a bewildering variety of text editors. In this course, we will sometimes use a very simple one called Nano. If you type

		$ nano filename
		
it will open filename for editing (or create it if it doesn't already exist). You can then move around with the arrow keys, delete characters with the backspace key, and so on. You can also do a few other operations with control-key combinations:

Ctrl-K: delete a line.
Ctrl-U: un-delete a line.
Ctrl-O: save the file ('O' stands for 'output').
Ctrl-X: exit the editor.

#### How do I commit changes?

To save the changes in the staging area, you use the command 

		$ git commit
		
It always saves everything that is in the staging area as one unit: as you will see later, when you want to undo changes to a project, you undo all of a commit or none of it.

When you commit changes, Git requires you to enter a log message. This serves the same purpose as a comment in a program: it tells the next person to examine the repository why you made a change.

By default, Git launches a text editor to let you write this message. To keep things simple, you can use  -m "some message in quotes" on the command line to enter a single-line message like this:

		$ git commit -m "Program appears to have become self-aware."
		
If you accidentally mistype a commit message, you can change it using the --amend flag.

		$ git commit --amend - m "new message"
		
#### How can I view a repository's history?

The command

		$ git log
		
is used to view the log of the project's history. Log entries are shown most recent first, and look like this:

		commit 0430705487381195993bac9c21512ccfb511056d
		Author: Rep Loop <repl@datacamp.com>
		Date:   Wed Sep 20 13:42:26 2017 +0000
		
		Added year to report title.
		    
The commit line displays a unique ID for the commit called a hash; we will explore these further in the next chapter. The other lines tell you who made the change, when, and what log message they wrote for the change.

When you run git log, Git automatically uses a pager to show one screen of output at a time. Press the space bar to go down a page or the 'q' key to quit.

You are in the directory dental, which is a Git repository. Use a single Git command to view the repository's history. What is the message on the very first entry in the log (which is displayed last)?

#### How can I view a specific file's history?

A project's entire log can be overwhelming, so it's often useful to inspect only the changes to particular files or directories. You can do this using 

		$ git log path
		
where path is the path to a specific file or directory. The log for a file shows changes made to that file; the log for a directory shows when files were added or deleted in that directory, rather than when the contents of the directory's files were changed.

You have been put in the dental repository. Use git log to display only the changes made to data/southern.csv. How many have there been?

#### How do I write a better log message?

Writing a one-line log message with 

		$ git commit -m "message"
		
is good enough for very small changes, but your collaborators (including your future self) will appreciate more information. If you run 

		$ git commit
		
without -m "message", Git launches a text editor with a template like this:

		# Please enter commit message for changes. Lines starting
		# with '#' will be ignored, an empty message aborts commit.
		# On branch master
		# Your branch is up-to-date with 'origin/master'.
		#
		# Changes to be committed:
		#       modified:   skynet.R
		
The lines starting with # are comments, and won't be saved. (They are there to remind you what you are supposed to do and what files you have changed.) Your message should go at the top, and may be as long and as detailed as you want.

### 2 Repositories

#### How does Git store information?

You may wonder what information is stored by each commit that you make. Git uses a three-level structure for this.

- A commit contains metadata such as the author, the commit message, and the time the commit happened. In the diagram below, the most recent commit is at the bottom (feed0098), and vertical arrows point up towards the previous ("parent") commits.
- Each commit also has a tree, which tracks the names and locations in the repository when that commit happened. In the oldest (top) commit, there were two files tracked by the repository.
- For each of the files listed in the tree, there is a blob. This contains a compressed snapshot of the contents of the file when the commit happened. (Blob is short for binary large object, which is a SQL database term for "may contain data of any kind".) In the middle commit, report.md and draft.md were changed, so the blobs are shown next to that commit. data/northern.csv didn't change in that commit, so the tree links to the blob from the previous commit. Reusing blobs between commits help make common operations fast and minimizes storage space.

#### What is a hash?

Every commit to a repository has a unique identifier called a hash (since it is generated by running the changes through a pseudo-random number generator called a hash function). This hash is normally written as a 40-character hexadecimal string like 7c35a3ce607a14953f070f0f83b5d74c2296ef93, but most of the time, you only have to give Git the first 6 or 8 characters in order to identify the commit you mean.

Hashes are what enable Git to share data efficiently between repositories. If two files are the same, their hashes are guaranteed to be the same. Similarly, if two commits contain the same files and have the same ancestors, their hashes will be the same as well. Git can therefore tell what information needs to be saved where by comparing hashes rather than comparing entire files.

#### How can I view a specific commit?

To view the details of a specific commit, you use the command 

		$ git show
		
with the first few characters of the commit's hash. For example, the command 

		$ git show 0da2f7
		
produces this:

		commit 0da2f7ad11664ca9ed933c1ccd1f3cd24d481e42
		Author: Rep Loop <repl@datacamp.com>
		Date:   Wed Sep 5 15:39:18 2018 +0000
		
    Added year to report title.

		diff --git a/report.txt b/report.txt
		index e713b17..4c0742a 100644
		--- a/report.txt
		+++ b/report.txt
		@@ -1,4 +1,4 @@
		-# Seasonal Dental Surgeries 2017-18
		+# Seasonal Dental Surgeries (2017) 2017-18
		
		 TODO: write executive summary.

The first part is the same as the log entry shown by git log. The second part shows the changes; as with git diff, lines that the change removed are prefixed with -, while lines that it added are prefixed with +.

#### What is Git's equivalent of a relative path?

A hash is like an absolute path: it identifies a specific commit. Another way to identify a commit is to use the equivalent of a relative path. The special label HEAD, which we saw in the previous chapter, always refers to the most recent commit. The label HEAD~1 then refers to the commit before it, while HEAD~2 refers to the commit before that, and so on.

Note that the symbol between HEAD and the number is a tilde ~, not a minus sign -, and that there cannot be spaces before or after the tilde.

#### How can I see who changed what in a file?

git log displays the overall history of a project or file, but Git can give even more information: the command git annotate file shows who made the last change to each line of a file and when. For example, the first three lines of output from

		$ git annotate report.txt
		
look something like this:

		04307054 (Rep Loop 2017-09-20 13:42:26 +0000 1)# Seasonal Dental Surgeries (2017) 2017-18
		5e6f92b6 (Rep Loop 2017-09-20 13:42:26 +0000 2)
		5e6f92b6 (Rep Loop 2017-09-20 13:42:26 +0000 3)TODO: write executive summary.
		
Each line contains five things, with two to four in parentheses.

- The first eight digits of the hash, 04307054.
- The author, Rep Loop.
- The time of the commit, 2017-09-20 13:42:26 +0000.
- The line number, 1.
- The contents of the line, # Seasonal Dental Surgeries (2017) 2017-18.

#### How can I see what changed between two commits?

git show with a commit ID shows the changes made in a particular commit. To see the changes between two commits, you can use git diff ID1..ID2, where ID1 and ID2 identify the two commits you're interested in, and the connector .. is a pair of dots. For example, git diff abc123..def456 shows the differences between the commits abc123 and def456, while git diff HEAD~1..HEAD~3 shows the differences between the state of the repository one commit in the past and its state three commits in the past.

You are in the dental repository. Use git diff to view the differences between its current state and its state two commits previously. Which of the following files have changed?

#### How do I add new files?

Git does not track files by default. Instead, it waits until you have used

		$ git add
		
at least once before it starts paying attention to a file.

In the diagram you saw at the start of the chapter, the untracked files won't have a blob, and won't be listed in a tree.

The untracked files won't benefit from version control, so to make sure you don't miss anything, git status will always tell you about files that are in your repository but aren't (yet) being tracked.

		$ git status
		$ git add untrackedFile.txt
		$ git commit -m "Starting to track data sources"

#### How do I tell Git to ignore certain files?

Data analysis often produces temporary or intermediate files that you don't want to save. You can tell it to stop paying attention to files you don't care about by creating a file in the root directory of your repository called .gitignore and storing a list of wildcard patterns that specify the files you don't want Git to pay attention to. For example, if .gitignore contains:

		build
		*.mpl
		
then Git will ignore any file or directory called build (and, if it's a directory, anything in it), as well as any file whose name ends in .mpl.

#### How can I remove unwanted files?

Git can help you clean up files that you have told it you don't want. The command

		$ git clean -n
		
will show you a list of files that are in the repository, but whose history Git is not currently tracking. A similar command 

		$ git clean -f
		
will then delete those files.

#### How can I see how Git is configured?

Like most complex pieces of software, Git allows you to change its default settings. To see what the settings are, you can use the command

		$ git config --list
		
with one of three additional options:

--system: settings for every user on this computer.
--global: settings for every one of your projects.
--local: settings for one specific project.

Each level overrides the one above it, so local settings (per-project) take precedence over global settings (per-user), which in turn take precedence over system settings (for all users on the computer).

#### How can I change my Git configuration?

Most of Git's settings should be left as they are. However, there are two you should set on every computer you use: your name and your email address. These are recorded in the log every time you commit a change, and are often used to identify the authors of a project's content in order to give credit (or assign blame, depending on the circumstances).

To change a configuration value for all of your projects on a particular computer, run the command:

		$ git config --global setting.name setting.value
		
with the setting's name and value in the appropriate places. The keys that identify your name and email address are user.name and user.email respectively. eg.

		$ git config --global user.email rep.loop@datacamp.com

### 3 Undo

#### How can I commit changes selectively?



#### How do I re-stage files?



#### How can I undo changes to unstaged files?



#### How can I undo changes to staged files?



#### How do I restore an old version of a file?



#### How can I undo all of the changes I have made?




### 4 Working with branches

### 5 Collaborating



________________________________
<a name="applications_in_adition"/>
# Additional (shell) programs to install

- "htop" - process manager alternatively to "top"
- "tmux" - manage your remote terminal sessions with tmux >>> https://www.youtube.com/watch?v=BHhA_ZKjyxo&index=25&list=PLtK75qxsQaMLZSo7KL-PmiRarU7hrpnwK
- "strace" - trace system calls and signals
- "cmatrix" - screensaver
- "tree" - filetree, eg "tree ."
- "git-all"
- "tcpdump" packet sniffer, traffic
- "homebrew" packet manager for os x
- "netstat" part of package "net-tools", eg view established connections
- "netcat" check transmission of data via tcp/udp
- "yum" package manager
- "rpm" package manager
- "ethtool" check physical connection
- "anaconda" python package
- "jupyter" jupyter notebook viewer
- "nmap" read open port numbers
- "gcc" c compiler
	- compile "gcc helloWorld.c -o runHello"
	- run "./runHello"
- python package: anaconda -> python + additional packages eg jupyter-notebook
- "neomutt" email program
	- https://gitlab.com/LukeSmithxyz/mutt-wizard
	- http://tomzai.ch/mutt-mail-client-step-by-step/
	- settings in ~/.config/mutt/muttrc
		- ""
	- "?" keys settings
- "nmcli" network manager
- "inxi" system information, eg "inxi -F"
- "lynx" browser
- "neofetch"
- "aircrack" 
- "pandoc"
	- "pandoc presentation.md -t beamer -o presentation.pdf" markdown to presentation
	- https://www.youtube.com/watch?v=dum7q6UXiCE
- "pterm" terminal version of putty
## (nonshell) programs:
- "pycharm": ide for python
- "nginx-full": webserver nginx
- "i3" window manager
	- https://i3wm.org/docs/userguide.html
	- "i3-msg reload" reload config
	- "i3-msg restart" restart confi restart configg
- "mupdf" pdf viewer
- "qutebrowser" vim-like browser
	- "open" search or enter web adress
	- "f" mark all links -> choose name
- "putty" ssh and telnet client

________________________________
ENDE
